2025-10-13 09:06:31,842 [trainer.py] => config: exps/sema_cifar.json
2025-10-13 09:06:31,842 [trainer.py] => eval: False
2025-10-13 09:06:31,842 [trainer.py] => prefix: reproduce
2025-10-13 09:06:31,842 [trainer.py] => dataset: cifar224
2025-10-13 09:06:31,843 [trainer.py] => memory_size: 0
2025-10-13 09:06:31,843 [trainer.py] => memory_per_class: 0
2025-10-13 09:06:31,843 [trainer.py] => fixed_memory: False
2025-10-13 09:06:31,843 [trainer.py] => shuffle: True
2025-10-13 09:06:31,843 [trainer.py] => init_cls: 10
2025-10-13 09:06:31,843 [trainer.py] => increment: 10
2025-10-13 09:06:31,844 [trainer.py] => model_name: sema
2025-10-13 09:06:31,844 [trainer.py] => backbone_type: pretrained_vit_b16_224_adapter
2025-10-13 09:06:31,844 [trainer.py] => device: [device(type='cuda', index=0)]
2025-10-13 09:06:31,845 [trainer.py] => seed: 1993
2025-10-13 09:06:31,845 [trainer.py] => batch_size: 32
2025-10-13 09:06:31,845 [trainer.py] => weight_decay: 0.0005
2025-10-13 09:06:31,845 [trainer.py] => min_lr: 0
2025-10-13 09:06:31,845 [trainer.py] => ffn_adapter_type: adaptmlp
2025-10-13 09:06:31,845 [trainer.py] => ffn_num: 16
2025-10-13 09:06:31,845 [trainer.py] => optimizer: sgd
2025-10-13 09:06:31,845 [trainer.py] => vpt_type: shallow
2025-10-13 09:06:31,846 [trainer.py] => prompt_token_num: 5
2025-10-13 09:06:31,846 [trainer.py] => func_epoch: 5
2025-10-13 09:06:31,846 [trainer.py] => rd_epoch: 20
2025-10-13 09:06:31,846 [trainer.py] => init_lr: 0.005
2025-10-13 09:06:31,846 [trainer.py] => rd_lr: 0.01
2025-10-13 09:06:31,846 [trainer.py] => rd_dim: 128
2025-10-13 09:06:31,846 [trainer.py] => buffer_size: 500
2025-10-13 09:06:31,846 [trainer.py] => detect_batch_size: 128
2025-10-13 09:06:31,846 [trainer.py] => exp_threshold: 2
2025-10-13 09:06:31,846 [trainer.py] => adapt_start_layer: 9
2025-10-13 09:06:31,847 [trainer.py] => adapt_end_layer: 11
2025-10-13 09:10:02,155 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-10-13 09:10:03,853 [sema_block.py] => Adapter 0.0 added at block 0
2025-10-13 09:10:03,879 [sema_block.py] => Adapter 1.0 added at block 1
2025-10-13 09:10:03,906 [sema_block.py] => Adapter 2.0 added at block 2
2025-10-13 09:10:03,934 [sema_block.py] => Adapter 3.0 added at block 3
2025-10-13 09:10:03,962 [sema_block.py] => Adapter 4.0 added at block 4
2025-10-13 09:10:03,989 [sema_block.py] => Adapter 5.0 added at block 5
2025-10-13 09:10:04,015 [sema_block.py] => Adapter 6.0 added at block 6
2025-10-13 09:10:04,041 [sema_block.py] => Adapter 7.0 added at block 7
2025-10-13 09:10:04,068 [sema_block.py] => Adapter 8.0 added at block 8
2025-10-13 09:10:04,097 [sema_block.py] => Adapter 9.0 added at block 9
2025-10-13 09:10:04,126 [sema_block.py] => Adapter 10.0 added at block 10
2025-10-13 09:10:04,153 [sema_block.py] => Adapter 11.0 added at block 11
2025-10-13 09:10:04,852 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-10-13 09:10:05,579 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-10-13 09:10:05,782 [trainer.py] => All params: 86704716
2025-10-13 09:10:05,783 [trainer.py] => Trainable params: 906060
2025-10-13 09:10:05,785 [sema.py] => Learning on 0-10
2025-10-13 09:12:50,592 [sema.py] => func Task 0, Epoch 5/5 => Loss 0.202, Train_accy 93.78, Test_accy 98.70
2025-10-13 09:23:49,714 [sema.py] => rd Task 0, Epoch 20/20 => Loss 0.703, Train_accy 93.56, Test_accy 98.70
2025-10-13 09:23:52,996 [trainer.py] => No NME accuracy.
2025-10-13 09:23:53,036 [trainer.py] => CNN: {'total': 98.7, '00-09': 98.7, 'old': 0, 'new': 98.7}
2025-10-13 09:23:53,036 [trainer.py] => CNN top1 curve: [98.7]
2025-10-13 09:23:53,037 [trainer.py] => CNN top5 curve: [100.0]

2025-10-13 09:23:53,037 [trainer.py] => Average Accuracy (CNN): 98.7 

2025-10-13 09:23:53,038 [trainer.py] => All params: 86781616
2025-10-13 09:23:53,039 [trainer.py] => Trainable params: 76900
2025-10-13 09:23:53,039 [sema.py] => Learning on 10-20
2025-10-13 09:23:53,740 [sema_block.py] => Adapter 11.1 added at block 11
2025-10-13 09:25:44,578 [sema.py] => func Task 1, Epoch 5/5 => Loss 0.265, Train_accy 91.30, Test_accy 96.00
2025-10-13 09:33:09,183 [sema.py] => rd Task 1, Epoch 20/20 => Loss 0.485, Train_accy 90.32, Test_accy 96.00
2025-10-13 09:33:45,166 [trainer.py] => No NME accuracy.
2025-10-13 09:33:45,167 [trainer.py] => CNN: {'total': 96.0, '00-09': 97.0, '10-19': 95.0, 'old': 97.0, 'new': 95.0}
2025-10-13 09:33:45,167 [trainer.py] => CNN top1 curve: [98.7, 96.0]
2025-10-13 09:33:45,167 [trainer.py] => CNN top5 curve: [100.0, 99.7]

2025-10-13 09:33:45,167 [trainer.py] => Average Accuracy (CNN): 97.35 

2025-10-13 09:33:45,168 [trainer.py] => All params: 87005249
2025-10-13 09:33:45,169 [trainer.py] => Trainable params: 76900
2025-10-13 09:33:45,169 [sema.py] => Learning on 20-30
2025-10-13 09:33:45,913 [sema_block.py] => Adapter 11.2 added at block 11
2025-10-13 09:35:55,591 [sema.py] => func Task 2, Epoch 5/5 => Loss 0.152, Train_accy 95.02, Test_accy 94.53
2025-10-13 09:44:28,325 [sema.py] => rd Task 2, Epoch 20/20 => Loss 0.459, Train_accy 95.58, Test_accy 94.53
2025-10-13 09:45:08,110 [trainer.py] => No NME accuracy.
2025-10-13 09:45:08,110 [trainer.py] => CNN: {'total': 94.53, '00-09': 94.9, '10-19': 93.2, '20-29': 95.5, 'old': 94.05, 'new': 95.5}
2025-10-13 09:45:08,111 [trainer.py] => CNN top1 curve: [98.7, 96.0, 94.53]
2025-10-13 09:45:08,111 [trainer.py] => CNN top5 curve: [100.0, 99.7, 99.57]

2025-10-13 09:45:08,111 [trainer.py] => Average Accuracy (CNN): 96.41000000000001 

2025-10-13 09:45:08,112 [trainer.py] => All params: 87228882
2025-10-13 09:45:08,112 [trainer.py] => Trainable params: 76900
2025-10-13 09:45:08,112 [sema.py] => Learning on 30-40
2025-10-13 09:45:08,768 [sema_block.py] => Adapter 11.3 added at block 11
2025-10-13 09:47:33,396 [sema.py] => func Task 3, Epoch 5/5 => Loss 0.208, Train_accy 92.58, Test_accy 93.50
2025-10-13 10:01:54,905 [sema.py] => rd Task 3, Epoch 20/20 => Loss 0.484, Train_accy 93.00, Test_accy 93.50
2025-10-13 10:06:48,959 [trainer.py] => No NME accuracy.
2025-10-13 10:06:48,959 [trainer.py] => CNN: {'total': 93.5, '00-09': 94.5, '10-19': 92.1, '20-29': 94.8, '30-39': 92.6, 'old': 93.8, 'new': 92.6}
2025-10-13 10:06:48,959 [trainer.py] => CNN top1 curve: [98.7, 96.0, 94.53, 93.5]
2025-10-13 10:06:48,960 [trainer.py] => CNN top5 curve: [100.0, 99.7, 99.57, 99.42]

2025-10-13 10:06:48,960 [trainer.py] => Average Accuracy (CNN): 95.6825 

2025-10-13 10:06:48,960 [trainer.py] => All params: 87452515
2025-10-13 10:06:48,961 [trainer.py] => Trainable params: 76900
2025-10-13 10:06:48,961 [sema.py] => Learning on 40-50
2025-10-13 10:06:57,704 [sema_block.py] => Adapter 11.4 added at block 11
2025-10-13 10:26:49,464 [sema.py] => func Task 4, Epoch 5/5 => Loss 0.169, Train_accy 94.78, Test_accy 91.52
2025-10-13 12:44:12,501 [sema.py] => rd Task 4, Epoch 20/20 => Loss 0.479, Train_accy 94.36, Test_accy 91.52
2025-10-13 12:51:54,288 [trainer.py] => No NME accuracy.
2025-10-13 12:51:54,289 [trainer.py] => CNN: {'total': 91.52, '00-09': 93.4, '10-19': 89.1, '20-29': 93.6, '30-39': 90.4, '40-49': 91.1, 'old': 91.62, 'new': 91.1}
2025-10-13 12:51:54,290 [trainer.py] => CNN top1 curve: [98.7, 96.0, 94.53, 93.5, 91.52]
2025-10-13 12:51:54,290 [trainer.py] => CNN top5 curve: [100.0, 99.7, 99.57, 99.42, 99.4]

2025-10-13 12:51:54,290 [trainer.py] => Average Accuracy (CNN): 94.85 

2025-10-13 12:51:54,291 [trainer.py] => All params: 87676148
2025-10-13 12:51:54,292 [trainer.py] => Trainable params: 76900
2025-10-13 12:51:54,292 [sema.py] => Learning on 50-60
2025-10-13 12:52:10,919 [sema_block.py] => Adapter 11.5 added at block 11
2025-10-13 13:12:44,081 [sema.py] => func Task 5, Epoch 5/5 => Loss 0.208, Train_accy 92.62, Test_accy 89.83
2025-10-13 15:10:12,033 [sema.py] => rd Task 5, Epoch 20/20 => Loss 0.506, Train_accy 92.64, Test_accy 89.83
2025-10-13 15:11:12,672 [sema_block.py] => Adapter 10.1 added at block 10
2025-10-13 15:23:00,113 [sema.py] => func Task 5, Epoch 5/5 => Loss 0.215, Train_accy 92.68, Test_accy 90.15
2025-10-13 16:09:27,583 [sema.py] => rd Task 5, Epoch 20/20 => Loss 0.171, Train_accy 92.44, Test_accy 90.15
2025-10-13 16:11:44,798 [trainer.py] => No NME accuracy.
2025-10-13 16:11:44,800 [trainer.py] => CNN: {'total': 90.15, '00-09': 91.3, '10-19': 88.9, '20-29': 92.2, '30-39': 89.2, '40-49': 86.8, '50-59': 92.5, 'old': 89.68, 'new': 92.5}
2025-10-13 16:11:44,800 [trainer.py] => CNN top1 curve: [98.7, 96.0, 94.53, 93.5, 91.52, 90.15]
2025-10-13 16:11:44,800 [trainer.py] => CNN top5 curve: [100.0, 99.7, 99.57, 99.42, 99.4, 99.2]

2025-10-13 16:11:44,800 [trainer.py] => Average Accuracy (CNN): 94.06666666666666 

2025-10-13 16:11:44,801 [trainer.py] => All params: 88123414
2025-10-13 16:11:44,802 [trainer.py] => Trainable params: 76900
2025-10-13 16:11:44,802 [sema.py] => Learning on 60-70
2025-10-13 16:11:48,451 [sema_block.py] => Adapter 11.6 added at block 11
2025-10-13 16:24:45,979 [sema.py] => func Task 6, Epoch 5/5 => Loss 0.167, Train_accy 94.68, Test_accy 90.03
2025-10-13 16:56:46,713 [sema.py] => rd Task 6, Epoch 20/20 => Loss 0.487, Train_accy 94.74, Test_accy 90.03
2025-10-13 16:59:13,778 [trainer.py] => No NME accuracy.
2025-10-13 16:59:13,778 [trainer.py] => CNN: {'total': 90.03, '00-09': 90.6, '10-19': 87.4, '20-29': 92.0, '30-39': 88.4, '40-49': 86.7, '50-59': 90.4, '60-69': 94.7, 'old': 89.25, 'new': 94.7}
2025-10-13 16:59:13,778 [trainer.py] => CNN top1 curve: [98.7, 96.0, 94.53, 93.5, 91.52, 90.15, 90.03]
2025-10-13 16:59:13,779 [trainer.py] => CNN top5 curve: [100.0, 99.7, 99.57, 99.42, 99.4, 99.2, 99.11]

2025-10-13 16:59:13,779 [trainer.py] => Average Accuracy (CNN): 93.49 

2025-10-13 16:59:13,780 [trainer.py] => All params: 88347047
2025-10-13 16:59:13,780 [trainer.py] => Trainable params: 76900
2025-10-13 16:59:13,780 [sema.py] => Learning on 70-80
2025-10-13 16:59:57,277 [sema_block.py] => Adapter 11.7 added at block 11
2025-10-13 17:09:50,907 [sema.py] => func Task 7, Epoch 5/5 => Loss 0.224, Train_accy 92.32, Test_accy 88.01
2025-10-13 17:49:12,897 [sema.py] => rd Task 7, Epoch 20/20 => Loss 0.511, Train_accy 92.32, Test_accy 88.01
2025-10-13 17:51:25,479 [trainer.py] => No NME accuracy.
2025-10-13 17:51:25,480 [trainer.py] => CNN: {'total': 88.01, '00-09': 89.9, '10-19': 87.3, '20-29': 91.7, '30-39': 88.1, '40-49': 83.2, '50-59': 86.7, '60-69': 90.6, '70-79': 86.6, 'old': 88.21, 'new': 86.6}
2025-10-13 17:51:25,480 [trainer.py] => CNN top1 curve: [98.7, 96.0, 94.53, 93.5, 91.52, 90.15, 90.03, 88.01]
2025-10-13 17:51:25,480 [trainer.py] => CNN top5 curve: [100.0, 99.7, 99.57, 99.42, 99.4, 99.2, 99.11, 99.0]

2025-10-13 17:51:25,481 [trainer.py] => Average Accuracy (CNN): 92.80499999999999 

2025-10-13 17:51:25,481 [trainer.py] => All params: 88570680
2025-10-13 17:51:25,482 [trainer.py] => Trainable params: 76900
2025-10-13 17:51:25,482 [sema.py] => Learning on 80-90
2025-10-13 17:51:28,401 [sema_block.py] => Adapter 11.8 added at block 11
2025-10-13 18:03:37,551 [sema.py] => func Task 8, Epoch 5/5 => Loss 0.201, Train_accy 93.26, Test_accy 87.27
2025-10-13 18:31:11,768 [sema.py] => rd Task 8, Epoch 20/20 => Loss 0.483, Train_accy 93.28, Test_accy 87.27
2025-10-13 18:32:15,204 [trainer.py] => No NME accuracy.
2025-10-13 18:32:15,205 [trainer.py] => CNN: {'total': 87.27, '00-09': 89.1, '10-19': 85.4, '20-29': 91.0, '30-39': 87.9, '40-49': 81.7, '50-59': 86.2, '60-69': 90.1, '70-79': 85.3, '80-89': 88.7, 'old': 87.09, 'new': 88.7}
2025-10-13 18:32:15,207 [trainer.py] => CNN top1 curve: [98.7, 96.0, 94.53, 93.5, 91.52, 90.15, 90.03, 88.01, 87.27]
2025-10-13 18:32:15,207 [trainer.py] => CNN top5 curve: [100.0, 99.7, 99.57, 99.42, 99.4, 99.2, 99.11, 99.0, 98.79]

2025-10-13 18:32:15,208 [trainer.py] => Average Accuracy (CNN): 92.19 

2025-10-13 18:32:15,208 [trainer.py] => All params: 88794313
2025-10-13 18:32:15,209 [trainer.py] => Trainable params: 76900
2025-10-13 18:32:15,209 [sema.py] => Learning on 90-100
2025-10-13 18:32:15,968 [sema_block.py] => Adapter 11.9 added at block 11
2025-10-13 18:36:40,593 [sema.py] => func Task 9, Epoch 5/5 => Loss 0.222, Train_accy 93.08, Test_accy 86.82
2025-10-13 18:54:11,759 [sema.py] => rd Task 9, Epoch 20/20 => Loss 0.465, Train_accy 92.18, Test_accy 86.82
2025-10-13 18:55:19,357 [trainer.py] => No NME accuracy.
2025-10-13 18:55:19,358 [trainer.py] => CNN: {'total': 86.82, '00-09': 87.8, '10-19': 85.1, '20-29': 90.5, '30-39': 87.5, '40-49': 80.7, '50-59': 85.9, '60-69': 89.1, '70-79': 84.6, '80-89': 88.2, '90-99': 88.8, 'old': 86.6, 'new': 88.8}
2025-10-13 18:55:19,358 [trainer.py] => CNN top1 curve: [98.7, 96.0, 94.53, 93.5, 91.52, 90.15, 90.03, 88.01, 87.27, 86.82]
2025-10-13 18:55:19,358 [trainer.py] => CNN top5 curve: [100.0, 99.7, 99.57, 99.42, 99.4, 99.2, 99.11, 99.0, 98.79, 98.56]

2025-10-13 18:55:19,359 [trainer.py] => Average Accuracy (CNN): 91.65299999999999 

2025-11-07 11:19:59,004 [trainer.py] => config: exps/sema_cifar.json
2025-11-07 11:19:59,004 [trainer.py] => eval: False
2025-11-07 11:19:59,004 [trainer.py] => prefix: reproduce
2025-11-07 11:19:59,005 [trainer.py] => dataset: cifar224
2025-11-07 11:19:59,005 [trainer.py] => memory_size: 0
2025-11-07 11:19:59,005 [trainer.py] => memory_per_class: 0
2025-11-07 11:19:59,005 [trainer.py] => fixed_memory: False
2025-11-07 11:19:59,006 [trainer.py] => shuffle: True
2025-11-07 11:19:59,006 [trainer.py] => init_cls: 10
2025-11-07 11:19:59,006 [trainer.py] => increment: 10
2025-11-07 11:19:59,007 [trainer.py] => model_name: sema
2025-11-07 11:19:59,007 [trainer.py] => backbone_type: pretrained_vit_b16_224_adapter
2025-11-07 11:19:59,007 [trainer.py] => device: [device(type='cuda', index=0)]
2025-11-07 11:19:59,007 [trainer.py] => seed: 1993
2025-11-07 11:19:59,008 [trainer.py] => batch_size: 32
2025-11-07 11:19:59,008 [trainer.py] => weight_decay: 0.0005
2025-11-07 11:19:59,008 [trainer.py] => min_lr: 0
2025-11-07 11:19:59,008 [trainer.py] => ffn_adapter_type: adaptmlp
2025-11-07 11:19:59,008 [trainer.py] => ffn_num: 16
2025-11-07 11:19:59,009 [trainer.py] => optimizer: sgd
2025-11-07 11:19:59,009 [trainer.py] => vpt_type: shallow
2025-11-07 11:19:59,009 [trainer.py] => prompt_token_num: 5
2025-11-07 11:19:59,009 [trainer.py] => func_epoch: 5
2025-11-07 11:19:59,010 [trainer.py] => rd_epoch: 20
2025-11-07 11:19:59,010 [trainer.py] => init_lr: 0.005
2025-11-07 11:19:59,010 [trainer.py] => rd_lr: 0.01
2025-11-07 11:19:59,010 [trainer.py] => rd_dim: 128
2025-11-07 11:19:59,011 [trainer.py] => buffer_size: 500
2025-11-07 11:19:59,011 [trainer.py] => detect_batch_size: 128
2025-11-07 11:19:59,011 [trainer.py] => exp_threshold: 2
2025-11-07 11:19:59,011 [trainer.py] => router_attn_dim: 128
2025-11-07 11:19:59,012 [trainer.py] => adapt_start_layer: 9
2025-11-07 11:19:59,012 [trainer.py] => adapt_end_layer: 11
2025-11-07 11:20:00,402 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-11-07 11:20:03,456 [sema_block.py] => Adapter 0.0 added at block 0
2025-11-07 11:20:03,488 [sema_block.py] => Adapter 1.0 added at block 1
2025-11-07 11:20:03,527 [sema_block.py] => Adapter 2.0 added at block 2
2025-11-07 11:20:03,579 [sema_block.py] => Adapter 3.0 added at block 3
2025-11-07 11:20:03,631 [sema_block.py] => Adapter 4.0 added at block 4
2025-11-07 11:20:03,675 [sema_block.py] => Adapter 5.0 added at block 5
2025-11-07 11:20:03,713 [sema_block.py] => Adapter 6.0 added at block 6
2025-11-07 11:20:03,749 [sema_block.py] => Adapter 7.0 added at block 7
2025-11-07 11:20:03,788 [sema_block.py] => Adapter 8.0 added at block 8
2025-11-07 11:20:03,828 [sema_block.py] => Adapter 9.0 added at block 9
2025-11-07 11:20:03,881 [sema_block.py] => Adapter 10.0 added at block 10
2025-11-07 11:20:03,930 [sema_block.py] => Adapter 11.0 added at block 11
2025-11-07 11:20:04,919 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-11-07 11:20:05,546 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-11-07 11:20:05,846 [trainer.py] => All params: 89057868
2025-11-07 11:20:05,848 [trainer.py] => Trainable params: 3259212
2025-11-07 11:20:05,850 [sema.py] => Learning on 0-10
2025-11-07 11:23:08,755 [sema.py] => func Task 0, Epoch 5/5 => Loss 0.224, Train_accy 92.54, Test_accy 98.60
2025-11-07 11:35:24,318 [sema.py] => rd Task 0, Epoch 20/20 => Loss 0.695, Train_accy 94.04, Test_accy 98.60
2025-11-07 11:35:28,271 [trainer.py] => No NME accuracy.
2025-11-07 11:35:28,272 [trainer.py] => CNN: {'total': 98.6, '00-09': 98.6, 'old': 0, 'new': 98.6}
2025-11-07 11:35:28,272 [trainer.py] => CNN top1 curve: [98.6]
2025-11-07 11:35:28,273 [trainer.py] => CNN top5 curve: [100.0]

2025-11-07 11:35:28,273 [trainer.py] => Average Accuracy (CNN): 98.6 

2025-11-07 11:35:28,275 [trainer.py] => All params: 89134768
2025-11-07 11:35:28,276 [trainer.py] => Trainable params: 76912
2025-11-07 11:35:28,276 [sema.py] => Learning on 10-20
2025-11-07 11:35:29,286 [sema_block.py] => Adapter 11.1 added at block 11
2025-11-07 11:37:55,480 [sema.py] => func Task 1, Epoch 5/5 => Loss 0.258, Train_accy 91.32, Test_accy 95.75
2025-11-07 11:47:45,143 [sema.py] => rd Task 1, Epoch 20/20 => Loss 0.471, Train_accy 90.96, Test_accy 95.75
2025-11-07 11:48:25,775 [trainer.py] => No NME accuracy.
2025-11-07 11:48:25,776 [trainer.py] => CNN: {'total': 95.75, '00-09': 97.0, '10-19': 94.5, 'old': 97.0, 'new': 94.5}
2025-11-07 11:48:25,779 [trainer.py] => CNN top1 curve: [98.6, 95.75]
2025-11-07 11:48:25,780 [trainer.py] => CNN top5 curve: [100.0, 99.8]

2025-11-07 11:48:25,780 [trainer.py] => Average Accuracy (CNN): 97.175 

2025-11-07 11:48:25,783 [trainer.py] => All params: 89357632
2025-11-07 11:48:25,785 [trainer.py] => Trainable params: 76912
2025-11-07 11:48:25,785 [sema.py] => Learning on 20-30
2025-11-07 11:48:26,771 [sema_block.py] => Adapter 11.2 added at block 11
2025-11-07 11:51:14,452 [sema.py] => func Task 2, Epoch 5/5 => Loss 0.158, Train_accy 94.50, Test_accy 94.30
2025-11-07 12:02:50,254 [sema.py] => rd Task 2, Epoch 20/20 => Loss 0.450, Train_accy 94.76, Test_accy 94.30
2025-11-07 12:03:35,018 [trainer.py] => No NME accuracy.
2025-11-07 12:03:35,018 [trainer.py] => CNN: {'total': 94.3, '00-09': 95.1, '10-19': 92.6, '20-29': 95.2, 'old': 93.85, 'new': 95.2}
2025-11-07 12:03:35,018 [trainer.py] => CNN top1 curve: [98.6, 95.75, 94.3]
2025-11-07 12:03:35,019 [trainer.py] => CNN top5 curve: [100.0, 99.8, 99.6]

2025-11-07 12:03:35,019 [trainer.py] => Average Accuracy (CNN): 96.21666666666665 

2025-11-07 12:03:35,020 [trainer.py] => All params: 89580496
2025-11-07 12:03:35,021 [trainer.py] => Trainable params: 76912
2025-11-07 12:03:35,021 [sema.py] => Learning on 30-40
2025-11-07 12:03:36,078 [sema_block.py] => Adapter 11.3 added at block 11
2025-11-07 12:06:48,797 [sema.py] => func Task 3, Epoch 5/5 => Loss 0.218, Train_accy 92.86, Test_accy 93.10
2025-11-07 12:20:19,551 [sema.py] => rd Task 3, Epoch 20/20 => Loss 0.472, Train_accy 92.40, Test_accy 93.10
2025-11-07 12:20:35,691 [sema_block.py] => Adapter 10.1 added at block 10
2025-11-07 12:23:47,518 [sema.py] => func Task 3, Epoch 5/5 => Loss 0.200, Train_accy 93.36, Test_accy 93.22
2025-11-07 12:36:21,807 [sema.py] => rd Task 3, Epoch 20/20 => Loss 0.165, Train_accy 93.86, Test_accy 93.22
2025-11-07 12:36:55,228 [trainer.py] => No NME accuracy.
2025-11-07 12:36:55,229 [trainer.py] => CNN: {'total': 93.22, '00-09': 94.3, '10-19': 91.4, '20-29': 94.3, '30-39': 92.9, 'old': 93.33, 'new': 92.9}
2025-11-07 12:36:55,230 [trainer.py] => CNN top1 curve: [98.6, 95.75, 94.3, 93.22]
2025-11-07 12:36:55,231 [trainer.py] => CNN top5 curve: [100.0, 99.8, 99.6, 99.52]

2025-11-07 12:36:55,232 [trainer.py] => Average Accuracy (CNN): 95.4675 

2025-11-07 12:36:55,234 [trainer.py] => All params: 90026224
2025-11-07 12:36:55,235 [trainer.py] => Trainable params: 76912
2025-11-07 12:36:55,236 [sema.py] => Learning on 40-50
2025-11-07 12:36:56,373 [sema_block.py] => Adapter 11.4 added at block 11
2025-11-07 12:40:32,137 [sema.py] => func Task 4, Epoch 5/5 => Loss 0.178, Train_accy 94.46, Test_accy 91.58
2025-11-07 12:55:39,415 [sema.py] => rd Task 4, Epoch 20/20 => Loss 0.465, Train_accy 94.86, Test_accy 91.58
2025-11-07 12:56:33,951 [trainer.py] => No NME accuracy.
2025-11-07 12:56:33,951 [trainer.py] => CNN: {'total': 91.58, '00-09': 92.7, '10-19': 88.1, '20-29': 93.3, '30-39': 90.5, '40-49': 93.3, 'old': 91.15, 'new': 93.3}
2025-11-07 12:56:33,952 [trainer.py] => CNN top1 curve: [98.6, 95.75, 94.3, 93.22, 91.58]
2025-11-07 12:56:33,952 [trainer.py] => CNN top5 curve: [100.0, 99.8, 99.6, 99.52, 99.38]

2025-11-07 12:56:33,953 [trainer.py] => Average Accuracy (CNN): 94.69 

2025-11-07 12:56:33,955 [trainer.py] => All params: 90249088
2025-11-07 12:56:33,957 [trainer.py] => Trainable params: 76912
2025-11-07 12:56:33,958 [sema.py] => Learning on 50-60
2025-11-07 12:56:35,032 [sema_block.py] => Adapter 11.5 added at block 11
2025-11-07 13:00:35,047 [sema.py] => func Task 5, Epoch 5/5 => Loss 0.242, Train_accy 92.02, Test_accy 90.50
2025-11-07 13:17:37,627 [sema.py] => rd Task 5, Epoch 20/20 => Loss 0.493, Train_accy 92.34, Test_accy 90.50
2025-11-07 13:18:37,219 [trainer.py] => No NME accuracy.
2025-11-07 13:18:37,220 [trainer.py] => CNN: {'total': 90.5, '00-09': 91.2, '10-19': 88.0, '20-29': 92.7, '30-39': 89.4, '40-49': 91.2, '50-59': 90.5, 'old': 90.5, 'new': 90.5}
2025-11-07 13:18:37,221 [trainer.py] => CNN top1 curve: [98.6, 95.75, 94.3, 93.22, 91.58, 90.5]
2025-11-07 13:18:37,221 [trainer.py] => CNN top5 curve: [100.0, 99.8, 99.6, 99.52, 99.38, 99.2]

2025-11-07 13:18:37,221 [trainer.py] => Average Accuracy (CNN): 93.99166666666667 

2025-11-07 13:18:37,223 [trainer.py] => All params: 90471952
2025-11-07 13:18:37,224 [trainer.py] => Trainable params: 76912
2025-11-07 13:18:37,224 [sema.py] => Learning on 60-70
2025-11-07 13:18:38,376 [sema_block.py] => Adapter 11.6 added at block 11
2025-11-07 13:23:03,920 [sema.py] => func Task 6, Epoch 5/5 => Loss 0.160, Train_accy 94.80, Test_accy 90.41
2025-11-07 13:42:00,209 [sema.py] => rd Task 6, Epoch 20/20 => Loss 0.472, Train_accy 94.54, Test_accy 90.41
2025-11-07 13:43:04,830 [trainer.py] => No NME accuracy.
2025-11-07 13:43:04,831 [trainer.py] => CNN: {'total': 90.41, '00-09': 90.2, '10-19': 86.5, '20-29': 92.6, '30-39': 88.9, '40-49': 91.0, '50-59': 88.3, '60-69': 95.4, 'old': 89.58, 'new': 95.4}
2025-11-07 13:43:04,832 [trainer.py] => CNN top1 curve: [98.6, 95.75, 94.3, 93.22, 91.58, 90.5, 90.41]
2025-11-07 13:43:04,832 [trainer.py] => CNN top5 curve: [100.0, 99.8, 99.6, 99.52, 99.38, 99.2, 99.13]

2025-11-07 13:43:04,832 [trainer.py] => Average Accuracy (CNN): 93.48 

2025-11-07 13:43:04,834 [trainer.py] => All params: 90694816
2025-11-07 13:43:04,834 [trainer.py] => Trainable params: 76912
2025-11-07 13:43:04,835 [sema.py] => Learning on 70-80
2025-11-07 13:43:09,649 [sema_block.py] => Adapter 11.7 added at block 11
2025-11-07 13:47:57,022 [sema.py] => func Task 7, Epoch 5/5 => Loss 0.221, Train_accy 92.58, Test_accy 88.24
2025-11-07 14:08:35,491 [sema.py] => rd Task 7, Epoch 20/20 => Loss 0.493, Train_accy 91.68, Test_accy 88.24
2025-11-07 14:10:00,937 [trainer.py] => No NME accuracy.
2025-11-07 14:10:00,938 [trainer.py] => CNN: {'total': 88.24, '00-09': 89.9, '10-19': 86.2, '20-29': 92.5, '30-39': 88.7, '40-49': 87.6, '50-59': 85.2, '60-69': 91.7, '70-79': 84.1, 'old': 88.83, 'new': 84.1}
2025-11-07 14:10:00,938 [trainer.py] => CNN top1 curve: [98.6, 95.75, 94.3, 93.22, 91.58, 90.5, 90.41, 88.24]
2025-11-07 14:10:00,939 [trainer.py] => CNN top5 curve: [100.0, 99.8, 99.6, 99.52, 99.38, 99.2, 99.13, 98.89]

2025-11-07 14:10:00,939 [trainer.py] => Average Accuracy (CNN): 92.825 

2025-11-07 14:10:00,940 [trainer.py] => All params: 90917680
2025-11-07 14:10:00,942 [trainer.py] => Trainable params: 76912
2025-11-07 14:10:00,942 [sema.py] => Learning on 80-90
2025-11-07 14:10:02,806 [sema_block.py] => Adapter 11.8 added at block 11
2025-11-07 14:15:20,348 [sema.py] => func Task 8, Epoch 5/5 => Loss 0.207, Train_accy 93.14, Test_accy 87.29
2025-11-07 14:37:59,351 [sema.py] => rd Task 8, Epoch 20/20 => Loss 0.468, Train_accy 93.00, Test_accy 87.29
2025-11-07 14:39:43,899 [trainer.py] => No NME accuracy.
2025-11-07 14:39:43,900 [trainer.py] => CNN: {'total': 87.29, '00-09': 89.5, '10-19': 84.2, '20-29': 91.5, '30-39': 88.1, '40-49': 85.3, '50-59': 84.3, '60-69': 91.1, '70-79': 82.7, '80-89': 88.9, 'old': 87.09, 'new': 88.9}
2025-11-07 14:39:43,900 [trainer.py] => CNN top1 curve: [98.6, 95.75, 94.3, 93.22, 91.58, 90.5, 90.41, 88.24, 87.29]
2025-11-07 14:39:43,900 [trainer.py] => CNN top5 curve: [100.0, 99.8, 99.6, 99.52, 99.38, 99.2, 99.13, 98.89, 98.68]

2025-11-07 14:39:43,901 [trainer.py] => Average Accuracy (CNN): 92.21 

2025-11-07 14:39:43,902 [trainer.py] => All params: 91140544
2025-11-07 14:39:43,903 [trainer.py] => Trainable params: 76912
2025-11-07 14:39:43,903 [sema.py] => Learning on 90-100
2025-11-07 14:39:48,075 [sema_block.py] => Adapter 11.9 added at block 11
2025-11-07 14:45:44,499 [sema.py] => func Task 9, Epoch 5/5 => Loss 0.218, Train_accy 92.46, Test_accy 86.75
2025-11-07 15:11:01,356 [sema.py] => rd Task 9, Epoch 20/20 => Loss 0.459, Train_accy 91.92, Test_accy 86.75
2025-11-07 15:12:51,350 [trainer.py] => No NME accuracy.
2025-11-07 15:12:51,351 [trainer.py] => CNN: {'total': 86.75, '00-09': 88.0, '10-19': 83.9, '20-29': 91.0, '30-39': 87.8, '40-49': 84.2, '50-59': 83.5, '60-69': 90.0, '70-79': 81.9, '80-89': 88.5, '90-99': 88.7, 'old': 86.53, 'new': 88.7}
2025-11-07 15:12:51,351 [trainer.py] => CNN top1 curve: [98.6, 95.75, 94.3, 93.22, 91.58, 90.5, 90.41, 88.24, 87.29, 86.75]
2025-11-07 15:12:51,352 [trainer.py] => CNN top5 curve: [100.0, 99.8, 99.6, 99.52, 99.38, 99.2, 99.13, 98.89, 98.68, 98.55]

2025-11-07 15:12:51,352 [trainer.py] => Average Accuracy (CNN): 91.664 

2025-11-10 10:08:37,057 [trainer.py] => config: exps/sema_cifar.json
2025-11-10 10:08:37,058 [trainer.py] => eval: False
2025-11-10 10:08:37,059 [trainer.py] => prefix: reproduce
2025-11-10 10:08:37,059 [trainer.py] => dataset: cifar224
2025-11-10 10:08:37,059 [trainer.py] => memory_size: 0
2025-11-10 10:08:37,060 [trainer.py] => memory_per_class: 0
2025-11-10 10:08:37,060 [trainer.py] => fixed_memory: False
2025-11-10 10:08:37,060 [trainer.py] => shuffle: True
2025-11-10 10:08:37,061 [trainer.py] => init_cls: 10
2025-11-10 10:08:37,061 [trainer.py] => increment: 10
2025-11-10 10:08:37,061 [trainer.py] => model_name: sema
2025-11-10 10:08:37,062 [trainer.py] => backbone_type: pretrained_vit_b16_224_adapter
2025-11-10 10:08:37,062 [trainer.py] => device: [device(type='cuda', index=0)]
2025-11-10 10:08:37,062 [trainer.py] => seed: 1993
2025-11-10 10:08:37,063 [trainer.py] => batch_size: 32
2025-11-10 10:08:37,063 [trainer.py] => weight_decay: 0.0005
2025-11-10 10:08:37,063 [trainer.py] => min_lr: 0
2025-11-10 10:08:37,063 [trainer.py] => ffn_adapter_type: adaptmlp
2025-11-10 10:08:37,064 [trainer.py] => ffn_num: 16
2025-11-10 10:08:37,064 [trainer.py] => optimizer: sgd
2025-11-10 10:08:37,064 [trainer.py] => vpt_type: shallow
2025-11-10 10:08:37,065 [trainer.py] => prompt_token_num: 5
2025-11-10 10:08:37,065 [trainer.py] => func_epoch: 5
2025-11-10 10:08:37,065 [trainer.py] => rd_epoch: 20
2025-11-10 10:08:37,066 [trainer.py] => init_lr: 0.005
2025-11-10 10:08:37,066 [trainer.py] => rd_lr: 0.01
2025-11-10 10:08:37,067 [trainer.py] => rd_dim: 128
2025-11-10 10:08:37,067 [trainer.py] => buffer_size: 500
2025-11-10 10:08:37,067 [trainer.py] => detect_batch_size: 128
2025-11-10 10:08:37,068 [trainer.py] => exp_threshold: 2
2025-11-10 10:08:37,068 [trainer.py] => exp_k_std: 3.0
2025-11-10 10:08:37,068 [trainer.py] => router_attn_dim: 128
2025-11-10 10:08:37,068 [trainer.py] => adapt_start_layer: 9
2025-11-10 10:08:37,069 [trainer.py] => adapt_end_layer: 11
2025-11-10 10:08:38,418 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-11-10 10:08:41,468 [sema_block.py] => Adapter 0.0 added at block 0
2025-11-10 10:08:41,521 [sema_block.py] => Adapter 1.0 added at block 1
2025-11-10 10:08:41,574 [sema_block.py] => Adapter 2.0 added at block 2
2025-11-10 10:08:41,615 [sema_block.py] => Adapter 3.0 added at block 3
2025-11-10 10:08:41,676 [sema_block.py] => Adapter 4.0 added at block 4
2025-11-10 10:08:41,722 [sema_block.py] => Adapter 5.0 added at block 5
2025-11-10 10:08:41,750 [sema_block.py] => Adapter 6.0 added at block 6
2025-11-10 10:08:41,789 [sema_block.py] => Adapter 7.0 added at block 7
2025-11-10 10:08:41,840 [sema_block.py] => Adapter 8.0 added at block 8
2025-11-10 10:08:41,888 [sema_block.py] => Adapter 9.0 added at block 9
2025-11-10 10:08:41,933 [sema_block.py] => Adapter 10.0 added at block 10
2025-11-10 10:08:41,973 [sema_block.py] => Adapter 11.0 added at block 11
2025-11-10 10:08:44,211 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-11-10 10:08:44,974 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-11-10 10:08:45,262 [trainer.py] => All params: 89057868
2025-11-10 10:08:45,265 [trainer.py] => Trainable params: 3259212
2025-11-10 10:08:45,267 [sema.py] => Learning on 0-10
2025-11-10 10:11:47,383 [sema.py] => func Task 0, Epoch 5/5 => Loss 0.224, Train_accy 92.54, Test_accy 98.60
2025-11-10 10:23:52,593 [sema.py] => rd Task 0, Epoch 20/20 => Loss 0.695, Train_accy 94.04, Test_accy 98.60
2025-11-10 10:23:56,412 [trainer.py] => No NME accuracy.
2025-11-10 10:23:56,413 [trainer.py] => CNN: {'total': 98.6, '00-09': 98.6, 'old': 0, 'new': 98.6}
2025-11-10 10:23:56,413 [trainer.py] => CNN top1 curve: [98.6]
2025-11-10 10:23:56,413 [trainer.py] => CNN top5 curve: [100.0]

2025-11-10 10:23:56,413 [trainer.py] => Average Accuracy (CNN): 98.6 

2025-11-10 10:23:56,415 [trainer.py] => All params: 89134768
2025-11-10 10:23:56,416 [trainer.py] => Trainable params: 76912
2025-11-10 10:23:56,416 [sema.py] => Learning on 10-20
2025-11-10 10:23:57,355 [sema_block.py] => Adapter 11.1 added at block 11
2025-11-10 10:26:20,818 [sema.py] => func Task 1, Epoch 5/5 => Loss 0.258, Train_accy 91.32, Test_accy 95.75
2025-11-10 10:35:57,057 [sema.py] => rd Task 1, Epoch 20/20 => Loss 0.471, Train_accy 90.96, Test_accy 95.75
2025-11-10 10:36:39,097 [trainer.py] => No NME accuracy.
2025-11-10 10:36:39,099 [trainer.py] => CNN: {'total': 95.75, '00-09': 97.0, '10-19': 94.5, 'old': 97.0, 'new': 94.5}
2025-11-10 10:36:39,099 [trainer.py] => CNN top1 curve: [98.6, 95.75]
2025-11-10 10:36:39,100 [trainer.py] => CNN top5 curve: [100.0, 99.8]

2025-11-10 10:36:39,101 [trainer.py] => Average Accuracy (CNN): 97.175 

2025-11-10 10:36:39,102 [trainer.py] => All params: 89357632
2025-11-10 10:36:39,103 [trainer.py] => Trainable params: 76912
2025-11-10 10:36:39,103 [sema.py] => Learning on 20-30
2025-11-10 10:36:40,110 [sema_block.py] => Adapter 11.2 added at block 11
2025-11-10 10:39:26,867 [sema.py] => func Task 2, Epoch 5/5 => Loss 0.158, Train_accy 94.50, Test_accy 94.30
2025-11-10 10:50:58,158 [sema.py] => rd Task 2, Epoch 20/20 => Loss 0.450, Train_accy 94.76, Test_accy 94.30
2025-11-10 10:51:30,612 [sema_block.py] => Adapter 9.1 added at block 9
2025-11-10 10:54:13,586 [sema.py] => func Task 2, Epoch 5/5 => Loss 0.140, Train_accy 95.56, Test_accy 94.30
2025-11-10 11:04:39,793 [sema.py] => rd Task 2, Epoch 20/20 => Loss 0.096, Train_accy 95.16, Test_accy 94.30
2025-11-10 11:05:09,432 [trainer.py] => No NME accuracy.
2025-11-10 11:05:09,432 [trainer.py] => CNN: {'total': 94.3, '00-09': 94.8, '10-19': 92.2, '20-29': 95.9, 'old': 93.5, 'new': 95.9}
2025-11-10 11:05:09,434 [trainer.py] => CNN top1 curve: [98.6, 95.75, 94.3]
2025-11-10 11:05:09,434 [trainer.py] => CNN top5 curve: [100.0, 99.8, 99.63]

2025-11-10 11:05:09,435 [trainer.py] => Average Accuracy (CNN): 96.21666666666665 

2025-11-10 11:05:09,436 [trainer.py] => All params: 89803360
2025-11-10 11:05:09,437 [trainer.py] => Trainable params: 76912
2025-11-10 11:05:09,438 [sema.py] => Learning on 30-40
2025-11-10 11:05:10,495 [sema_block.py] => Adapter 11.3 added at block 11
2025-11-10 11:08:19,778 [sema.py] => func Task 3, Epoch 5/5 => Loss 0.216, Train_accy 92.38, Test_accy 93.08
2025-11-10 11:21:39,377 [sema.py] => rd Task 3, Epoch 20/20 => Loss 0.476, Train_accy 92.56, Test_accy 93.08
2025-11-10 11:22:28,030 [trainer.py] => No NME accuracy.
2025-11-10 11:22:28,030 [trainer.py] => CNN: {'total': 93.08, '00-09': 94.3, '10-19': 91.3, '20-29': 95.2, '30-39': 91.5, 'old': 93.6, 'new': 91.5}
2025-11-10 11:22:28,031 [trainer.py] => CNN top1 curve: [98.6, 95.75, 94.3, 93.08]
2025-11-10 11:22:28,031 [trainer.py] => CNN top5 curve: [100.0, 99.8, 99.63, 99.42]

2025-11-10 11:22:28,032 [trainer.py] => Average Accuracy (CNN): 95.43249999999999 

2025-11-10 11:22:28,033 [trainer.py] => All params: 90026224
2025-11-10 11:22:28,034 [trainer.py] => Trainable params: 76912
2025-11-10 11:22:28,034 [sema.py] => Learning on 40-50
2025-11-10 11:22:29,041 [sema_block.py] => Adapter 11.4 added at block 11
2025-11-10 11:26:01,615 [sema.py] => func Task 4, Epoch 5/5 => Loss 0.167, Train_accy 94.72, Test_accy 91.24
2025-11-10 11:41:01,555 [sema.py] => rd Task 4, Epoch 20/20 => Loss 0.468, Train_accy 94.62, Test_accy 91.24
2025-11-10 11:41:56,903 [trainer.py] => No NME accuracy.
2025-11-10 11:41:56,904 [trainer.py] => CNN: {'total': 91.24, '00-09': 93.3, '10-19': 88.4, '20-29': 94.1, '30-39': 88.7, '40-49': 91.7, 'old': 91.12, 'new': 91.7}
2025-11-10 11:41:56,904 [trainer.py] => CNN top1 curve: [98.6, 95.75, 94.3, 93.08, 91.24]
2025-11-10 11:41:56,904 [trainer.py] => CNN top5 curve: [100.0, 99.8, 99.63, 99.42, 99.38]

2025-11-10 11:41:56,905 [trainer.py] => Average Accuracy (CNN): 94.594 

2025-11-10 11:41:56,906 [trainer.py] => All params: 90249088
2025-11-10 11:41:56,907 [trainer.py] => Trainable params: 76912
2025-11-10 11:41:56,907 [sema.py] => Learning on 50-60
2025-11-10 11:41:57,989 [sema_block.py] => Adapter 11.5 added at block 11
2025-11-10 11:45:59,091 [sema.py] => func Task 5, Epoch 5/5 => Loss 0.221, Train_accy 92.00, Test_accy 90.05
2025-11-10 12:03:06,279 [sema.py] => rd Task 5, Epoch 20/20 => Loss 0.493, Train_accy 92.44, Test_accy 90.05
2025-11-10 12:04:07,038 [trainer.py] => No NME accuracy.
2025-11-10 12:04:07,039 [trainer.py] => CNN: {'total': 90.05, '00-09': 91.4, '10-19': 88.1, '20-29': 93.3, '30-39': 87.7, '40-49': 89.6, '50-59': 90.2, 'old': 90.02, 'new': 90.2}
2025-11-10 12:04:07,039 [trainer.py] => CNN top1 curve: [98.6, 95.75, 94.3, 93.08, 91.24, 90.05]
2025-11-10 12:04:07,039 [trainer.py] => CNN top5 curve: [100.0, 99.8, 99.63, 99.42, 99.38, 99.2]

2025-11-10 12:04:07,040 [trainer.py] => Average Accuracy (CNN): 93.83666666666666 

2025-11-10 12:04:07,042 [trainer.py] => All params: 90471952
2025-11-10 12:04:07,043 [trainer.py] => Trainable params: 76912
2025-11-10 12:04:07,044 [sema.py] => Learning on 60-70
2025-11-10 12:04:08,203 [sema_block.py] => Adapter 11.6 added at block 11
2025-11-10 12:08:33,518 [sema.py] => func Task 6, Epoch 5/5 => Loss 0.181, Train_accy 94.06, Test_accy 90.01
2025-11-10 12:27:48,384 [sema.py] => rd Task 6, Epoch 20/20 => Loss 0.474, Train_accy 94.50, Test_accy 90.01
2025-11-10 12:28:55,740 [trainer.py] => No NME accuracy.
2025-11-10 12:28:55,740 [trainer.py] => CNN: {'total': 90.01, '00-09': 90.7, '10-19': 86.6, '20-29': 93.2, '30-39': 87.3, '40-49': 89.5, '50-59': 88.1, '60-69': 94.7, 'old': 89.23, 'new': 94.7}
2025-11-10 12:28:55,743 [trainer.py] => CNN top1 curve: [98.6, 95.75, 94.3, 93.08, 91.24, 90.05, 90.01]
2025-11-10 12:28:55,744 [trainer.py] => CNN top5 curve: [100.0, 99.8, 99.63, 99.42, 99.38, 99.2, 99.11]

2025-11-10 12:28:55,744 [trainer.py] => Average Accuracy (CNN): 93.28999999999999 

2025-11-10 12:28:55,745 [trainer.py] => All params: 90694816
2025-11-10 12:28:55,746 [trainer.py] => Trainable params: 76912
2025-11-10 12:28:55,746 [sema.py] => Learning on 70-80
2025-11-10 12:33:57,235 [sema.py] => func Task 7, Epoch 5/5 => Loss 0.207, Train_accy 93.30, Test_accy 88.00
2025-11-10 12:34:30,653 [trainer.py] => No NME accuracy.
2025-11-10 12:34:30,654 [trainer.py] => CNN: {'total': 88.0, '00-09': 90.4, '10-19': 86.5, '20-29': 93.0, '30-39': 87.0, '40-49': 86.0, '50-59': 83.7, '60-69': 91.6, '70-79': 85.8, 'old': 88.31, 'new': 85.8}
2025-11-10 12:34:30,654 [trainer.py] => CNN top1 curve: [98.6, 95.75, 94.3, 93.08, 91.24, 90.05, 90.01, 88.0]
2025-11-10 12:34:30,655 [trainer.py] => CNN top5 curve: [100.0, 99.8, 99.63, 99.42, 99.38, 99.2, 99.11, 98.98]

2025-11-10 12:34:30,655 [trainer.py] => Average Accuracy (CNN): 92.62875 

2025-11-10 12:34:30,658 [trainer.py] => All params: 90694816
2025-11-10 12:34:30,659 [trainer.py] => Trainable params: 76912
2025-11-10 12:34:30,660 [sema.py] => Learning on 80-90
2025-11-10 12:34:31,815 [sema_block.py] => Adapter 11.7 added at block 11
2025-11-10 12:39:42,978 [sema.py] => func Task 8, Epoch 5/5 => Loss 0.223, Train_accy 92.72, Test_accy 87.14
2025-11-10 13:01:31,321 [sema.py] => rd Task 8, Epoch 20/20 => Loss 0.467, Train_accy 92.92, Test_accy 87.14
2025-11-10 13:02:27,294 [sema_block.py] => Adapter 10.1 added at block 10
2025-11-10 13:07:35,018 [sema.py] => func Task 8, Epoch 5/5 => Loss 0.202, Train_accy 93.66, Test_accy 87.01
2025-11-10 13:27:43,770 [sema.py] => rd Task 8, Epoch 20/20 => Loss 0.168, Train_accy 93.46, Test_accy 87.01
2025-11-10 13:28:48,480 [trainer.py] => No NME accuracy.
2025-11-10 13:28:48,480 [trainer.py] => CNN: {'total': 87.01, '00-09': 89.4, '10-19': 84.0, '20-29': 92.3, '30-39': 86.7, '40-49': 83.0, '50-59': 82.3, '60-69': 90.1, '70-79': 84.0, '80-89': 91.3, 'old': 86.48, 'new': 91.3}
2025-11-10 13:28:48,481 [trainer.py] => CNN top1 curve: [98.6, 95.75, 94.3, 93.08, 91.24, 90.05, 90.01, 88.0, 87.01]
2025-11-10 13:28:48,481 [trainer.py] => CNN top5 curve: [100.0, 99.8, 99.63, 99.42, 99.38, 99.2, 99.11, 98.98, 98.77]

2025-11-10 13:28:48,482 [trainer.py] => Average Accuracy (CNN): 92.00444444444445 

2025-11-10 13:28:48,485 [trainer.py] => All params: 91140544
2025-11-10 13:28:48,487 [trainer.py] => Trainable params: 76912
2025-11-10 13:28:48,487 [sema.py] => Learning on 90-100
2025-11-10 13:28:49,616 [sema_block.py] => Adapter 11.8 added at block 11
2025-11-10 13:34:21,242 [sema.py] => func Task 9, Epoch 5/5 => Loss 0.235, Train_accy 92.02, Test_accy 86.62
2025-11-10 13:57:54,396 [sema.py] => rd Task 9, Epoch 20/20 => Loss 0.460, Train_accy 91.38, Test_accy 86.62
2025-11-10 13:59:41,893 [trainer.py] => No NME accuracy.
2025-11-10 13:59:41,895 [trainer.py] => CNN: {'total': 86.62, '00-09': 88.2, '10-19': 83.7, '20-29': 91.9, '30-39': 86.4, '40-49': 82.4, '50-59': 81.5, '60-69': 89.3, '70-79': 83.0, '80-89': 90.9, '90-99': 88.9, 'old': 86.37, 'new': 88.9}
2025-11-10 13:59:41,895 [trainer.py] => CNN top1 curve: [98.6, 95.75, 94.3, 93.08, 91.24, 90.05, 90.01, 88.0, 87.01, 86.62]
2025-11-10 13:59:41,895 [trainer.py] => CNN top5 curve: [100.0, 99.8, 99.63, 99.42, 99.38, 99.2, 99.11, 98.98, 98.77, 98.52]

2025-11-10 13:59:41,896 [trainer.py] => Average Accuracy (CNN): 91.466 

2025-11-11 10:25:36,663 [trainer.py] => config: exps/sema_cifar.json
2025-11-11 10:25:36,664 [trainer.py] => eval: False
2025-11-11 10:25:36,664 [trainer.py] => prefix: reproduce
2025-11-11 10:25:36,664 [trainer.py] => dataset: cifar224
2025-11-11 10:25:36,665 [trainer.py] => memory_size: 0
2025-11-11 10:25:36,665 [trainer.py] => memory_per_class: 0
2025-11-11 10:25:36,665 [trainer.py] => fixed_memory: False
2025-11-11 10:25:36,666 [trainer.py] => shuffle: True
2025-11-11 10:25:36,666 [trainer.py] => init_cls: 10
2025-11-11 10:25:36,666 [trainer.py] => increment: 10
2025-11-11 10:25:36,675 [trainer.py] => model_name: sema
2025-11-11 10:25:36,675 [trainer.py] => backbone_type: pretrained_vit_b16_224_adapter
2025-11-11 10:25:36,675 [trainer.py] => device: [device(type='cuda', index=0)]
2025-11-11 10:25:36,676 [trainer.py] => seed: 1993
2025-11-11 10:25:36,676 [trainer.py] => batch_size: 32
2025-11-11 10:25:36,676 [trainer.py] => weight_decay: 0.0005
2025-11-11 10:25:36,676 [trainer.py] => min_lr: 0
2025-11-11 10:25:36,677 [trainer.py] => ffn_adapter_type: adaptmlp
2025-11-11 10:25:36,677 [trainer.py] => ffn_num: 16
2025-11-11 10:25:36,677 [trainer.py] => optimizer: sgd
2025-11-11 10:25:36,677 [trainer.py] => vpt_type: shallow
2025-11-11 10:25:36,677 [trainer.py] => prompt_token_num: 5
2025-11-11 10:25:36,677 [trainer.py] => func_epoch: 5
2025-11-11 10:25:36,677 [trainer.py] => rd_epoch: 20
2025-11-11 10:25:36,678 [trainer.py] => init_lr: 0.005
2025-11-11 10:25:36,678 [trainer.py] => rd_lr: 0.01
2025-11-11 10:25:36,678 [trainer.py] => rd_dim: 128
2025-11-11 10:25:36,678 [trainer.py] => buffer_size: 500
2025-11-11 10:25:36,678 [trainer.py] => detect_batch_size: 128
2025-11-11 10:25:36,678 [trainer.py] => exp_threshold: 2
2025-11-11 10:25:36,678 [trainer.py] => exp_k_std: 3.0
2025-11-11 10:25:36,679 [trainer.py] => router_attn_dim: 128
2025-11-11 10:25:36,679 [trainer.py] => adapt_start_layer: 9
2025-11-11 10:25:36,679 [trainer.py] => adapt_end_layer: 11
2025-11-11 10:25:37,655 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-11-11 10:25:39,299 [sema_block.py] => Adapter 0.0 added at block 0
2025-11-11 10:25:39,327 [sema_block.py] => Adapter 1.0 added at block 1
2025-11-11 10:25:39,354 [sema_block.py] => Adapter 2.0 added at block 2
2025-11-11 10:25:39,382 [sema_block.py] => Adapter 3.0 added at block 3
2025-11-11 10:25:39,410 [sema_block.py] => Adapter 4.0 added at block 4
2025-11-11 10:25:39,438 [sema_block.py] => Adapter 5.0 added at block 5
2025-11-11 10:25:39,465 [sema_block.py] => Adapter 6.0 added at block 6
2025-11-11 10:25:39,493 [sema_block.py] => Adapter 7.0 added at block 7
2025-11-11 10:25:39,520 [sema_block.py] => Adapter 8.0 added at block 8
2025-11-11 10:25:39,549 [sema_block.py] => Adapter 9.0 added at block 9
2025-11-11 10:25:39,578 [sema_block.py] => Adapter 10.0 added at block 10
2025-11-11 10:25:39,606 [sema_block.py] => Adapter 11.0 added at block 11
2025-11-11 10:25:40,136 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-11-11 10:25:40,864 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-11-11 10:25:41,034 [trainer.py] => All params: 89057868
2025-11-11 10:25:41,035 [trainer.py] => Trainable params: 3259212
2025-11-11 10:25:41,036 [sema.py] => Learning on 0-10
2025-11-11 10:28:26,954 [sema.py] => func Task 0, Epoch 5/5 => Loss 0.224, Train_accy 92.54, Test_accy 98.60
2025-11-11 10:39:21,498 [sema.py] => rd Task 0, Epoch 20/20 => Loss 0.695, Train_accy 94.04, Test_accy 98.60
2025-11-11 10:39:24,717 [trainer.py] => No NME accuracy.
2025-11-11 10:39:24,718 [trainer.py] => CNN: {'total': 98.6, '00-09': 98.6, 'old': 0, 'new': 98.6}
2025-11-11 10:39:24,718 [trainer.py] => CNN top1 curve: [98.6]
2025-11-11 10:39:24,718 [trainer.py] => CNN top5 curve: [100.0]

2025-11-11 10:39:24,719 [trainer.py] => Average Accuracy (CNN): 98.6 

2025-11-11 10:39:24,720 [trainer.py] => All params: 89134768
2025-11-11 10:39:24,720 [trainer.py] => Trainable params: 2439280
2025-11-11 10:39:24,720 [sema.py] => Learning on 10-20
2025-11-11 10:47:49,674 [trainer.py] => config: exps/sema_cifar.json
2025-11-11 10:47:49,674 [trainer.py] => eval: False
2025-11-11 10:47:49,674 [trainer.py] => prefix: reproduce
2025-11-11 10:47:49,675 [trainer.py] => dataset: cifar224
2025-11-11 10:47:49,675 [trainer.py] => memory_size: 0
2025-11-11 10:47:49,675 [trainer.py] => memory_per_class: 0
2025-11-11 10:47:49,675 [trainer.py] => fixed_memory: False
2025-11-11 10:47:49,676 [trainer.py] => shuffle: True
2025-11-11 10:47:49,676 [trainer.py] => init_cls: 10
2025-11-11 10:47:49,676 [trainer.py] => increment: 10
2025-11-11 10:47:49,676 [trainer.py] => model_name: sema
2025-11-11 10:47:49,676 [trainer.py] => backbone_type: pretrained_vit_b16_224_adapter
2025-11-11 10:47:49,676 [trainer.py] => device: [device(type='cuda', index=0)]
2025-11-11 10:47:49,677 [trainer.py] => seed: 1993
2025-11-11 10:47:49,677 [trainer.py] => batch_size: 32
2025-11-11 10:47:49,677 [trainer.py] => weight_decay: 0.0005
2025-11-11 10:47:49,677 [trainer.py] => min_lr: 0
2025-11-11 10:47:49,677 [trainer.py] => ffn_adapter_type: adaptmlp
2025-11-11 10:47:49,677 [trainer.py] => ffn_num: 16
2025-11-11 10:47:49,677 [trainer.py] => optimizer: sgd
2025-11-11 10:47:49,677 [trainer.py] => vpt_type: shallow
2025-11-11 10:47:49,677 [trainer.py] => prompt_token_num: 5
2025-11-11 10:47:49,677 [trainer.py] => func_epoch: 5
2025-11-11 10:47:49,677 [trainer.py] => rd_epoch: 20
2025-11-11 10:47:49,678 [trainer.py] => init_lr: 0.005
2025-11-11 10:47:49,678 [trainer.py] => rd_lr: 0.01
2025-11-11 10:47:49,678 [trainer.py] => rd_dim: 128
2025-11-11 10:47:49,678 [trainer.py] => buffer_size: 500
2025-11-11 10:47:49,678 [trainer.py] => detect_batch_size: 128
2025-11-11 10:47:49,678 [trainer.py] => exp_threshold: 2
2025-11-11 10:47:49,678 [trainer.py] => exp_k_std: 3.0
2025-11-11 10:47:49,678 [trainer.py] => router_attn_dim: 128
2025-11-11 10:47:49,678 [trainer.py] => adapt_start_layer: 9
2025-11-11 10:47:49,678 [trainer.py] => adapt_end_layer: 11
2025-11-11 10:47:50,559 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-11-11 10:47:51,459 [sema_block.py] => Adapter 0.0 added at block 0
2025-11-11 10:47:51,485 [sema_block.py] => Adapter 1.0 added at block 1
2025-11-11 10:47:51,512 [sema_block.py] => Adapter 2.0 added at block 2
2025-11-11 10:47:51,539 [sema_block.py] => Adapter 3.0 added at block 3
2025-11-11 10:47:51,567 [sema_block.py] => Adapter 4.0 added at block 4
2025-11-11 10:47:51,593 [sema_block.py] => Adapter 5.0 added at block 5
2025-11-11 10:47:51,620 [sema_block.py] => Adapter 6.0 added at block 6
2025-11-11 10:47:51,647 [sema_block.py] => Adapter 7.0 added at block 7
2025-11-11 10:47:51,672 [sema_block.py] => Adapter 8.0 added at block 8
2025-11-11 10:47:51,700 [sema_block.py] => Adapter 9.0 added at block 9
2025-11-11 10:47:51,729 [sema_block.py] => Adapter 10.0 added at block 10
2025-11-11 10:47:51,757 [sema_block.py] => Adapter 11.0 added at block 11
2025-11-11 10:47:52,312 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-11-11 10:47:52,920 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-11-11 10:47:53,030 [trainer.py] => All params: 89057868
2025-11-11 10:47:53,032 [trainer.py] => Trainable params: 3259212
2025-11-11 10:50:58,785 [trainer.py] => config: exps/sema_cifar.json
2025-11-11 10:50:58,786 [trainer.py] => eval: False
2025-11-11 10:50:58,786 [trainer.py] => prefix: reproduce
2025-11-11 10:50:58,786 [trainer.py] => dataset: cifar224
2025-11-11 10:50:58,786 [trainer.py] => memory_size: 0
2025-11-11 10:50:58,786 [trainer.py] => memory_per_class: 0
2025-11-11 10:50:58,786 [trainer.py] => fixed_memory: False
2025-11-11 10:50:58,786 [trainer.py] => shuffle: True
2025-11-11 10:50:58,787 [trainer.py] => init_cls: 10
2025-11-11 10:50:58,787 [trainer.py] => increment: 10
2025-11-11 10:50:58,787 [trainer.py] => model_name: sema
2025-11-11 10:50:58,787 [trainer.py] => backbone_type: pretrained_vit_b16_224_adapter
2025-11-11 10:50:58,787 [trainer.py] => device: [device(type='cuda', index=0)]
2025-11-11 10:50:58,788 [trainer.py] => seed: 1993
2025-11-11 10:50:58,788 [trainer.py] => batch_size: 32
2025-11-11 10:50:58,788 [trainer.py] => weight_decay: 0.0005
2025-11-11 10:50:58,788 [trainer.py] => min_lr: 0
2025-11-11 10:50:58,788 [trainer.py] => ffn_adapter_type: adaptmlp
2025-11-11 10:50:58,788 [trainer.py] => ffn_num: 16
2025-11-11 10:50:58,788 [trainer.py] => optimizer: sgd
2025-11-11 10:50:58,788 [trainer.py] => vpt_type: shallow
2025-11-11 10:50:58,789 [trainer.py] => prompt_token_num: 5
2025-11-11 10:50:58,789 [trainer.py] => func_epoch: 5
2025-11-11 10:50:58,789 [trainer.py] => rd_epoch: 20
2025-11-11 10:50:58,789 [trainer.py] => init_lr: 0.005
2025-11-11 10:50:58,789 [trainer.py] => rd_lr: 0.01
2025-11-11 10:50:58,789 [trainer.py] => rd_dim: 128
2025-11-11 10:50:58,789 [trainer.py] => buffer_size: 500
2025-11-11 10:50:58,789 [trainer.py] => detect_batch_size: 128
2025-11-11 10:50:58,789 [trainer.py] => exp_threshold: 2
2025-11-11 10:50:58,790 [trainer.py] => exp_k_std: 3.0
2025-11-11 10:50:58,790 [trainer.py] => router_attn_dim: 128
2025-11-11 10:50:58,790 [trainer.py] => adapt_start_layer: 9
2025-11-11 10:50:58,790 [trainer.py] => adapt_end_layer: 11
2025-11-11 10:50:59,761 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-11-11 10:51:01,210 [sema_block.py] => Adapter 0.0 added at block 0
2025-11-11 10:51:01,240 [sema_block.py] => Adapter 1.0 added at block 1
2025-11-11 10:51:01,269 [sema_block.py] => Adapter 2.0 added at block 2
2025-11-11 10:51:01,299 [sema_block.py] => Adapter 3.0 added at block 3
2025-11-11 10:51:01,328 [sema_block.py] => Adapter 4.0 added at block 4
2025-11-11 10:51:01,359 [sema_block.py] => Adapter 5.0 added at block 5
2025-11-11 10:51:01,390 [sema_block.py] => Adapter 6.0 added at block 6
2025-11-11 10:51:01,420 [sema_block.py] => Adapter 7.0 added at block 7
2025-11-11 10:51:01,449 [sema_block.py] => Adapter 8.0 added at block 8
2025-11-11 10:51:01,479 [sema_block.py] => Adapter 9.0 added at block 9
2025-11-11 10:51:01,510 [sema_block.py] => Adapter 10.0 added at block 10
2025-11-11 10:51:01,540 [sema_block.py] => Adapter 11.0 added at block 11
2025-11-11 10:51:02,108 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-11-11 10:51:02,687 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-11-11 10:51:02,877 [trainer.py] => All params: 89057868
2025-11-11 10:51:02,879 [trainer.py] => Trainable params: 3259212
2025-11-11 10:51:02,881 [sema.py] => Learning on 0-10
2025-11-11 10:53:45,814 [sema.py] => func Task 0, Epoch 5/5 => Loss 0.224, Train_accy 92.54, Test_accy 98.60
2025-11-11 11:04:41,073 [sema.py] => rd Task 0, Epoch 20/20 => Loss 0.695, Train_accy 94.04, Test_accy 98.60
2025-11-11 11:04:44,020 [trainer.py] => No NME accuracy.
2025-11-11 11:04:44,021 [trainer.py] => CNN: {'total': 98.6, '00-09': 98.6, 'old': 0, 'new': 98.6}
2025-11-11 11:04:44,021 [trainer.py] => CNN top1 curve: [98.6]
2025-11-11 11:04:44,022 [trainer.py] => CNN top5 curve: [100.0]

2025-11-11 11:04:44,022 [trainer.py] => Average Accuracy (CNN): 98.6 

2025-11-11 11:04:44,023 [trainer.py] => All params: 89134768
2025-11-11 11:04:44,023 [trainer.py] => Trainable params: 2439280
2025-11-11 11:04:44,024 [sema.py] => Learning on 10-20
2025-11-11 11:04:44,680 [sema_block.py] => Adapter 11.1 added at block 11
2025-11-11 11:06:49,222 [sema.py] => func Task 1, Epoch 5/5 => Loss 0.258, Train_accy 91.32, Test_accy 95.75
2025-11-11 11:14:54,055 [sema.py] => rd Task 1, Epoch 20/20 => Loss 0.471, Train_accy 90.96, Test_accy 95.75
2025-11-11 11:15:37,601 [trainer.py] => No NME accuracy.
2025-11-11 11:15:37,601 [trainer.py] => CNN: {'total': 95.75, '00-09': 97.0, '10-19': 94.5, 'old': 97.0, 'new': 94.5}
2025-11-11 11:15:37,602 [trainer.py] => CNN top1 curve: [98.6, 95.75]
2025-11-11 11:15:37,602 [trainer.py] => CNN top5 curve: [100.0, 99.8]

2025-11-11 11:15:37,602 [trainer.py] => Average Accuracy (CNN): 97.175 

2025-11-11 11:15:37,603 [trainer.py] => All params: 89357632
2025-11-11 11:15:37,603 [trainer.py] => Trainable params: 2439280
2025-11-11 11:15:37,604 [sema.py] => Learning on 20-30
2025-11-11 11:15:35,880 [sema_block.py] => Adapter 11.2 added at block 11
2025-11-11 11:17:57,352 [sema.py] => func Task 2, Epoch 5/5 => Loss 0.158, Train_accy 94.50, Test_accy 94.30
2025-11-11 11:27:19,370 [sema.py] => rd Task 2, Epoch 20/20 => Loss 0.450, Train_accy 94.76, Test_accy 94.30
2025-11-11 11:27:49,434 [sema_block.py] => Adapter 9.1 added at block 9
2025-11-11 11:30:13,314 [sema.py] => func Task 2, Epoch 5/5 => Loss 0.140, Train_accy 95.56, Test_accy 94.30
2025-11-11 11:38:54,049 [sema.py] => rd Task 2, Epoch 20/20 => Loss 0.096, Train_accy 95.16, Test_accy 94.30
2025-11-11 11:39:19,093 [trainer.py] => No NME accuracy.
2025-11-11 11:39:19,094 [trainer.py] => CNN: {'total': 94.3, '00-09': 94.8, '10-19': 92.2, '20-29': 95.9, 'old': 93.5, 'new': 95.9}
2025-11-11 11:39:19,094 [trainer.py] => CNN top1 curve: [98.6, 95.75, 94.3]
2025-11-11 11:39:19,095 [trainer.py] => CNN top5 curve: [100.0, 99.8, 99.63]

2025-11-11 11:39:19,095 [trainer.py] => Average Accuracy (CNN): 96.21666666666665 

2025-11-11 11:39:19,096 [trainer.py] => All params: 89803360
2025-11-11 11:39:19,097 [trainer.py] => Trainable params: 2439280
2025-11-11 11:39:19,097 [sema.py] => Learning on 30-40
2025-11-11 11:39:19,949 [sema_block.py] => Adapter 11.3 added at block 11
2025-11-11 11:42:05,695 [sema.py] => func Task 3, Epoch 5/5 => Loss 0.216, Train_accy 92.38, Test_accy 93.08
2025-11-11 11:52:57,777 [sema.py] => rd Task 3, Epoch 20/20 => Loss 0.476, Train_accy 92.56, Test_accy 93.08
2025-11-11 11:53:41,170 [trainer.py] => No NME accuracy.
2025-11-11 11:53:41,171 [trainer.py] => CNN: {'total': 93.08, '00-09': 94.3, '10-19': 91.3, '20-29': 95.2, '30-39': 91.5, 'old': 93.6, 'new': 91.5}
2025-11-11 11:53:41,171 [trainer.py] => CNN top1 curve: [98.6, 95.75, 94.3, 93.08]
2025-11-11 11:53:41,171 [trainer.py] => CNN top5 curve: [100.0, 99.8, 99.63, 99.42]

2025-11-11 11:53:41,172 [trainer.py] => Average Accuracy (CNN): 95.43249999999999 

2025-11-11 11:53:41,172 [trainer.py] => All params: 90026224
2025-11-11 11:53:41,173 [trainer.py] => Trainable params: 2439280
2025-11-11 11:53:41,173 [sema.py] => Learning on 40-50
2025-11-11 11:53:41,972 [sema_block.py] => Adapter 11.4 added at block 11
2025-11-11 11:56:41,886 [sema.py] => func Task 4, Epoch 5/5 => Loss 0.167, Train_accy 94.72, Test_accy 91.24
2025-11-11 12:09:00,177 [sema.py] => rd Task 4, Epoch 20/20 => Loss 0.468, Train_accy 94.62, Test_accy 91.24
2025-11-11 12:09:47,848 [trainer.py] => No NME accuracy.
2025-11-11 12:09:47,848 [trainer.py] => CNN: {'total': 91.24, '00-09': 93.3, '10-19': 88.4, '20-29': 94.1, '30-39': 88.7, '40-49': 91.7, 'old': 91.12, 'new': 91.7}
2025-11-11 12:09:47,848 [trainer.py] => CNN top1 curve: [98.6, 95.75, 94.3, 93.08, 91.24]
2025-11-11 12:09:47,848 [trainer.py] => CNN top5 curve: [100.0, 99.8, 99.63, 99.42, 99.38]

2025-11-11 12:09:47,848 [trainer.py] => Average Accuracy (CNN): 94.594 

2025-11-11 12:09:47,849 [trainer.py] => All params: 90249088
2025-11-11 12:09:47,850 [trainer.py] => Trainable params: 2439280
2025-11-11 12:09:47,850 [sema.py] => Learning on 50-60
2025-11-11 12:09:48,558 [sema_block.py] => Adapter 11.5 added at block 11
2025-11-11 12:13:08,551 [sema.py] => func Task 5, Epoch 5/5 => Loss 0.221, Train_accy 92.00, Test_accy 90.05
2025-11-11 12:26:46,148 [sema.py] => rd Task 5, Epoch 20/20 => Loss 0.493, Train_accy 92.44, Test_accy 90.05
2025-11-11 12:27:38,403 [trainer.py] => No NME accuracy.
2025-11-11 12:27:38,404 [trainer.py] => CNN: {'total': 90.05, '00-09': 91.4, '10-19': 88.1, '20-29': 93.3, '30-39': 87.7, '40-49': 89.6, '50-59': 90.2, 'old': 90.02, 'new': 90.2}
2025-11-11 12:27:38,404 [trainer.py] => CNN top1 curve: [98.6, 95.75, 94.3, 93.08, 91.24, 90.05]
2025-11-11 12:27:38,405 [trainer.py] => CNN top5 curve: [100.0, 99.8, 99.63, 99.42, 99.38, 99.2]

2025-11-11 12:27:38,405 [trainer.py] => Average Accuracy (CNN): 93.83666666666666 

2025-11-11 12:27:38,406 [trainer.py] => All params: 90471952
2025-11-11 12:27:38,407 [trainer.py] => Trainable params: 2439280
2025-11-11 12:27:38,407 [sema.py] => Learning on 60-70
2025-11-11 12:27:39,290 [sema_block.py] => Adapter 11.6 added at block 11
2025-11-11 12:31:19,825 [sema.py] => func Task 6, Epoch 5/5 => Loss 0.181, Train_accy 94.06, Test_accy 90.01
2025-11-11 12:46:27,452 [sema.py] => rd Task 6, Epoch 20/20 => Loss 0.474, Train_accy 94.50, Test_accy 90.01
2025-11-11 12:47:25,084 [trainer.py] => No NME accuracy.
2025-11-11 12:47:25,085 [trainer.py] => CNN: {'total': 90.01, '00-09': 90.7, '10-19': 86.6, '20-29': 93.2, '30-39': 87.3, '40-49': 89.5, '50-59': 88.1, '60-69': 94.7, 'old': 89.23, 'new': 94.7}
2025-11-11 12:47:25,085 [trainer.py] => CNN top1 curve: [98.6, 95.75, 94.3, 93.08, 91.24, 90.05, 90.01]
2025-11-11 12:47:25,086 [trainer.py] => CNN top5 curve: [100.0, 99.8, 99.63, 99.42, 99.38, 99.2, 99.11]

2025-11-11 12:47:25,086 [trainer.py] => Average Accuracy (CNN): 93.28999999999999 

2025-11-11 12:47:25,087 [trainer.py] => All params: 90694816
2025-11-11 12:47:25,088 [trainer.py] => Trainable params: 2439280
2025-11-11 12:47:25,088 [sema.py] => Learning on 70-80
2025-11-11 12:51:41,330 [sema.py] => func Task 7, Epoch 5/5 => Loss 0.207, Train_accy 93.30, Test_accy 88.00
2025-11-11 12:52:09,732 [trainer.py] => No NME accuracy.
2025-11-11 12:52:09,733 [trainer.py] => CNN: {'total': 88.0, '00-09': 90.4, '10-19': 86.5, '20-29': 93.0, '30-39': 87.0, '40-49': 86.0, '50-59': 83.7, '60-69': 91.6, '70-79': 85.8, 'old': 88.31, 'new': 85.8}
2025-11-11 12:52:09,733 [trainer.py] => CNN top1 curve: [98.6, 95.75, 94.3, 93.08, 91.24, 90.05, 90.01, 88.0]
2025-11-11 12:52:09,733 [trainer.py] => CNN top5 curve: [100.0, 99.8, 99.63, 99.42, 99.38, 99.2, 99.11, 98.98]

2025-11-11 12:52:09,734 [trainer.py] => Average Accuracy (CNN): 92.62875 

2025-11-11 12:52:09,735 [trainer.py] => All params: 90694816
2025-11-11 12:52:09,735 [trainer.py] => Trainable params: 2439280
2025-11-11 12:52:09,736 [sema.py] => Learning on 80-90
2025-11-11 12:52:10,514 [sema_block.py] => Adapter 11.7 added at block 11
2025-11-11 12:56:31,992 [sema.py] => func Task 8, Epoch 5/5 => Loss 0.223, Train_accy 92.72, Test_accy 87.14
2025-11-11 13:14:36,133 [sema.py] => rd Task 8, Epoch 20/20 => Loss 0.467, Train_accy 92.92, Test_accy 87.14
2025-11-11 13:15:21,903 [sema_block.py] => Adapter 10.1 added at block 10
2025-11-11 13:19:49,928 [sema.py] => func Task 8, Epoch 5/5 => Loss 0.202, Train_accy 93.66, Test_accy 87.01
2025-11-11 13:36:59,436 [sema.py] => rd Task 8, Epoch 20/20 => Loss 0.168, Train_accy 93.46, Test_accy 87.01
2025-11-11 13:37:56,541 [trainer.py] => No NME accuracy.
2025-11-11 13:37:56,541 [trainer.py] => CNN: {'total': 87.01, '00-09': 89.4, '10-19': 84.0, '20-29': 92.3, '30-39': 86.7, '40-49': 83.0, '50-59': 82.3, '60-69': 90.1, '70-79': 84.0, '80-89': 91.3, 'old': 86.48, 'new': 91.3}
2025-11-11 13:37:56,542 [trainer.py] => CNN top1 curve: [98.6, 95.75, 94.3, 93.08, 91.24, 90.05, 90.01, 88.0, 87.01]
2025-11-11 13:37:56,542 [trainer.py] => CNN top5 curve: [100.0, 99.8, 99.63, 99.42, 99.38, 99.2, 99.11, 98.98, 98.77]

2025-11-11 13:37:56,542 [trainer.py] => Average Accuracy (CNN): 92.00444444444445 

2025-11-11 13:37:56,543 [trainer.py] => All params: 91140544
2025-11-11 13:37:56,544 [trainer.py] => Trainable params: 2439280
2025-11-11 13:37:56,544 [sema.py] => Learning on 90-100
2025-11-11 13:37:57,273 [sema_block.py] => Adapter 11.8 added at block 11
2025-11-11 13:42:45,309 [sema.py] => func Task 9, Epoch 5/5 => Loss 0.235, Train_accy 92.02, Test_accy 86.62
2025-11-11 14:02:30,187 [sema.py] => rd Task 9, Epoch 20/20 => Loss 0.460, Train_accy 91.38, Test_accy 86.62
2025-11-11 14:04:02,758 [trainer.py] => No NME accuracy.
2025-11-11 14:04:02,758 [trainer.py] => CNN: {'total': 86.62, '00-09': 88.2, '10-19': 83.7, '20-29': 91.9, '30-39': 86.4, '40-49': 82.4, '50-59': 81.5, '60-69': 89.3, '70-79': 83.0, '80-89': 90.9, '90-99': 88.9, 'old': 86.37, 'new': 88.9}
2025-11-11 14:04:02,758 [trainer.py] => CNN top1 curve: [98.6, 95.75, 94.3, 93.08, 91.24, 90.05, 90.01, 88.0, 87.01, 86.62]
2025-11-11 14:04:02,758 [trainer.py] => CNN top5 curve: [100.0, 99.8, 99.63, 99.42, 99.38, 99.2, 99.11, 98.98, 98.77, 98.52]

2025-11-11 14:04:02,758 [trainer.py] => Average Accuracy (CNN): 91.466 

2025-11-13 10:29:31,517 [trainer.py] => config: exps/sema_cifar.json
2025-11-13 10:29:31,518 [trainer.py] => eval: False
2025-11-13 10:29:31,518 [trainer.py] => prefix: reproduce
2025-11-13 10:29:31,519 [trainer.py] => dataset: cifar224
2025-11-13 10:29:31,519 [trainer.py] => memory_size: 2000
2025-11-13 10:29:31,519 [trainer.py] => memory_per_class: None
2025-11-13 10:29:31,520 [trainer.py] => fixed_memory: False
2025-11-13 10:29:31,520 [trainer.py] => shuffle: True
2025-11-13 10:29:31,520 [trainer.py] => init_cls: 10
2025-11-13 10:29:31,520 [trainer.py] => increment: 10
2025-11-13 10:29:31,521 [trainer.py] => model_name: sema
2025-11-13 10:29:31,521 [trainer.py] => backbone_type: pretrained_vit_b16_224_adapter
2025-11-13 10:29:31,521 [trainer.py] => device: [device(type='cuda', index=0)]
2025-11-13 10:29:31,521 [trainer.py] => seed: 1993
2025-11-13 10:29:31,522 [trainer.py] => batch_size: 32
2025-11-13 10:29:31,522 [trainer.py] => weight_decay: 0.0005
2025-11-13 10:29:31,522 [trainer.py] => min_lr: 0
2025-11-13 10:29:31,522 [trainer.py] => ffn_adapter_type: adaptmlp
2025-11-13 10:29:31,522 [trainer.py] => ffn_num: 16
2025-11-13 10:29:31,523 [trainer.py] => optimizer: sgd
2025-11-13 10:29:31,523 [trainer.py] => vpt_type: shallow
2025-11-13 10:29:31,523 [trainer.py] => prompt_token_num: 5
2025-11-13 10:29:31,524 [trainer.py] => func_epoch: 5
2025-11-13 10:29:31,524 [trainer.py] => rd_epoch: 20
2025-11-13 10:29:31,524 [trainer.py] => init_lr: 0.005
2025-11-13 10:29:31,524 [trainer.py] => rd_lr: 0.01
2025-11-13 10:29:31,524 [trainer.py] => rd_dim: 128
2025-11-13 10:29:31,525 [trainer.py] => buffer_size: 500
2025-11-13 10:29:31,525 [trainer.py] => detect_batch_size: 128
2025-11-13 10:29:31,525 [trainer.py] => exp_threshold: 2
2025-11-13 10:29:31,525 [trainer.py] => exp_k_std: 3.0
2025-11-13 10:29:31,526 [trainer.py] => router_attn_dim: 128
2025-11-13 10:29:31,526 [trainer.py] => adapt_start_layer: 9
2025-11-13 10:29:31,526 [trainer.py] => adapt_end_layer: 11
2025-11-13 10:29:33,016 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-11-13 10:29:36,323 [sema_block.py] => Adapter 0.0 added at block 0
2025-11-13 10:29:36,372 [sema_block.py] => Adapter 1.0 added at block 1
2025-11-13 10:29:36,429 [sema_block.py] => Adapter 2.0 added at block 2
2025-11-13 10:29:36,482 [sema_block.py] => Adapter 3.0 added at block 3
2025-11-13 10:29:36,518 [sema_block.py] => Adapter 4.0 added at block 4
2025-11-13 10:29:36,553 [sema_block.py] => Adapter 5.0 added at block 5
2025-11-13 10:29:36,584 [sema_block.py] => Adapter 6.0 added at block 6
2025-11-13 10:29:36,617 [sema_block.py] => Adapter 7.0 added at block 7
2025-11-13 10:29:36,650 [sema_block.py] => Adapter 8.0 added at block 8
2025-11-13 10:29:36,698 [sema_block.py] => Adapter 9.0 added at block 9
2025-11-13 10:29:36,729 [sema_block.py] => Adapter 10.0 added at block 10
2025-11-13 10:29:36,774 [sema_block.py] => Adapter 11.0 added at block 11
2025-11-13 10:29:37,749 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-11-13 10:29:38,462 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-11-13 10:29:38,754 [trainer.py] => All params: 89057868
2025-11-13 10:29:38,756 [trainer.py] => Trainable params: 3259212
2025-11-13 10:29:38,759 [sema.py] => Learning on 0-10
2025-11-13 10:32:39,122 [sema.py] => func Task 0, Epoch 5/5 => Loss 2.550, Train_accy 9.26, Test_accy 12.10
2025-11-13 10:43:09,640 [trainer.py] => config: exps/sema_cifar.json
2025-11-13 10:43:09,640 [trainer.py] => eval: False
2025-11-13 10:43:09,641 [trainer.py] => prefix: reproduce
2025-11-13 10:43:09,641 [trainer.py] => dataset: cifar224
2025-11-13 10:43:09,641 [trainer.py] => memory_size: 2000
2025-11-13 10:43:09,641 [trainer.py] => memory_per_class: None
2025-11-13 10:43:09,642 [trainer.py] => fixed_memory: False
2025-11-13 10:43:09,642 [trainer.py] => shuffle: True
2025-11-13 10:43:09,642 [trainer.py] => init_cls: 10
2025-11-13 10:43:09,642 [trainer.py] => increment: 10
2025-11-13 10:43:09,643 [trainer.py] => model_name: sema
2025-11-13 10:43:09,643 [trainer.py] => backbone_type: pretrained_vit_b16_224_adapter
2025-11-13 10:43:09,643 [trainer.py] => device: [device(type='cuda', index=0)]
2025-11-13 10:43:09,644 [trainer.py] => seed: 1993
2025-11-13 10:43:09,644 [trainer.py] => batch_size: 32
2025-11-13 10:43:09,644 [trainer.py] => weight_decay: 0.0005
2025-11-13 10:43:09,644 [trainer.py] => min_lr: 0
2025-11-13 10:43:09,645 [trainer.py] => ffn_adapter_type: adaptmlp
2025-11-13 10:43:09,645 [trainer.py] => ffn_num: 16
2025-11-13 10:43:09,645 [trainer.py] => optimizer: sgd
2025-11-13 10:43:09,645 [trainer.py] => vpt_type: shallow
2025-11-13 10:43:09,646 [trainer.py] => prompt_token_num: 5
2025-11-13 10:43:09,646 [trainer.py] => func_epoch: 5
2025-11-13 10:43:09,646 [trainer.py] => rd_epoch: 20
2025-11-13 10:43:09,646 [trainer.py] => init_lr: 0.005
2025-11-13 10:43:09,647 [trainer.py] => rd_lr: 0.01
2025-11-13 10:43:09,647 [trainer.py] => rd_dim: 128
2025-11-13 10:43:09,647 [trainer.py] => buffer_size: 500
2025-11-13 10:43:09,647 [trainer.py] => detect_batch_size: 128
2025-11-13 10:43:09,648 [trainer.py] => exp_threshold: 2
2025-11-13 10:43:09,648 [trainer.py] => exp_k_std: 3.0
2025-11-13 10:43:09,648 [trainer.py] => router_attn_dim: 128
2025-11-13 10:43:09,648 [trainer.py] => adapt_start_layer: 9
2025-11-13 10:43:09,649 [trainer.py] => adapt_end_layer: 11
2025-11-13 10:43:11,012 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-11-13 10:43:12,377 [sema_block.py] => Adapter 0.0 added at block 0
2025-11-13 10:43:12,416 [sema_block.py] => Adapter 1.0 added at block 1
2025-11-13 10:43:12,463 [sema_block.py] => Adapter 2.0 added at block 2
2025-11-13 10:43:12,494 [sema_block.py] => Adapter 3.0 added at block 3
2025-11-13 10:43:12,541 [sema_block.py] => Adapter 4.0 added at block 4
2025-11-13 10:43:12,581 [sema_block.py] => Adapter 5.0 added at block 5
2025-11-13 10:43:12,630 [sema_block.py] => Adapter 6.0 added at block 6
2025-11-13 10:43:12,680 [sema_block.py] => Adapter 7.0 added at block 7
2025-11-13 10:43:12,711 [sema_block.py] => Adapter 8.0 added at block 8
2025-11-13 10:43:12,762 [sema_block.py] => Adapter 9.0 added at block 9
2025-11-13 10:43:12,799 [sema_block.py] => Adapter 10.0 added at block 10
2025-11-13 10:43:12,835 [sema_block.py] => Adapter 11.0 added at block 11
2025-11-13 10:43:13,549 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-11-13 10:43:13,951 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-11-13 10:43:14,108 [trainer.py] => All params: 89057868
2025-11-13 10:43:14,110 [trainer.py] => Trainable params: 3259212
2025-11-13 10:43:14,112 [sema.py] => Learning on 0-10
2025-11-13 10:45:15,245 [sema.py] => func Task 0, Epoch 5/5 => Loss 0.224, Train_accy 92.70, Test_accy 98.80
2025-11-13 11:00:55,462 [trainer.py] => config: exps/sema_cifar.json
2025-11-13 11:00:55,463 [trainer.py] => eval: False
2025-11-13 11:00:55,463 [trainer.py] => prefix: reproduce
2025-11-13 11:00:55,464 [trainer.py] => dataset: cifar224
2025-11-13 11:00:55,464 [trainer.py] => memory_size: 2000
2025-11-13 11:00:55,465 [trainer.py] => memory_per_class: None
2025-11-13 11:00:55,465 [trainer.py] => fixed_memory: False
2025-11-13 11:00:55,465 [trainer.py] => shuffle: True
2025-11-13 11:00:55,465 [trainer.py] => init_cls: 10
2025-11-13 11:00:55,466 [trainer.py] => increment: 10
2025-11-13 11:00:55,466 [trainer.py] => model_name: sema
2025-11-13 11:00:55,466 [trainer.py] => backbone_type: pretrained_vit_b16_224_adapter
2025-11-13 11:00:55,467 [trainer.py] => device: [device(type='cuda', index=0)]
2025-11-13 11:00:55,467 [trainer.py] => seed: 1993
2025-11-13 11:00:55,467 [trainer.py] => batch_size: 32
2025-11-13 11:00:55,468 [trainer.py] => weight_decay: 0.0005
2025-11-13 11:00:55,468 [trainer.py] => min_lr: 0
2025-11-13 11:00:55,469 [trainer.py] => ffn_adapter_type: adaptmlp
2025-11-13 11:00:55,469 [trainer.py] => ffn_num: 16
2025-11-13 11:00:55,469 [trainer.py] => optimizer: sgd
2025-11-13 11:00:55,470 [trainer.py] => vpt_type: shallow
2025-11-13 11:00:55,470 [trainer.py] => prompt_token_num: 5
2025-11-13 11:00:55,470 [trainer.py] => func_epoch: 5
2025-11-13 11:00:55,471 [trainer.py] => rd_epoch: 20
2025-11-13 11:00:55,471 [trainer.py] => init_lr: 0.005
2025-11-13 11:00:55,471 [trainer.py] => rd_lr: 0.01
2025-11-13 11:00:55,472 [trainer.py] => rd_dim: 128
2025-11-13 11:00:55,472 [trainer.py] => buffer_size: 500
2025-11-13 11:00:55,472 [trainer.py] => detect_batch_size: 128
2025-11-13 11:00:55,473 [trainer.py] => exp_threshold: 2
2025-11-13 11:00:55,473 [trainer.py] => exp_k_std: 3.0
2025-11-13 11:00:55,473 [trainer.py] => router_attn_dim: 128
2025-11-13 11:00:55,474 [trainer.py] => adapt_start_layer: 9
2025-11-13 11:00:55,474 [trainer.py] => adapt_end_layer: 11
2025-11-13 11:00:56,805 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-11-13 11:00:58,302 [sema_block.py] => Adapter 0.0 added at block 0
2025-11-13 11:00:58,334 [sema_block.py] => Adapter 1.0 added at block 1
2025-11-13 11:00:58,367 [sema_block.py] => Adapter 2.0 added at block 2
2025-11-13 11:00:58,405 [sema_block.py] => Adapter 3.0 added at block 3
2025-11-13 11:00:58,441 [sema_block.py] => Adapter 4.0 added at block 4
2025-11-13 11:00:58,478 [sema_block.py] => Adapter 5.0 added at block 5
2025-11-13 11:00:58,509 [sema_block.py] => Adapter 6.0 added at block 6
2025-11-13 11:00:58,538 [sema_block.py] => Adapter 7.0 added at block 7
2025-11-13 11:00:58,565 [sema_block.py] => Adapter 8.0 added at block 8
2025-11-13 11:00:58,594 [sema_block.py] => Adapter 9.0 added at block 9
2025-11-13 11:00:58,622 [sema_block.py] => Adapter 10.0 added at block 10
2025-11-13 11:00:58,652 [sema_block.py] => Adapter 11.0 added at block 11
2025-11-13 11:00:59,771 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-11-13 11:01:01,179 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-11-13 11:01:01,396 [trainer.py] => All params: 89057868
2025-11-13 11:01:01,398 [trainer.py] => Trainable params: 3259212
2025-11-13 11:01:01,400 [sema.py] => Learning on 0-10
2025-11-13 11:03:01,166 [sema.py] => func Task 0, Epoch 5/5 => Loss 0.224, Train_accy 92.70, Test_accy 98.80
2025-11-13 11:11:30,808 [sema.py] => rd Task 0, Epoch 20/20 => Loss 0.692, Train_accy 94.18, Test_accy 98.80
2025-11-13 11:11:34,788 [sema.py] => Building rehearsal memory...
2025-11-13 11:11:34,789 [base.py] => Reducing exemplars...(200 per classes)
2025-11-13 11:15:54,677 [trainer.py] => config: exps/sema_cifar.json
2025-11-13 11:15:54,677 [trainer.py] => eval: False
2025-11-13 11:15:54,677 [trainer.py] => prefix: reproduce
2025-11-13 11:15:54,678 [trainer.py] => dataset: cifar224
2025-11-13 11:15:54,678 [trainer.py] => memory_size: 2000
2025-11-13 11:15:54,678 [trainer.py] => memory_per_class: None
2025-11-13 11:15:54,679 [trainer.py] => fixed_memory: False
2025-11-13 11:15:54,679 [trainer.py] => shuffle: True
2025-11-13 11:15:54,679 [trainer.py] => init_cls: 10
2025-11-13 11:15:54,680 [trainer.py] => increment: 10
2025-11-13 11:15:54,680 [trainer.py] => model_name: sema
2025-11-13 11:15:54,680 [trainer.py] => backbone_type: pretrained_vit_b16_224_adapter
2025-11-13 11:15:54,680 [trainer.py] => device: [device(type='cuda', index=0)]
2025-11-13 11:15:54,681 [trainer.py] => seed: 1993
2025-11-13 11:15:54,681 [trainer.py] => batch_size: 32
2025-11-13 11:15:54,681 [trainer.py] => weight_decay: 0.0005
2025-11-13 11:15:54,681 [trainer.py] => min_lr: 0
2025-11-13 11:15:54,681 [trainer.py] => ffn_adapter_type: adaptmlp
2025-11-13 11:15:54,681 [trainer.py] => ffn_num: 16
2025-11-13 11:15:54,682 [trainer.py] => optimizer: sgd
2025-11-13 11:15:54,682 [trainer.py] => vpt_type: shallow
2025-11-13 11:15:54,682 [trainer.py] => prompt_token_num: 5
2025-11-13 11:15:54,682 [trainer.py] => func_epoch: 5
2025-11-13 11:15:54,683 [trainer.py] => rd_epoch: 20
2025-11-13 11:15:54,683 [trainer.py] => init_lr: 0.005
2025-11-13 11:15:54,683 [trainer.py] => rd_lr: 0.01
2025-11-13 11:15:54,683 [trainer.py] => rd_dim: 128
2025-11-13 11:15:54,684 [trainer.py] => buffer_size: 500
2025-11-13 11:15:54,684 [trainer.py] => detect_batch_size: 128
2025-11-13 11:15:54,684 [trainer.py] => exp_threshold: 2
2025-11-13 11:15:54,684 [trainer.py] => exp_k_std: 3.0
2025-11-13 11:15:54,684 [trainer.py] => router_attn_dim: 128
2025-11-13 11:15:54,685 [trainer.py] => adapt_start_layer: 9
2025-11-13 11:15:54,685 [trainer.py] => adapt_end_layer: 11
2025-11-13 11:15:56,179 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-11-13 11:15:57,975 [sema_block.py] => Adapter 0.0 added at block 0
2025-11-13 11:15:58,021 [sema_block.py] => Adapter 1.0 added at block 1
2025-11-13 11:15:58,061 [sema_block.py] => Adapter 2.0 added at block 2
2025-11-13 11:15:58,110 [sema_block.py] => Adapter 3.0 added at block 3
2025-11-13 11:15:58,148 [sema_block.py] => Adapter 4.0 added at block 4
2025-11-13 11:15:58,189 [sema_block.py] => Adapter 5.0 added at block 5
2025-11-13 11:15:58,232 [sema_block.py] => Adapter 6.0 added at block 6
2025-11-13 11:15:58,277 [sema_block.py] => Adapter 7.0 added at block 7
2025-11-13 11:15:58,322 [sema_block.py] => Adapter 8.0 added at block 8
2025-11-13 11:15:58,381 [sema_block.py] => Adapter 9.0 added at block 9
2025-11-13 11:15:58,439 [sema_block.py] => Adapter 10.0 added at block 10
2025-11-13 11:15:58,487 [sema_block.py] => Adapter 11.0 added at block 11
2025-11-13 11:15:59,912 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-11-13 11:16:00,629 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-11-13 11:16:00,777 [trainer.py] => All params: 89057868
2025-11-13 11:16:00,779 [trainer.py] => Trainable params: 3259212
2025-11-13 11:16:00,782 [sema.py] => Learning on 0-10
2025-11-13 11:17:59,995 [sema.py] => func Task 0, Epoch 5/5 => Loss 0.224, Train_accy 92.70, Test_accy 98.80
2025-11-13 11:26:24,230 [sema.py] => rd Task 0, Epoch 20/20 => Loss 0.692, Train_accy 94.18, Test_accy 98.80
2025-11-13 11:26:28,188 [sema.py] => Building rehearsal memory...
2025-11-13 11:26:28,189 [base.py] => Constructing exemplars...(200 per classes)
2025-11-13 11:26:28,189 [sema.py] => Memory size: 0
2025-11-13 11:26:28,189 [trainer.py] => No NME accuracy.
2025-11-13 11:26:28,190 [trainer.py] => CNN: {'total': 98.8, '00-09': 98.8, 'old': 0, 'new': 98.8}
2025-11-13 11:26:28,190 [trainer.py] => CNN top1 curve: [98.8]
2025-11-13 11:26:28,190 [trainer.py] => CNN top5 curve: [100.0]

2025-11-13 11:26:28,191 [trainer.py] => Average Accuracy (CNN): 98.8 

2025-11-13 11:26:28,192 [trainer.py] => All params: 89134768
2025-11-13 11:26:28,193 [trainer.py] => Trainable params: 2439280
2025-11-13 11:26:28,194 [sema.py] => Learning on 10-20
2025-11-13 11:26:28,256 [sema.py] => [DEBUG] Task 1: New data size: 5000, Memory Size: 0, Total Dataset Size: 5000
2025-11-13 11:26:29,477 [sema_block.py] => Adapter 11.1 added at block 11
2025-11-13 11:28:13,375 [sema.py] => func Task 1, Epoch 5/5 => Loss 0.257, Train_accy 91.26, Test_accy 82.60
2025-11-13 11:35:35,294 [sema.py] => rd Task 1, Epoch 20/20 => Loss 0.470, Train_accy 91.06, Test_accy 82.60
2025-11-13 11:35:45,699 [sema_block.py] => Adapter 10.1 added at block 10
2025-11-13 11:37:33,969 [sema.py] => func Task 1, Epoch 5/5 => Loss 0.260, Train_accy 91.22, Test_accy 80.90
2025-11-13 11:39:31,230 [trainer.py] => config: exps/sema_cifar.json
2025-11-13 11:39:31,231 [trainer.py] => eval: False
2025-11-13 11:39:31,231 [trainer.py] => prefix: reproduce
2025-11-13 11:39:31,231 [trainer.py] => dataset: cifar224
2025-11-13 11:39:31,232 [trainer.py] => memory_size: 2000
2025-11-13 11:39:31,232 [trainer.py] => memory_per_class: None
2025-11-13 11:39:31,232 [trainer.py] => fixed_memory: False
2025-11-13 11:39:31,233 [trainer.py] => shuffle: True
2025-11-13 11:39:31,233 [trainer.py] => init_cls: 10
2025-11-13 11:39:31,233 [trainer.py] => increment: 10
2025-11-13 11:39:31,233 [trainer.py] => model_name: sema
2025-11-13 11:39:31,234 [trainer.py] => backbone_type: pretrained_vit_b16_224_adapter
2025-11-13 11:39:31,234 [trainer.py] => device: [device(type='cuda', index=0)]
2025-11-13 11:39:31,234 [trainer.py] => seed: 1993
2025-11-13 11:39:31,234 [trainer.py] => batch_size: 32
2025-11-13 11:39:31,235 [trainer.py] => weight_decay: 0.0005
2025-11-13 11:39:31,235 [trainer.py] => min_lr: 0
2025-11-13 11:39:31,235 [trainer.py] => ffn_adapter_type: adaptmlp
2025-11-13 11:39:31,236 [trainer.py] => ffn_num: 16
2025-11-13 11:39:31,236 [trainer.py] => optimizer: sgd
2025-11-13 11:39:31,236 [trainer.py] => vpt_type: shallow
2025-11-13 11:39:31,236 [trainer.py] => prompt_token_num: 5
2025-11-13 11:39:31,237 [trainer.py] => func_epoch: 5
2025-11-13 11:39:31,237 [trainer.py] => rd_epoch: 20
2025-11-13 11:39:31,237 [trainer.py] => init_lr: 0.005
2025-11-13 11:39:31,238 [trainer.py] => rd_lr: 0.01
2025-11-13 11:39:31,238 [trainer.py] => rd_dim: 128
2025-11-13 11:39:31,238 [trainer.py] => buffer_size: 500
2025-11-13 11:39:31,239 [trainer.py] => detect_batch_size: 128
2025-11-13 11:39:31,239 [trainer.py] => exp_threshold: 2
2025-11-13 11:39:31,239 [trainer.py] => exp_k_std: 3.0
2025-11-13 11:39:31,240 [trainer.py] => router_attn_dim: 128
2025-11-13 11:39:31,240 [trainer.py] => adapt_start_layer: 9
2025-11-13 11:39:31,240 [trainer.py] => adapt_end_layer: 11
2025-11-13 11:39:32,601 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-11-13 11:39:34,749 [sema_block.py] => Adapter 0.0 added at block 0
2025-11-13 11:39:34,796 [sema_block.py] => Adapter 1.0 added at block 1
2025-11-13 11:39:34,848 [sema_block.py] => Adapter 2.0 added at block 2
2025-11-13 11:39:34,898 [sema_block.py] => Adapter 3.0 added at block 3
2025-11-13 11:39:34,941 [sema_block.py] => Adapter 4.0 added at block 4
2025-11-13 11:39:34,983 [sema_block.py] => Adapter 5.0 added at block 5
2025-11-13 11:39:35,024 [sema_block.py] => Adapter 6.0 added at block 6
2025-11-13 11:39:35,066 [sema_block.py] => Adapter 7.0 added at block 7
2025-11-13 11:39:35,106 [sema_block.py] => Adapter 8.0 added at block 8
2025-11-13 11:39:35,150 [sema_block.py] => Adapter 9.0 added at block 9
2025-11-13 11:39:35,193 [sema_block.py] => Adapter 10.0 added at block 10
2025-11-13 11:39:35,235 [sema_block.py] => Adapter 11.0 added at block 11
2025-11-13 11:39:36,614 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-11-13 11:39:38,060 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-11-13 11:39:38,747 [trainer.py] => All params: 89057868
2025-11-13 11:39:38,749 [trainer.py] => Trainable params: 3259212
2025-11-13 11:39:38,752 [sema.py] => Learning on 0-10
2025-11-13 11:41:35,747 [sema.py] => func Task 0, Epoch 5/5 => Loss 0.224, Train_accy 92.70, Test_accy 98.80
2025-11-13 11:49:46,456 [sema.py] => rd Task 0, Epoch 20/20 => Loss 0.692, Train_accy 94.18, Test_accy 98.80
2025-11-13 11:49:50,359 [sema.py] => Building rehearsal memory...
2025-11-13 11:49:50,359 [base.py] => Constructing exemplars...(200 per classes)
2025-11-13 11:49:50,360 [sema.py] => Memory size: 0
2025-11-13 11:49:50,360 [trainer.py] => No NME accuracy.
2025-11-13 11:49:50,361 [trainer.py] => CNN: {'total': 98.8, '00-09': 98.8, 'old': 0, 'new': 98.8}
2025-11-13 11:49:50,361 [trainer.py] => CNN top1 curve: [98.8]
2025-11-13 11:49:50,362 [trainer.py] => CNN top5 curve: [100.0]

2025-11-13 11:49:50,362 [trainer.py] => Average Accuracy (CNN): 98.8 

2025-11-13 11:49:50,364 [trainer.py] => All params: 89134768
2025-11-13 11:49:50,368 [trainer.py] => Trainable params: 2439280
2025-11-13 11:49:50,369 [sema.py] => Learning on 10-20
2025-11-13 11:49:50,393 [sema.py] => [DEBUG] Task 1: New data size: 5000, Memory Size: 0, Total Dataset Size: 5000
2025-11-13 11:49:51,458 [sema_block.py] => Adapter 11.1 added at block 11
2025-11-13 11:51:34,215 [sema.py] => func Task 1, Epoch 5/5 => Loss 0.257, Train_accy 91.26, Test_accy 82.60
2025-11-13 11:53:45,278 [trainer.py] => config: exps/sema_cifar.json
2025-11-13 11:53:45,280 [trainer.py] => eval: False
2025-11-13 11:53:45,280 [trainer.py] => prefix: reproduce
2025-11-13 11:53:45,281 [trainer.py] => dataset: cifar224
2025-11-13 11:53:45,281 [trainer.py] => memory_size: 2000
2025-11-13 11:53:45,281 [trainer.py] => memory_per_class: None
2025-11-13 11:53:45,282 [trainer.py] => fixed_memory: False
2025-11-13 11:53:45,282 [trainer.py] => shuffle: True
2025-11-13 11:53:45,282 [trainer.py] => init_cls: 10
2025-11-13 11:53:45,283 [trainer.py] => increment: 10
2025-11-13 11:53:45,283 [trainer.py] => model_name: sema
2025-11-13 11:53:45,284 [trainer.py] => backbone_type: pretrained_vit_b16_224_adapter
2025-11-13 11:53:45,284 [trainer.py] => device: [device(type='cuda', index=0)]
2025-11-13 11:53:45,284 [trainer.py] => seed: 1993
2025-11-13 11:53:45,285 [trainer.py] => batch_size: 32
2025-11-13 11:53:45,285 [trainer.py] => weight_decay: 0.0005
2025-11-13 11:53:45,285 [trainer.py] => min_lr: 0
2025-11-13 11:53:45,286 [trainer.py] => ffn_adapter_type: adaptmlp
2025-11-13 11:53:45,286 [trainer.py] => ffn_num: 16
2025-11-13 11:53:45,286 [trainer.py] => optimizer: sgd
2025-11-13 11:53:45,286 [trainer.py] => vpt_type: shallow
2025-11-13 11:53:45,287 [trainer.py] => prompt_token_num: 5
2025-11-13 11:53:45,288 [trainer.py] => func_epoch: 5
2025-11-13 11:53:45,288 [trainer.py] => rd_epoch: 20
2025-11-13 11:53:45,289 [trainer.py] => init_lr: 0.005
2025-11-13 11:53:45,290 [trainer.py] => rd_lr: 0.01
2025-11-13 11:53:45,290 [trainer.py] => rd_dim: 128
2025-11-13 11:53:45,291 [trainer.py] => buffer_size: 500
2025-11-13 11:53:45,291 [trainer.py] => detect_batch_size: 128
2025-11-13 11:53:45,291 [trainer.py] => exp_threshold: 2
2025-11-13 11:53:45,292 [trainer.py] => exp_k_std: 3.0
2025-11-13 11:53:45,292 [trainer.py] => router_attn_dim: 128
2025-11-13 11:53:45,292 [trainer.py] => adapt_start_layer: 9
2025-11-13 11:53:45,293 [trainer.py] => adapt_end_layer: 11
2025-11-13 11:53:46,636 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-11-13 11:53:49,529 [sema_block.py] => Adapter 0.0 added at block 0
2025-11-13 11:53:49,582 [sema_block.py] => Adapter 1.0 added at block 1
2025-11-13 11:53:49,670 [sema_block.py] => Adapter 2.0 added at block 2
2025-11-13 11:53:49,807 [sema_block.py] => Adapter 3.0 added at block 3
2025-11-13 11:53:49,899 [sema_block.py] => Adapter 4.0 added at block 4
2025-11-13 11:53:49,996 [sema_block.py] => Adapter 5.0 added at block 5
2025-11-13 11:53:50,102 [sema_block.py] => Adapter 6.0 added at block 6
2025-11-13 11:53:50,171 [sema_block.py] => Adapter 7.0 added at block 7
2025-11-13 11:53:50,208 [sema_block.py] => Adapter 8.0 added at block 8
2025-11-13 11:53:50,262 [sema_block.py] => Adapter 9.0 added at block 9
2025-11-13 11:53:50,309 [sema_block.py] => Adapter 10.0 added at block 10
2025-11-13 11:53:50,368 [sema_block.py] => Adapter 11.0 added at block 11
2025-11-13 11:53:51,857 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-11-13 11:53:52,402 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-11-13 11:53:52,707 [trainer.py] => All params: 89057868
2025-11-13 11:53:52,708 [trainer.py] => Trainable params: 3259212
2025-11-13 11:53:52,709 [sema.py] => Learning on 0-10
2025-11-13 11:55:49,508 [sema.py] => func Task 0, Epoch 5/5 => Loss 0.224, Train_accy 92.70, Test_accy 98.80
2025-11-13 12:04:03,685 [sema.py] => rd Task 0, Epoch 20/20 => Loss 0.692, Train_accy 94.18, Test_accy 98.80
2025-11-13 12:04:07,651 [sema.py] => Building rehearsal memory...
2025-11-13 12:04:07,651 [base.py] => Constructing exemplars...(200 per classes)
2025-11-13 12:04:07,651 [sema.py] => Memory size: 0
2025-11-13 12:04:07,652 [trainer.py] => No NME accuracy.
2025-11-13 12:04:07,652 [trainer.py] => CNN: {'total': 98.8, '00-09': 98.8, 'old': 0, 'new': 98.8}
2025-11-13 12:04:07,652 [trainer.py] => CNN top1 curve: [98.8]
2025-11-13 12:04:07,652 [trainer.py] => CNN top5 curve: [100.0]

2025-11-13 12:04:07,652 [trainer.py] => Average Accuracy (CNN): 98.8 

2025-11-13 12:04:07,653 [trainer.py] => All params: 89134768
2025-11-13 12:04:07,654 [trainer.py] => Trainable params: 2439280
2025-11-13 12:04:07,654 [sema.py] => Learning on 10-20
2025-11-13 12:04:07,727 [sema.py] => [DEBUG] Task 1: New data size: 5000, Memory Size: 0, Total Dataset Size: 5000
2025-11-13 12:04:08,559 [sema_block.py] => Adapter 11.1 added at block 11
2025-11-13 12:05:51,927 [sema.py] => func Task 1, Epoch 5/5 => Loss 0.257, Train_accy 91.26, Test_accy 82.60
2025-11-13 12:13:13,156 [sema.py] => rd Task 1, Epoch 20/20 => Loss 0.470, Train_accy 91.06, Test_accy 82.60
2025-11-13 12:13:23,230 [sema_block.py] => Adapter 10.1 added at block 10
2025-11-13 12:13:40,414 [trainer.py] => config: exps/sema_cifar.json
2025-11-13 12:13:40,414 [trainer.py] => eval: False
2025-11-13 12:13:40,415 [trainer.py] => prefix: reproduce
2025-11-13 12:13:40,415 [trainer.py] => dataset: cifar224
2025-11-13 12:13:40,415 [trainer.py] => memory_size: 2000
2025-11-13 12:13:40,415 [trainer.py] => memory_per_class: None
2025-11-13 12:13:40,416 [trainer.py] => fixed_memory: False
2025-11-13 12:13:40,416 [trainer.py] => shuffle: True
2025-11-13 12:13:40,417 [trainer.py] => init_cls: 10
2025-11-13 12:13:40,417 [trainer.py] => increment: 10
2025-11-13 12:13:40,417 [trainer.py] => model_name: sema
2025-11-13 12:13:40,418 [trainer.py] => backbone_type: pretrained_vit_b16_224_adapter
2025-11-13 12:13:40,418 [trainer.py] => device: [device(type='cuda', index=0)]
2025-11-13 12:13:40,418 [trainer.py] => seed: 1993
2025-11-13 12:13:40,419 [trainer.py] => batch_size: 32
2025-11-13 12:13:40,419 [trainer.py] => weight_decay: 0.0005
2025-11-13 12:13:40,419 [trainer.py] => min_lr: 0
2025-11-13 12:13:40,420 [trainer.py] => ffn_adapter_type: adaptmlp
2025-11-13 12:13:40,420 [trainer.py] => ffn_num: 16
2025-11-13 12:13:40,420 [trainer.py] => optimizer: sgd
2025-11-13 12:13:40,420 [trainer.py] => vpt_type: shallow
2025-11-13 12:13:40,420 [trainer.py] => prompt_token_num: 5
2025-11-13 12:13:40,421 [trainer.py] => func_epoch: 5
2025-11-13 12:13:40,421 [trainer.py] => rd_epoch: 20
2025-11-13 12:13:40,421 [trainer.py] => init_lr: 0.005
2025-11-13 12:13:40,421 [trainer.py] => rd_lr: 0.01
2025-11-13 12:13:40,422 [trainer.py] => rd_dim: 128
2025-11-13 12:13:40,422 [trainer.py] => buffer_size: 500
2025-11-13 12:13:40,422 [trainer.py] => detect_batch_size: 128
2025-11-13 12:13:40,422 [trainer.py] => exp_threshold: 2
2025-11-13 12:13:40,422 [trainer.py] => exp_k_std: 3.0
2025-11-13 12:13:40,422 [trainer.py] => router_attn_dim: 128
2025-11-13 12:13:40,423 [trainer.py] => adapt_start_layer: 9
2025-11-13 12:13:40,423 [trainer.py] => adapt_end_layer: 11
2025-11-13 12:13:41,814 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-11-13 12:13:44,087 [sema_block.py] => Adapter 0.0 added at block 0
2025-11-13 12:13:44,111 [sema_block.py] => Adapter 1.0 added at block 1
2025-11-13 12:13:44,136 [sema_block.py] => Adapter 2.0 added at block 2
2025-11-13 12:13:44,177 [sema_block.py] => Adapter 3.0 added at block 3
2025-11-13 12:13:44,226 [sema_block.py] => Adapter 4.0 added at block 4
2025-11-13 12:13:44,272 [sema_block.py] => Adapter 5.0 added at block 5
2025-11-13 12:13:44,334 [sema_block.py] => Adapter 6.0 added at block 6
2025-11-13 12:13:44,365 [sema_block.py] => Adapter 7.0 added at block 7
2025-11-13 12:13:44,396 [sema_block.py] => Adapter 8.0 added at block 8
2025-11-13 12:13:44,441 [sema_block.py] => Adapter 9.0 added at block 9
2025-11-13 12:13:44,477 [sema_block.py] => Adapter 10.0 added at block 10
2025-11-13 12:13:44,530 [sema_block.py] => Adapter 11.0 added at block 11
2025-11-13 12:13:45,853 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-11-13 12:13:46,604 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-11-13 12:13:47,133 [trainer.py] => All params: 89057868
2025-11-13 12:13:47,134 [trainer.py] => Trainable params: 3259212
2025-11-13 12:13:47,135 [sema.py] => Learning on 0-10
2025-11-13 12:15:44,560 [sema.py] => func Task 0, Epoch 5/5 => Loss 0.224, Train_accy 92.70, Test_accy 98.80
2025-11-13 12:23:55,159 [sema.py] => rd Task 0, Epoch 20/20 => Loss 0.692, Train_accy 94.18, Test_accy 98.80
2025-11-13 12:23:58,962 [sema.py] => Building rehearsal memory...
2025-11-13 12:23:58,963 [base.py] => Constructing exemplars...(200 per classes)
2025-11-17 09:38:22,158 [trainer.py] => config: exps/sema_cifar.json
2025-11-17 09:38:22,159 [trainer.py] => eval: False
2025-11-17 09:38:22,159 [trainer.py] => prefix: reproduce
2025-11-17 09:38:22,159 [trainer.py] => dataset: cifar224
2025-11-17 09:38:22,159 [trainer.py] => memory_size: 2000
2025-11-17 09:38:22,159 [trainer.py] => memory_per_class: None
2025-11-17 09:38:22,160 [trainer.py] => fixed_memory: False
2025-11-17 09:38:22,160 [trainer.py] => shuffle: True
2025-11-17 09:38:22,160 [trainer.py] => init_cls: 10
2025-11-17 09:38:22,160 [trainer.py] => increment: 10
2025-11-17 09:38:22,161 [trainer.py] => model_name: sema
2025-11-17 09:38:22,161 [trainer.py] => backbone_type: pretrained_vit_b16_224_adapter
2025-11-17 09:38:22,161 [trainer.py] => device: [device(type='cuda', index=0)]
2025-11-17 09:38:22,161 [trainer.py] => seed: 1993
2025-11-17 09:38:22,161 [trainer.py] => batch_size: 32
2025-11-17 09:38:22,161 [trainer.py] => weight_decay: 0.0005
2025-11-17 09:38:22,162 [trainer.py] => min_lr: 0
2025-11-17 09:38:22,162 [trainer.py] => ffn_adapter_type: adaptmlp
2025-11-17 09:38:22,162 [trainer.py] => ffn_num: 16
2025-11-17 09:38:22,162 [trainer.py] => optimizer: sgd
2025-11-17 09:38:22,162 [trainer.py] => vpt_type: shallow
2025-11-17 09:38:22,162 [trainer.py] => prompt_token_num: 5
2025-11-17 09:38:22,162 [trainer.py] => func_epoch: 5
2025-11-17 09:38:22,162 [trainer.py] => rd_epoch: 20
2025-11-17 09:38:22,162 [trainer.py] => init_lr: 0.005
2025-11-17 09:38:22,163 [trainer.py] => rd_lr: 0.01
2025-11-17 09:38:22,163 [trainer.py] => rd_dim: 128
2025-11-17 09:38:22,163 [trainer.py] => buffer_size: 500
2025-11-17 09:38:22,163 [trainer.py] => detect_batch_size: 128
2025-11-17 09:38:22,163 [trainer.py] => exp_threshold: 2
2025-11-17 09:38:22,163 [trainer.py] => exp_k_std: 3.0
2025-11-17 09:38:22,163 [trainer.py] => router_attn_dim: 128
2025-11-17 09:38:22,163 [trainer.py] => adapt_start_layer: 9
2025-11-17 09:38:22,163 [trainer.py] => adapt_end_layer: 11
2025-11-17 09:38:23,153 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-11-17 09:38:24,940 [sema_block.py] => Adapter 0.0 added at block 0
2025-11-17 09:38:24,968 [sema_block.py] => Adapter 1.0 added at block 1
2025-11-17 09:38:24,997 [sema_block.py] => Adapter 2.0 added at block 2
2025-11-17 09:38:25,027 [sema_block.py] => Adapter 3.0 added at block 3
2025-11-17 09:38:25,056 [sema_block.py] => Adapter 4.0 added at block 4
2025-11-17 09:38:25,085 [sema_block.py] => Adapter 5.0 added at block 5
2025-11-17 09:38:25,115 [sema_block.py] => Adapter 6.0 added at block 6
2025-11-17 09:38:25,144 [sema_block.py] => Adapter 7.0 added at block 7
2025-11-17 09:38:25,172 [sema_block.py] => Adapter 8.0 added at block 8
2025-11-17 09:38:25,202 [sema_block.py] => Adapter 9.0 added at block 9
2025-11-17 09:38:25,233 [sema_block.py] => Adapter 10.0 added at block 10
2025-11-17 09:38:25,263 [sema_block.py] => Adapter 11.0 added at block 11
2025-11-17 09:38:25,827 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-11-17 09:38:35,290 [_http.py] => '(MaxRetryError("HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /timm/vit_base_patch16_224.augreg2_in21k_ft_in1k/resolve/main/model.safetensors (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1016)')))"), '(Request ID: 93e1c1e2-d6eb-4a97-b841-9ccf1adbec0f)')' thrown while requesting HEAD https://huggingface.co/timm/vit_base_patch16_224.augreg2_in21k_ft_in1k/resolve/main/model.safetensors
2025-11-17 09:38:35,291 [_http.py] => Retrying in 1s [Retry 1/5].
2025-11-17 09:38:36,598 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-11-17 09:38:36,785 [trainer.py] => All params: 89057868
2025-11-17 09:38:36,786 [trainer.py] => Trainable params: 3259212
2025-11-17 09:38:36,789 [sema.py] => Learning on 0-10
2025-11-17 09:40:20,473 [sema.py] => func Task 0, Epoch 5/5 => Loss 0.224, Train_accy 92.70, Test_accy 98.80
2025-11-17 09:46:58,298 [sema.py] => rd Task 0, Epoch 20/20 => Loss 0.692, Train_accy 94.18, Test_accy 98.80
2025-11-17 09:47:03,461 [sema.py] => Building rehearsal memory...
2025-11-17 09:47:03,462 [base.py] => Constructing exemplars...(200 per classes)
2025-11-17 09:47:25,833 [sema.py] => Memory size: 2000
2025-11-17 09:47:25,834 [trainer.py] => No NME accuracy.
2025-11-17 09:47:25,834 [trainer.py] => CNN: {'total': 98.8, '00-09': 98.8, 'old': 0, 'new': 98.8}
2025-11-17 09:47:25,834 [trainer.py] => CNN top1 curve: [98.8]
2025-11-17 09:47:25,834 [trainer.py] => CNN top5 curve: [100.0]

2025-11-17 09:47:25,835 [trainer.py] => Average Accuracy (CNN): 98.8 

2025-11-17 09:47:25,835 [trainer.py] => All params: 89134768
2025-11-17 09:47:25,836 [trainer.py] => Trainable params: 2439280
2025-11-17 09:47:25,836 [sema.py] => Learning on 10-20
2025-11-17 09:47:25,875 [sema.py] => [DEBUG] Task 1: New data size: 5000, Memory Size: 2000, Total Dataset Size: 7000
2025-11-17 09:47:26,511 [sema_block.py] => Adapter 11.1 added at block 11
2025-11-17 09:49:06,538 [sema.py] => func Task 1, Epoch 5/5 => Loss 0.360, Train_accy 88.96, Test_accy 96.70
2025-11-17 09:54:55,421 [sema.py] => rd Task 1, Epoch 20/20 => Loss 0.427, Train_accy 88.72, Test_accy 96.70
2025-11-17 09:55:48,742 [sema.py] => Building rehearsal memory...
2025-11-17 09:55:48,743 [base.py] => Reducing exemplars...(100 per classes)
2025-11-17 09:55:52,841 [base.py] => Constructing exemplars...(100 per classes)
2025-11-17 09:56:14,223 [sema.py] => Memory size: 2000
2025-11-17 09:56:14,224 [trainer.py] => CNN: {'total': 96.7, '00-09': 96.6, '10-19': 96.8, 'old': 96.6, 'new': 96.8}
2025-11-17 09:56:14,224 [trainer.py] => NME: {'total': 49.1, '00-09': 98.2, '10-19': 0.0, 'old': 98.2, 'new': 0.0}
2025-11-17 09:56:14,224 [trainer.py] => CNN top1 curve: [98.8, 96.7]
2025-11-17 09:56:14,224 [trainer.py] => CNN top5 curve: [100.0, 99.75]
2025-11-17 09:56:14,224 [trainer.py] => NME top1 curve: [49.1]
2025-11-17 09:56:14,224 [trainer.py] => NME top5 curve: [49.9]

2025-11-17 09:56:14,225 [trainer.py] => Average Accuracy (CNN): 97.75
2025-11-17 09:56:14,225 [trainer.py] => Average Accuracy (NME): 49.1
2025-11-17 09:56:14,226 [trainer.py] => All params: 89357632
2025-11-17 09:56:14,226 [trainer.py] => Trainable params: 2439280
2025-11-17 09:56:14,226 [sema.py] => Learning on 20-30
2025-11-17 09:56:14,264 [sema.py] => [DEBUG] Task 2: New data size: 5000, Memory Size: 2000, Total Dataset Size: 7000
2025-11-17 09:56:14,890 [sema_block.py] => Adapter 11.2 added at block 11
2025-11-17 09:58:16,294 [sema.py] => func Task 2, Epoch 5/5 => Loss 0.331, Train_accy 90.00, Test_accy 95.57
2025-11-17 10:05:24,072 [sema.py] => rd Task 2, Epoch 20/20 => Loss 0.405, Train_accy 92.64, Test_accy 95.57
2025-11-17 10:05:24,716 [sema_block.py] => Adapter 10.1 added at block 10
2025-11-17 10:07:28,051 [sema.py] => func Task 2, Epoch 5/5 => Loss 0.330, Train_accy 90.49, Test_accy 95.40
2025-11-17 10:14:20,840 [sema.py] => rd Task 2, Epoch 20/20 => Loss 0.151, Train_accy 93.52, Test_accy 95.40
2025-11-17 10:14:42,835 [sema_block.py] => Adapter 9.1 added at block 9
2025-11-17 10:16:51,816 [sema.py] => func Task 2, Epoch 5/5 => Loss 0.300, Train_accy 91.11, Test_accy 95.30
2025-11-17 10:23:32,740 [sema.py] => rd Task 2, Epoch 20/20 => Loss 0.090, Train_accy 93.40, Test_accy 95.30
2025-11-17 10:25:00,282 [sema.py] => Building rehearsal memory...
2025-11-17 10:25:00,282 [base.py] => Reducing exemplars...(66 per classes)
2025-11-17 10:25:07,481 [base.py] => Constructing exemplars...(66 per classes)
2025-11-17 10:25:30,997 [sema.py] => Memory size: 1980
2025-11-17 10:25:30,998 [trainer.py] => CNN: {'total': 95.3, '00-09': 94.6, '10-19': 92.8, '20-29': 98.5, 'old': 93.7, 'new': 98.5}
2025-11-17 10:25:30,998 [trainer.py] => NME: {'total': 64.2, '00-09': 97.1, '10-19': 95.5, '20-29': 0.0, 'old': 96.3, 'new': 0.0}
2025-11-17 10:25:30,998 [trainer.py] => CNN top1 curve: [98.8, 96.7, 95.3]
2025-11-17 10:25:30,998 [trainer.py] => CNN top5 curve: [100.0, 99.75, 99.63]
2025-11-17 10:25:30,999 [trainer.py] => NME top1 curve: [49.1, 64.2]
2025-11-17 10:25:30,999 [trainer.py] => NME top5 curve: [49.9, 66.33]

2025-11-17 10:25:30,999 [trainer.py] => Average Accuracy (CNN): 96.93333333333334
2025-11-17 10:25:30,999 [trainer.py] => Average Accuracy (NME): 56.650000000000006
2025-11-17 10:25:31,000 [trainer.py] => All params: 90026224
2025-11-17 10:25:31,001 [trainer.py] => Trainable params: 2439280
2025-11-17 10:25:31,001 [sema.py] => Learning on 30-40
2025-11-17 10:25:31,044 [sema.py] => [DEBUG] Task 3: New data size: 5000, Memory Size: 1980, Total Dataset Size: 6980
2025-11-17 10:25:31,778 [sema_block.py] => Adapter 11.3 added at block 11
2025-11-17 10:28:24,765 [sema.py] => func Task 3, Epoch 5/5 => Loss 0.383, Train_accy 88.44, Test_accy 94.40
2025-11-17 10:38:43,723 [sema.py] => rd Task 3, Epoch 20/20 => Loss 0.426, Train_accy 90.64, Test_accy 94.40
2025-11-17 10:38:44,423 [sema_block.py] => Adapter 10.2 added at block 10
2025-11-17 10:41:26,323 [sema.py] => func Task 3, Epoch 5/5 => Loss 0.353, Train_accy 89.01, Test_accy 94.40
2025-11-17 10:50:50,433 [sema.py] => rd Task 3, Epoch 20/20 => Loss 0.151, Train_accy 90.80, Test_accy 94.40
2025-11-17 10:52:14,760 [sema.py] => Building rehearsal memory...
2025-11-17 10:52:14,760 [base.py] => Reducing exemplars...(50 per classes)
2025-11-17 10:52:23,714 [base.py] => Constructing exemplars...(50 per classes)
2025-11-17 10:52:46,539 [sema.py] => Memory size: 2000
2025-11-17 10:52:46,539 [trainer.py] => CNN: {'total': 94.4, '00-09': 93.0, '10-19': 92.9, '20-29': 94.3, '30-39': 97.4, 'old': 93.4, 'new': 97.4}
2025-11-17 10:52:46,540 [trainer.py] => NME: {'total': 71.05, '00-09': 95.6, '10-19': 93.4, '20-29': 95.2, '30-39': 0.0, 'old': 94.73, 'new': 0.0}
2025-11-17 10:52:46,540 [trainer.py] => CNN top1 curve: [98.8, 96.7, 95.3, 94.4]
2025-11-17 10:52:46,540 [trainer.py] => CNN top5 curve: [100.0, 99.75, 99.63, 99.38]
2025-11-17 10:52:46,540 [trainer.py] => NME top1 curve: [49.1, 64.2, 71.05]
2025-11-17 10:52:46,540 [trainer.py] => NME top5 curve: [49.9, 66.33, 74.65]

2025-11-17 10:52:46,541 [trainer.py] => Average Accuracy (CNN): 96.30000000000001
2025-11-17 10:52:46,541 [trainer.py] => Average Accuracy (NME): 61.45000000000001
2025-11-17 10:52:46,542 [trainer.py] => All params: 90471952
2025-11-17 10:52:46,542 [trainer.py] => Trainable params: 2439280
2025-11-17 10:52:46,543 [sema.py] => Learning on 40-50
2025-11-17 10:52:46,612 [sema.py] => [DEBUG] Task 4: New data size: 5000, Memory Size: 2000, Total Dataset Size: 7000
2025-11-17 10:52:47,733 [sema_block.py] => Adapter 10.3 added at block 10
2025-11-17 10:55:47,983 [sema.py] => func Task 4, Epoch 5/5 => Loss 0.386, Train_accy 88.69, Test_accy 92.24
2025-11-17 11:06:43,607 [sema.py] => rd Task 4, Epoch 20/20 => Loss 0.150, Train_accy 93.08, Test_accy 92.24
2025-11-17 11:06:44,551 [sema_block.py] => Adapter 11.4 added at block 11
2025-11-17 11:10:31,064 [sema.py] => func Task 4, Epoch 5/5 => Loss 0.348, Train_accy 90.24, Test_accy 92.50
2025-11-17 11:22:47,483 [sema.py] => rd Task 4, Epoch 20/20 => Loss 0.424, Train_accy 93.00, Test_accy 92.50
2025-11-17 11:26:42,759 [sema.py] => Building rehearsal memory...
2025-11-17 11:26:42,760 [base.py] => Reducing exemplars...(40 per classes)
2025-11-17 11:26:52,935 [base.py] => Constructing exemplars...(40 per classes)
2025-11-17 11:27:17,954 [sema.py] => Memory size: 2000
2025-11-17 11:27:17,955 [trainer.py] => CNN: {'total': 92.5, '00-09': 92.6, '10-19': 88.4, '20-29': 93.0, '30-39': 90.6, '40-49': 97.9, 'old': 91.15, 'new': 97.9}
2025-11-17 11:27:17,955 [trainer.py] => NME: {'total': 74.9, '00-09': 94.7, '10-19': 92.6, '20-29': 94.1, '30-39': 93.1, '40-49': 0.0, 'old': 93.62, 'new': 0.0}
2025-11-17 11:27:17,955 [trainer.py] => CNN top1 curve: [98.8, 96.7, 95.3, 94.4, 92.5]
2025-11-17 11:27:17,955 [trainer.py] => CNN top5 curve: [100.0, 99.75, 99.63, 99.38, 99.28]
2025-11-17 11:27:17,956 [trainer.py] => NME top1 curve: [49.1, 64.2, 71.05, 74.9]
2025-11-17 11:27:17,956 [trainer.py] => NME top5 curve: [49.9, 66.33, 74.65, 79.56]

2025-11-17 11:27:17,956 [trainer.py] => Average Accuracy (CNN): 95.54
2025-11-17 11:27:17,956 [trainer.py] => Average Accuracy (NME): 64.8125
2025-11-17 11:27:17,957 [trainer.py] => All params: 90917680
2025-11-17 11:27:17,957 [trainer.py] => Trainable params: 2439280
2025-11-17 11:27:17,958 [sema.py] => Learning on 50-60
2025-11-17 11:27:18,013 [sema.py] => [DEBUG] Task 5: New data size: 5000, Memory Size: 2000, Total Dataset Size: 7000
2025-11-17 11:27:19,486 [sema_block.py] => Adapter 10.4 added at block 10
2025-11-17 11:30:46,256 [sema.py] => func Task 5, Epoch 5/5 => Loss 0.433, Train_accy 87.36, Test_accy 90.73
2025-11-17 11:43:22,828 [sema.py] => rd Task 5, Epoch 20/20 => Loss 0.153, Train_accy 91.00, Test_accy 90.73
2025-11-17 11:43:24,503 [sema_block.py] => Adapter 11.5 added at block 11
2025-11-17 11:47:02,608 [sema.py] => func Task 5, Epoch 5/5 => Loss 0.399, Train_accy 88.24, Test_accy 91.00
2025-11-17 12:00:26,311 [sema.py] => rd Task 5, Epoch 20/20 => Loss 0.441, Train_accy 91.48, Test_accy 91.00
2025-11-17 12:02:59,632 [sema_block.py] => Adapter 9.2 added at block 9
2025-11-17 12:11:22,999 [sema.py] => func Task 5, Epoch 5/5 => Loss 0.385, Train_accy 88.63, Test_accy 90.87
2025-11-17 12:26:46,860 [sema.py] => rd Task 5, Epoch 20/20 => Loss 0.090, Train_accy 92.02, Test_accy 90.87
2025-11-17 12:29:37,791 [sema.py] => Building rehearsal memory...
2025-11-17 12:29:37,792 [base.py] => Reducing exemplars...(33 per classes)
2025-11-17 12:29:50,107 [base.py] => Constructing exemplars...(33 per classes)
2025-11-17 12:30:17,359 [sema.py] => Memory size: 1980
2025-11-17 12:30:17,360 [trainer.py] => CNN: {'total': 90.87, '00-09': 89.2, '10-19': 89.4, '20-29': 91.1, '30-39': 90.2, '40-49': 88.6, '50-59': 96.7, 'old': 89.7, 'new': 96.7}
2025-11-17 12:30:17,360 [trainer.py] => NME: {'total': 76.67, '00-09': 93.9, '10-19': 90.7, '20-29': 92.6, '30-39': 90.5, '40-49': 92.3, '50-59': 0.0, 'old': 92.0, 'new': 0.0}
2025-11-17 12:30:17,360 [trainer.py] => CNN top1 curve: [98.8, 96.7, 95.3, 94.4, 92.5, 90.87]
2025-11-17 12:30:17,361 [trainer.py] => CNN top5 curve: [100.0, 99.75, 99.63, 99.38, 99.28, 99.07]
2025-11-17 12:30:17,361 [trainer.py] => NME top1 curve: [49.1, 64.2, 71.05, 74.9, 76.67]
2025-11-17 12:30:17,361 [trainer.py] => NME top5 curve: [49.9, 66.33, 74.65, 79.56, 82.7]

2025-11-17 12:30:17,361 [trainer.py] => Average Accuracy (CNN): 94.76166666666667
2025-11-17 12:30:17,361 [trainer.py] => Average Accuracy (NME): 67.184
2025-11-17 12:30:17,362 [trainer.py] => All params: 91586272
2025-11-17 12:30:17,363 [trainer.py] => Trainable params: 2439280
2025-11-17 12:30:17,363 [sema.py] => Learning on 60-70
2025-11-17 12:30:17,428 [sema.py] => [DEBUG] Task 6: New data size: 5000, Memory Size: 1980, Total Dataset Size: 6980
2025-11-17 12:30:18,541 [sema_block.py] => Adapter 11.6 added at block 11
2025-11-17 12:34:11,850 [sema.py] => func Task 6, Epoch 5/5 => Loss 0.352, Train_accy 90.52, Test_accy 90.60
2025-11-17 12:49:03,287 [sema.py] => rd Task 6, Epoch 20/20 => Loss 0.430, Train_accy 93.36, Test_accy 90.60
2025-11-17 12:49:03,686 [sema_block.py] => Adapter 10.5 added at block 10
2025-11-17 12:53:01,860 [sema.py] => func Task 6, Epoch 5/5 => Loss 0.341, Train_accy 90.47, Test_accy 90.51
2025-11-17 13:08:45,116 [sema.py] => rd Task 6, Epoch 20/20 => Loss 0.157, Train_accy 93.48, Test_accy 90.51
2025-11-17 13:08:45,441 [sema_block.py] => Adapter 9.3 added at block 9
2025-11-17 13:14:21,904 [sema.py] => func Task 6, Epoch 5/5 => Loss 0.328, Train_accy 90.72, Test_accy 90.41
2025-11-17 13:33:53,701 [sema.py] => rd Task 6, Epoch 20/20 => Loss 0.094, Train_accy 94.28, Test_accy 90.41
2025-11-17 13:39:58,870 [sema.py] => Building rehearsal memory...
2025-11-17 13:39:58,870 [base.py] => Reducing exemplars...(28 per classes)
2025-11-17 13:40:20,398 [base.py] => Constructing exemplars...(28 per classes)
2025-11-17 13:40:54,209 [sema.py] => Memory size: 1960
2025-11-17 13:40:54,210 [trainer.py] => CNN: {'total': 90.41, '00-09': 89.6, '10-19': 87.6, '20-29': 92.2, '30-39': 90.0, '40-49': 90.4, '50-59': 84.4, '60-69': 98.7, 'old': 89.03, 'new': 98.7}
2025-11-17 13:40:54,210 [trainer.py] => NME: {'total': 77.74, '00-09': 93.0, '10-19': 90.3, '20-29': 91.4, '30-39': 90.0, '40-49': 91.2, '50-59': 88.3, '60-69': 0.0, 'old': 90.7, 'new': 0.0}
2025-11-17 13:40:54,211 [trainer.py] => CNN top1 curve: [98.8, 96.7, 95.3, 94.4, 92.5, 90.87, 90.41]
2025-11-17 13:40:54,211 [trainer.py] => CNN top5 curve: [100.0, 99.75, 99.63, 99.38, 99.28, 99.07, 99.0]
2025-11-17 13:40:54,211 [trainer.py] => NME top1 curve: [49.1, 64.2, 71.05, 74.9, 76.67, 77.74]
2025-11-17 13:40:54,211 [trainer.py] => NME top5 curve: [49.9, 66.33, 74.65, 79.56, 82.7, 84.87]

2025-11-17 13:40:54,212 [trainer.py] => Average Accuracy (CNN): 94.14
2025-11-17 13:40:54,212 [trainer.py] => Average Accuracy (NME): 68.94333333333334
2025-11-17 13:40:54,215 [trainer.py] => All params: 92254864
2025-11-17 13:40:54,219 [trainer.py] => Trainable params: 2439280
2025-11-17 13:40:54,222 [sema.py] => Learning on 70-80
2025-11-17 13:40:54,350 [sema.py] => [DEBUG] Task 7: New data size: 5000, Memory Size: 1960, Total Dataset Size: 6960
2025-11-17 13:41:14,191 [sema_block.py] => Adapter 10.6 added at block 10
2025-11-17 13:47:18,441 [sema.py] => func Task 7, Epoch 5/5 => Loss 0.438, Train_accy 87.49, Test_accy 87.25
2025-11-17 14:10:39,412 [sema.py] => rd Task 7, Epoch 20/20 => Loss 0.154, Train_accy 91.28, Test_accy 87.25
2025-11-17 14:10:40,283 [sema_block.py] => Adapter 11.7 added at block 11
2025-11-17 14:19:33,198 [sema.py] => func Task 7, Epoch 5/5 => Loss 0.413, Train_accy 88.74, Test_accy 87.38
2025-11-17 14:50:14,778 [sema.py] => rd Task 7, Epoch 20/20 => Loss 0.447, Train_accy 91.46, Test_accy 87.38
2025-11-17 14:56:36,085 [sema.py] => Building rehearsal memory...
2025-11-17 14:56:36,086 [base.py] => Reducing exemplars...(25 per classes)
2025-11-17 14:56:59,634 [base.py] => Constructing exemplars...(25 per classes)
2025-11-17 14:57:28,813 [sema.py] => Memory size: 2000
2025-11-17 14:57:28,814 [trainer.py] => CNN: {'total': 87.38, '00-09': 87.7, '10-19': 88.9, '20-29': 91.8, '30-39': 88.1, '40-49': 82.0, '50-59': 74.5, '60-69': 89.8, '70-79': 96.2, 'old': 86.11, 'new': 96.2}
2025-11-17 14:57:28,815 [trainer.py] => NME: {'total': 79.09, '00-09': 93.0, '10-19': 90.0, '20-29': 91.5, '30-39': 90.0, '40-49': 90.6, '50-59': 85.6, '60-69': 92.0, '70-79': 0.0, 'old': 90.39, 'new': 0.0}
2025-11-17 14:57:28,815 [trainer.py] => CNN top1 curve: [98.8, 96.7, 95.3, 94.4, 92.5, 90.87, 90.41, 87.38]
2025-11-17 14:57:28,816 [trainer.py] => CNN top5 curve: [100.0, 99.75, 99.63, 99.38, 99.28, 99.07, 99.0, 98.64]
2025-11-17 14:57:28,816 [trainer.py] => NME top1 curve: [49.1, 64.2, 71.05, 74.9, 76.67, 77.74, 79.09]
2025-11-17 14:57:28,817 [trainer.py] => NME top5 curve: [49.9, 66.33, 74.65, 79.56, 82.7, 84.87, 86.64]

2025-11-17 14:57:28,817 [trainer.py] => Average Accuracy (CNN): 93.295
2025-11-17 14:57:28,817 [trainer.py] => Average Accuracy (NME): 70.39285714285714
2025-11-17 14:57:28,819 [trainer.py] => All params: 92700592
2025-11-17 14:57:28,820 [trainer.py] => Trainable params: 2439280
2025-11-17 14:57:28,821 [sema.py] => Learning on 80-90
2025-11-17 14:57:29,121 [sema.py] => [DEBUG] Task 8: New data size: 5000, Memory Size: 2000, Total Dataset Size: 7000
2025-11-17 14:57:30,935 [sema_block.py] => Adapter 11.8 added at block 11
2025-11-17 15:34:16,978 [sema.py] => func Task 8, Epoch 5/5 => Loss 0.431, Train_accy 87.93, Test_accy 86.76
2025-11-17 18:06:20,722 [sema.py] => rd Task 8, Epoch 20/20 => Loss 0.426, Train_accy 91.28, Test_accy 86.76
2025-11-17 18:06:44,736 [sema_block.py] => Adapter 10.7 added at block 10
2025-11-17 18:13:51,713 [sema.py] => func Task 8, Epoch 5/5 => Loss 0.396, Train_accy 88.94, Test_accy 86.74
2025-11-17 18:37:07,917 [sema.py] => rd Task 8, Epoch 20/20 => Loss 0.153, Train_accy 92.56, Test_accy 86.74
2025-11-17 18:41:01,061 [sema.py] => Building rehearsal memory...
2025-11-17 18:41:01,062 [base.py] => Reducing exemplars...(22 per classes)
2025-11-17 18:41:18,323 [base.py] => Constructing exemplars...(22 per classes)
2025-11-17 18:41:52,548 [sema.py] => Memory size: 1980
2025-11-17 18:41:52,549 [trainer.py] => CNN: {'total': 86.74, '00-09': 88.1, '10-19': 82.3, '20-29': 88.9, '30-39': 88.6, '40-49': 81.8, '50-59': 79.6, '60-69': 89.7, '70-79': 84.5, '80-89': 97.2, 'old': 85.44, 'new': 97.2}
2025-11-17 18:41:52,549 [trainer.py] => NME: {'total': 78.33, '00-09': 92.2, '10-19': 88.9, '20-29': 91.4, '30-39': 89.3, '40-49': 88.0, '50-59': 82.5, '60-69': 90.4, '70-79': 82.3, '80-89': 0.0, 'old': 88.12, 'new': 0.0}
2025-11-17 18:41:52,550 [trainer.py] => CNN top1 curve: [98.8, 96.7, 95.3, 94.4, 92.5, 90.87, 90.41, 87.38, 86.74]
2025-11-17 18:41:52,550 [trainer.py] => CNN top5 curve: [100.0, 99.75, 99.63, 99.38, 99.28, 99.07, 99.0, 98.64, 98.32]
2025-11-17 18:41:52,550 [trainer.py] => NME top1 curve: [49.1, 64.2, 71.05, 74.9, 76.67, 77.74, 79.09, 78.33]
2025-11-17 18:41:52,550 [trainer.py] => NME top5 curve: [49.9, 66.33, 74.65, 79.56, 82.7, 84.87, 86.64, 87.88]

2025-11-17 18:41:52,551 [trainer.py] => Average Accuracy (CNN): 92.56666666666666
2025-11-17 18:41:52,551 [trainer.py] => Average Accuracy (NME): 71.385
2025-11-17 18:41:52,552 [trainer.py] => All params: 93146320
2025-11-17 18:41:52,553 [trainer.py] => Trainable params: 2439280
2025-11-17 18:41:52,553 [sema.py] => Learning on 90-100
2025-11-17 18:41:52,684 [sema.py] => [DEBUG] Task 9: New data size: 5000, Memory Size: 1980, Total Dataset Size: 6980
2025-11-17 18:41:53,990 [sema_block.py] => Adapter 11.9 added at block 11
2025-11-17 18:47:40,229 [sema.py] => func Task 9, Epoch 5/5 => Loss 0.427, Train_accy 88.11, Test_accy 86.72
2025-11-17 19:09:44,488 [sema.py] => rd Task 9, Epoch 20/20 => Loss 0.409, Train_accy 91.28, Test_accy 86.72
2025-11-17 19:09:59,394 [sema_block.py] => Adapter 9.4 added at block 9
2025-11-17 19:17:14,206 [sema.py] => func Task 9, Epoch 5/5 => Loss 0.417, Train_accy 88.87, Test_accy 86.74
2025-11-17 19:40:42,378 [sema.py] => rd Task 9, Epoch 20/20 => Loss 0.095, Train_accy 91.40, Test_accy 86.74
2025-11-17 19:40:58,479 [sema_block.py] => Adapter 10.8 added at block 10
2025-11-17 19:49:38,840 [sema.py] => func Task 9, Epoch 5/5 => Loss 0.395, Train_accy 88.98, Test_accy 86.90
2025-11-17 20:17:54,410 [sema.py] => rd Task 9, Epoch 20/20 => Loss 0.150, Train_accy 92.26, Test_accy 86.90
2025-11-17 20:21:52,454 [sema.py] => Building rehearsal memory...
2025-11-17 20:21:52,455 [base.py] => Reducing exemplars...(20 per classes)
2025-11-17 20:22:11,079 [base.py] => Constructing exemplars...(20 per classes)
2025-11-17 20:22:59,624 [sema.py] => Memory size: 2000
2025-11-17 20:22:59,625 [trainer.py] => CNN: {'total': 86.9, '00-09': 86.1, '10-19': 86.8, '20-29': 90.5, '30-39': 87.1, '40-49': 81.3, '50-59': 79.9, '60-69': 88.6, '70-79': 81.7, '80-89': 90.5, '90-99': 96.5, 'old': 85.83, 'new': 96.5}
2025-11-17 20:22:59,625 [trainer.py] => NME: {'total': 78.86, '00-09': 91.9, '10-19': 88.2, '20-29': 90.4, '30-39': 89.7, '40-49': 86.5, '50-59': 82.2, '60-69': 88.7, '70-79': 81.7, '80-89': 89.3, '90-99': 0.0, 'old': 87.62, 'new': 0.0}
2025-11-17 20:22:59,625 [trainer.py] => CNN top1 curve: [98.8, 96.7, 95.3, 94.4, 92.5, 90.87, 90.41, 87.38, 86.74, 86.9]
2025-11-17 20:22:59,625 [trainer.py] => CNN top5 curve: [100.0, 99.75, 99.63, 99.38, 99.28, 99.07, 99.0, 98.64, 98.32, 98.06]
2025-11-17 20:22:59,625 [trainer.py] => NME top1 curve: [49.1, 64.2, 71.05, 74.9, 76.67, 77.74, 79.09, 78.33, 78.86]
2025-11-17 20:22:59,626 [trainer.py] => NME top5 curve: [49.9, 66.33, 74.65, 79.56, 82.7, 84.87, 86.64, 87.88, 88.71]

2025-11-17 20:22:59,626 [trainer.py] => Average Accuracy (CNN): 92.0
2025-11-17 20:22:59,626 [trainer.py] => Average Accuracy (NME): 72.21555555555557
2025-11-18 08:34:52,949 [trainer.py] => config: exps/sema_cifar.json
2025-11-18 08:34:52,949 [trainer.py] => eval: False
2025-11-18 08:34:52,950 [trainer.py] => prefix: reproduce
2025-11-18 08:34:52,950 [trainer.py] => dataset: cifar224
2025-11-18 08:34:52,950 [trainer.py] => memory_size: 2000
2025-11-18 08:34:52,950 [trainer.py] => memory_per_class: None
2025-11-18 08:34:52,950 [trainer.py] => fixed_memory: False
2025-11-18 08:34:52,951 [trainer.py] => shuffle: True
2025-11-18 08:34:52,951 [trainer.py] => init_cls: 10
2025-11-18 08:34:52,951 [trainer.py] => increment: 10
2025-11-18 08:34:52,951 [trainer.py] => model_name: sema
2025-11-18 08:34:52,951 [trainer.py] => backbone_type: pretrained_vit_b16_224_adapter
2025-11-18 08:34:52,951 [trainer.py] => device: [device(type='cuda', index=0)]
2025-11-18 08:34:52,952 [trainer.py] => seed: 1993
2025-11-18 08:34:52,952 [trainer.py] => batch_size: 32
2025-11-18 08:34:52,952 [trainer.py] => weight_decay: 0.0005
2025-11-18 08:34:52,952 [trainer.py] => min_lr: 0
2025-11-18 08:34:52,952 [trainer.py] => ffn_adapter_type: adaptmlp
2025-11-18 08:34:52,952 [trainer.py] => ffn_num: 16
2025-11-18 08:34:52,953 [trainer.py] => optimizer: sgd
2025-11-18 08:34:52,953 [trainer.py] => vpt_type: shallow
2025-11-18 08:34:52,953 [trainer.py] => prompt_token_num: 5
2025-11-18 08:34:52,953 [trainer.py] => func_epoch: 5
2025-11-18 08:34:52,953 [trainer.py] => rd_epoch: 20
2025-11-18 08:34:52,953 [trainer.py] => init_lr: 0.005
2025-11-18 08:34:52,953 [trainer.py] => rd_lr: 0.01
2025-11-18 08:34:52,954 [trainer.py] => rd_dim: 128
2025-11-18 08:34:52,954 [trainer.py] => buffer_size: 500
2025-11-18 08:34:52,954 [trainer.py] => detect_batch_size: 128
2025-11-18 08:34:52,954 [trainer.py] => exp_threshold: 2
2025-11-18 08:34:52,954 [trainer.py] => exp_k_std: 3.0
2025-11-18 08:34:52,954 [trainer.py] => router_attn_dim: 128
2025-11-18 08:34:52,954 [trainer.py] => adapt_start_layer: 9
2025-11-18 08:34:52,955 [trainer.py] => adapt_end_layer: 11
2025-11-18 08:34:53,871 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-11-18 08:34:55,582 [sema_block.py] => Adapter 0.0 added at block 0
2025-11-18 08:34:55,607 [sema_block.py] => Adapter 1.0 added at block 1
2025-11-18 08:34:55,632 [sema_block.py] => Adapter 2.0 added at block 2
2025-11-18 08:34:55,659 [sema_block.py] => Adapter 3.0 added at block 3
2025-11-18 08:34:55,684 [sema_block.py] => Adapter 4.0 added at block 4
2025-11-18 08:34:55,711 [sema_block.py] => Adapter 5.0 added at block 5
2025-11-18 08:34:55,736 [sema_block.py] => Adapter 6.0 added at block 6
2025-11-18 08:34:55,761 [sema_block.py] => Adapter 7.0 added at block 7
2025-11-18 08:34:55,786 [sema_block.py] => Adapter 8.0 added at block 8
2025-11-18 08:34:55,812 [sema_block.py] => Adapter 9.0 added at block 9
2025-11-18 08:34:55,840 [sema_block.py] => Adapter 10.0 added at block 10
2025-11-18 08:34:55,867 [sema_block.py] => Adapter 11.0 added at block 11
2025-11-18 08:34:56,393 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-11-18 08:34:57,969 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-11-18 08:34:58,188 [trainer.py] => All params: 89057868
2025-11-18 08:34:58,188 [trainer.py] => Trainable params: 3259212
2025-11-18 08:34:58,190 [sema.py] => Learning on 0-10
2025-11-18 08:36:36,654 [sema.py] => func Task 0, Epoch 5/5 => Loss 0.224, Train_accy 92.70, Test_accy 98.80
2025-11-18 08:40:50,185 [trainer.py] => config: exps/sema_cifar.json
2025-11-18 08:40:50,185 [trainer.py] => eval: False
2025-11-18 08:40:50,185 [trainer.py] => prefix: reproduce
2025-11-18 08:40:50,186 [trainer.py] => dataset: cifar224
2025-11-18 08:40:50,186 [trainer.py] => memory_size: 2000
2025-11-18 08:40:50,186 [trainer.py] => memory_per_class: None
2025-11-18 08:40:50,186 [trainer.py] => fixed_memory: False
2025-11-18 08:40:50,186 [trainer.py] => shuffle: True
2025-11-18 08:40:50,186 [trainer.py] => init_cls: 10
2025-11-18 08:40:50,187 [trainer.py] => increment: 10
2025-11-18 08:40:50,187 [trainer.py] => model_name: sema
2025-11-18 08:40:50,187 [trainer.py] => backbone_type: pretrained_vit_b16_224_adapter
2025-11-18 08:40:50,187 [trainer.py] => device: [device(type='cuda', index=0)]
2025-11-18 08:40:50,187 [trainer.py] => seed: 1993
2025-11-18 08:40:50,187 [trainer.py] => batch_size: 32
2025-11-18 08:40:50,188 [trainer.py] => weight_decay: 0.0005
2025-11-18 08:40:50,188 [trainer.py] => min_lr: 0
2025-11-18 08:40:50,188 [trainer.py] => ffn_adapter_type: adaptmlp
2025-11-18 08:40:50,188 [trainer.py] => ffn_num: 16
2025-11-18 08:40:50,188 [trainer.py] => optimizer: sgd
2025-11-18 08:40:50,188 [trainer.py] => vpt_type: shallow
2025-11-18 08:40:50,188 [trainer.py] => prompt_token_num: 5
2025-11-18 08:40:50,189 [trainer.py] => func_epoch: 5
2025-11-18 08:40:50,189 [trainer.py] => rd_epoch: 20
2025-11-18 08:40:50,189 [trainer.py] => init_lr: 0.005
2025-11-18 08:40:50,189 [trainer.py] => rd_lr: 0.01
2025-11-18 08:40:50,189 [trainer.py] => rd_dim: 128
2025-11-18 08:40:50,189 [trainer.py] => buffer_size: 500
2025-11-18 08:40:50,189 [trainer.py] => detect_batch_size: 128
2025-11-18 08:40:50,189 [trainer.py] => exp_threshold: 2
2025-11-18 08:40:50,189 [trainer.py] => exp_k_std: 3.0
2025-11-18 08:40:50,190 [trainer.py] => router_attn_dim: 128
2025-11-18 08:40:50,190 [trainer.py] => adapt_start_layer: 9
2025-11-18 08:40:50,190 [trainer.py] => adapt_end_layer: 11
2025-11-18 08:40:51,191 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-11-18 08:40:52,716 [sema_block.py] => Adapter 0.0 added at block 0
2025-11-18 08:40:52,743 [sema_block.py] => Adapter 1.0 added at block 1
2025-11-18 08:40:52,771 [sema_block.py] => Adapter 2.0 added at block 2
2025-11-18 08:40:52,799 [sema_block.py] => Adapter 3.0 added at block 3
2025-11-18 08:40:52,823 [sema_block.py] => Adapter 4.0 added at block 4
2025-11-18 08:40:52,852 [sema_block.py] => Adapter 5.0 added at block 5
2025-11-18 08:40:52,879 [sema_block.py] => Adapter 6.0 added at block 6
2025-11-18 08:40:52,909 [sema_block.py] => Adapter 7.0 added at block 7
2025-11-18 08:40:52,937 [sema_block.py] => Adapter 8.0 added at block 8
2025-11-18 08:40:52,967 [sema_block.py] => Adapter 9.0 added at block 9
2025-11-18 08:40:52,997 [sema_block.py] => Adapter 10.0 added at block 10
2025-11-18 08:40:53,026 [sema_block.py] => Adapter 11.0 added at block 11
2025-11-18 08:40:53,578 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-11-18 08:40:54,312 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-11-18 08:40:54,502 [trainer.py] => All params: 89057868
2025-11-18 08:40:54,505 [trainer.py] => Trainable params: 3259212
2025-11-18 08:40:54,506 [sema.py] => Learning on 0-10
2025-11-18 08:42:28,644 [trainer.py] => config: exps/sema_cifar.json
2025-11-18 08:42:28,644 [trainer.py] => eval: False
2025-11-18 08:42:28,644 [trainer.py] => prefix: reproduce
2025-11-18 08:42:28,644 [trainer.py] => dataset: cifar224
2025-11-18 08:42:28,645 [trainer.py] => memory_size: 2000
2025-11-18 08:42:28,645 [trainer.py] => memory_per_class: None
2025-11-18 08:42:28,645 [trainer.py] => fixed_memory: False
2025-11-18 08:42:28,645 [trainer.py] => shuffle: True
2025-11-18 08:42:28,645 [trainer.py] => init_cls: 10
2025-11-18 08:42:28,646 [trainer.py] => increment: 10
2025-11-18 08:42:28,646 [trainer.py] => model_name: sema
2025-11-18 08:42:28,646 [trainer.py] => backbone_type: pretrained_vit_b16_224_adapter
2025-11-18 08:42:28,646 [trainer.py] => device: [device(type='cuda', index=0)]
2025-11-18 08:42:28,646 [trainer.py] => seed: 1993
2025-11-18 08:42:28,646 [trainer.py] => batch_size: 32
2025-11-18 08:42:28,646 [trainer.py] => weight_decay: 0.0005
2025-11-18 08:42:28,646 [trainer.py] => min_lr: 0
2025-11-18 08:42:28,647 [trainer.py] => ffn_adapter_type: adaptmlp
2025-11-18 08:42:28,647 [trainer.py] => ffn_num: 16
2025-11-18 08:42:28,647 [trainer.py] => optimizer: sgd
2025-11-18 08:42:28,647 [trainer.py] => vpt_type: shallow
2025-11-18 08:42:28,647 [trainer.py] => prompt_token_num: 5
2025-11-18 08:42:28,647 [trainer.py] => func_epoch: 5
2025-11-18 08:42:28,647 [trainer.py] => rd_epoch: 20
2025-11-18 08:42:28,647 [trainer.py] => init_lr: 0.005
2025-11-18 08:42:28,647 [trainer.py] => rd_lr: 0.01
2025-11-18 08:42:28,648 [trainer.py] => rd_dim: 128
2025-11-18 08:42:28,648 [trainer.py] => buffer_size: 500
2025-11-18 08:42:28,648 [trainer.py] => detect_batch_size: 128
2025-11-18 08:42:28,648 [trainer.py] => exp_threshold: 2
2025-11-18 08:42:28,648 [trainer.py] => exp_k_std: 3.0
2025-11-18 08:42:28,648 [trainer.py] => router_attn_dim: 128
2025-11-18 08:42:28,648 [trainer.py] => adapt_start_layer: 9
2025-11-18 08:42:28,648 [trainer.py] => adapt_end_layer: 11
2025-11-18 08:42:29,583 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-11-18 08:42:31,342 [sema_block.py] => Adapter 0.0 added at block 0
2025-11-18 08:42:31,365 [sema_block.py] => Adapter 1.0 added at block 1
2025-11-18 08:42:31,388 [sema_block.py] => Adapter 2.0 added at block 2
2025-11-18 08:42:31,411 [sema_block.py] => Adapter 3.0 added at block 3
2025-11-18 08:42:31,433 [sema_block.py] => Adapter 4.0 added at block 4
2025-11-18 08:42:31,457 [sema_block.py] => Adapter 5.0 added at block 5
2025-11-18 08:42:31,480 [sema_block.py] => Adapter 6.0 added at block 6
2025-11-18 08:42:31,503 [sema_block.py] => Adapter 7.0 added at block 7
2025-11-18 08:42:31,525 [sema_block.py] => Adapter 8.0 added at block 8
2025-11-18 08:42:31,549 [sema_block.py] => Adapter 9.0 added at block 9
2025-11-18 08:42:31,572 [sema_block.py] => Adapter 10.0 added at block 10
2025-11-18 08:42:31,597 [sema_block.py] => Adapter 11.0 added at block 11
2025-11-18 08:42:32,210 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-11-18 08:42:32,892 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-11-18 08:42:33,083 [trainer.py] => All params: 89057868
2025-11-18 08:42:33,084 [trainer.py] => Trainable params: 3259212
2025-11-18 08:42:33,085 [sema.py] => Learning on 0-10
2025-11-18 08:50:15,582 [trainer.py] => config: exps/sema_cifar.json
2025-11-18 08:50:15,583 [trainer.py] => eval: False
2025-11-18 08:50:15,583 [trainer.py] => prefix: reproduce
2025-11-18 08:50:15,583 [trainer.py] => dataset: cifar224
2025-11-18 08:50:15,583 [trainer.py] => memory_size: 2000
2025-11-18 08:50:15,583 [trainer.py] => memory_per_class: None
2025-11-18 08:50:15,583 [trainer.py] => fixed_memory: False
2025-11-18 08:50:15,584 [trainer.py] => shuffle: True
2025-11-18 08:50:15,584 [trainer.py] => init_cls: 10
2025-11-18 08:50:15,584 [trainer.py] => increment: 10
2025-11-18 08:50:15,584 [trainer.py] => model_name: sema
2025-11-18 08:50:15,584 [trainer.py] => backbone_type: pretrained_vit_b16_224_adapter
2025-11-18 08:50:15,584 [trainer.py] => device: [device(type='cuda', index=0)]
2025-11-18 08:50:15,584 [trainer.py] => seed: 1993
2025-11-18 08:50:15,584 [trainer.py] => batch_size: 32
2025-11-18 08:50:15,595 [trainer.py] => weight_decay: 0.0005
2025-11-18 08:50:15,595 [trainer.py] => min_lr: 0
2025-11-18 08:50:15,596 [trainer.py] => ffn_adapter_type: adaptmlp
2025-11-18 08:50:15,596 [trainer.py] => ffn_num: 16
2025-11-18 08:50:15,596 [trainer.py] => optimizer: sgd
2025-11-18 08:50:15,596 [trainer.py] => vpt_type: shallow
2025-11-18 08:50:15,596 [trainer.py] => prompt_token_num: 5
2025-11-18 08:50:15,596 [trainer.py] => func_epoch: 5
2025-11-18 08:50:15,597 [trainer.py] => rd_epoch: 20
2025-11-18 08:50:15,597 [trainer.py] => init_lr: 0.005
2025-11-18 08:50:15,597 [trainer.py] => rd_lr: 0.01
2025-11-18 08:50:15,597 [trainer.py] => rd_dim: 128
2025-11-18 08:50:15,597 [trainer.py] => buffer_size: 500
2025-11-18 08:50:15,597 [trainer.py] => detect_batch_size: 128
2025-11-18 08:50:15,597 [trainer.py] => exp_threshold: 2
2025-11-18 08:50:15,597 [trainer.py] => exp_k_std: 3.0
2025-11-18 08:50:15,597 [trainer.py] => router_attn_dim: 128
2025-11-18 08:50:15,598 [trainer.py] => adapt_start_layer: 9
2025-11-18 08:50:15,598 [trainer.py] => adapt_end_layer: 11
2025-11-18 08:50:16,543 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-11-18 08:50:17,961 [sema_block.py] => Adapter 0.0 added at block 0
2025-11-18 08:50:17,985 [sema_block.py] => Adapter 1.0 added at block 1
2025-11-18 08:50:18,010 [sema_block.py] => Adapter 2.0 added at block 2
2025-11-18 08:50:18,037 [sema_block.py] => Adapter 3.0 added at block 3
2025-11-18 08:50:18,062 [sema_block.py] => Adapter 4.0 added at block 4
2025-11-18 08:50:18,088 [sema_block.py] => Adapter 5.0 added at block 5
2025-11-18 08:50:18,113 [sema_block.py] => Adapter 6.0 added at block 6
2025-11-18 08:50:18,140 [sema_block.py] => Adapter 7.0 added at block 7
2025-11-18 08:50:18,165 [sema_block.py] => Adapter 8.0 added at block 8
2025-11-18 08:50:18,192 [sema_block.py] => Adapter 9.0 added at block 9
2025-11-18 08:50:18,219 [sema_block.py] => Adapter 10.0 added at block 10
2025-11-18 08:50:18,244 [sema_block.py] => Adapter 11.0 added at block 11
2025-11-18 08:50:18,762 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-11-18 08:50:19,230 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-11-18 08:50:19,405 [trainer.py] => All params: 89057868
2025-11-18 08:50:19,406 [trainer.py] => Trainable params: 3259212
2025-11-18 08:50:19,407 [sema.py] => Learning on 0-10
2025-11-18 08:51:57,749 [sema.py] => func Task 0, Epoch 5/5 => Loss 0.224, Train_accy 92.70, Test_accy 98.80
2025-11-18 08:58:43,949 [sema.py] => rd Task 0, Epoch 20/20 => Loss 0.692, Train_accy 94.18, Test_accy 98.80
2025-11-18 08:58:46,913 [sema.py] => Building rehearsal memory...
2025-11-18 08:58:46,914 [base.py] => Constructing exemplars...(200 per classes)
2025-11-18 08:59:11,255 [sema.py] => Memory size: 2000
2025-11-18 08:59:11,256 [trainer.py] => No NME accuracy.
2025-11-18 08:59:11,256 [trainer.py] => CNN: {'total': 98.8, '00-09': 98.8, 'old': 0, 'new': 98.8}
2025-11-18 08:59:11,256 [trainer.py] => CNN top1 curve: [98.8]
2025-11-18 08:59:11,256 [trainer.py] => CNN top5 curve: [100.0]

2025-11-18 08:59:11,257 [trainer.py] => Average Accuracy (CNN): 98.8 

2025-11-18 08:59:11,257 [trainer.py] => All params: 89134768
2025-11-18 08:59:11,258 [trainer.py] => Trainable params: 2439280
2025-11-18 08:59:11,258 [sema.py] => Learning on 10-20
2025-11-18 08:59:11,282 [sema.py] => [DEBUG] Task 1: New data size: 5000, Memory Size: 2000, Total Dataset Size: 7000
2025-11-18 08:59:11,294 [sema.py] => Unfreezing all the adapters and routers for rehearsal...
2025-11-18 08:59:14,754 [sema_block.py] => Adapter 11.1 added at block 11
2025-11-18 09:08:13,862 [sema.py] => func Task 1, Epoch 5/5 => Loss 0.329, Train_accy 89.91, Test_accy 97.05
2025-11-18 09:13:03,547 [trainer.py] => config: exps/sema_cifar.json
2025-11-18 09:13:03,547 [trainer.py] => eval: False
2025-11-18 09:13:03,548 [trainer.py] => prefix: reproduce
2025-11-18 09:13:03,548 [trainer.py] => dataset: cifar224
2025-11-18 09:13:03,548 [trainer.py] => memory_size: 2000
2025-11-18 09:13:03,548 [trainer.py] => memory_per_class: None
2025-11-18 09:13:03,548 [trainer.py] => fixed_memory: False
2025-11-18 09:13:03,549 [trainer.py] => shuffle: True
2025-11-18 09:13:03,549 [trainer.py] => init_cls: 10
2025-11-18 09:13:03,549 [trainer.py] => increment: 10
2025-11-18 09:13:03,549 [trainer.py] => model_name: sema
2025-11-18 09:13:03,549 [trainer.py] => backbone_type: pretrained_vit_b16_224_adapter
2025-11-18 09:13:03,550 [trainer.py] => device: [device(type='cuda', index=0)]
2025-11-18 09:13:03,550 [trainer.py] => seed: 1993
2025-11-18 09:13:03,550 [trainer.py] => batch_size: 32
2025-11-18 09:13:03,550 [trainer.py] => weight_decay: 0.0005
2025-11-18 09:13:03,550 [trainer.py] => min_lr: 0
2025-11-18 09:13:03,550 [trainer.py] => ffn_adapter_type: adaptmlp
2025-11-18 09:13:03,551 [trainer.py] => ffn_num: 16
2025-11-18 09:13:03,551 [trainer.py] => optimizer: sgd
2025-11-18 09:13:03,551 [trainer.py] => vpt_type: shallow
2025-11-18 09:13:03,551 [trainer.py] => prompt_token_num: 5
2025-11-18 09:13:03,551 [trainer.py] => func_epoch: 5
2025-11-18 09:13:03,551 [trainer.py] => rd_epoch: 20
2025-11-18 09:13:03,552 [trainer.py] => init_lr: 0.005
2025-11-18 09:13:03,552 [trainer.py] => rd_lr: 0.01
2025-11-18 09:13:03,552 [trainer.py] => rd_dim: 128
2025-11-18 09:13:03,552 [trainer.py] => buffer_size: 500
2025-11-18 09:13:03,552 [trainer.py] => detect_batch_size: 128
2025-11-18 09:13:03,552 [trainer.py] => exp_threshold: 2
2025-11-18 09:13:03,552 [trainer.py] => exp_k_std: 3.0
2025-11-18 09:13:03,552 [trainer.py] => router_attn_dim: 128
2025-11-18 09:13:03,553 [trainer.py] => adapt_start_layer: 9
2025-11-18 09:13:03,553 [trainer.py] => adapt_end_layer: 11
2025-11-18 09:13:06,568 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-11-18 09:13:17,315 [sema_block.py] => Adapter 0.0 added at block 0
2025-11-18 09:13:17,350 [sema_block.py] => Adapter 1.0 added at block 1
2025-11-18 09:13:17,382 [sema_block.py] => Adapter 2.0 added at block 2
2025-11-18 09:13:17,419 [sema_block.py] => Adapter 3.0 added at block 3
2025-11-18 09:13:17,460 [sema_block.py] => Adapter 4.0 added at block 4
2025-11-18 09:13:17,494 [sema_block.py] => Adapter 5.0 added at block 5
2025-11-18 09:13:17,548 [sema_block.py] => Adapter 6.0 added at block 6
2025-11-18 09:13:17,604 [sema_block.py] => Adapter 7.0 added at block 7
2025-11-18 09:13:17,654 [sema_block.py] => Adapter 8.0 added at block 8
2025-11-18 09:13:17,723 [sema_block.py] => Adapter 9.0 added at block 9
2025-11-18 09:13:17,780 [sema_block.py] => Adapter 10.0 added at block 10
2025-11-18 09:13:17,839 [sema_block.py] => Adapter 11.0 added at block 11
2025-11-18 09:13:18,669 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-11-18 09:13:19,297 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-11-18 09:13:20,087 [trainer.py] => All params: 89057868
2025-11-18 09:13:20,088 [trainer.py] => Trainable params: 3259212
2025-11-18 09:13:20,089 [sema.py] => Learning on 0-10
2025-11-18 10:18:33,608 [sema.py] => rd Task 1, Epoch 20/20 => Loss 0.385, Train_accy 90.22, Test_accy 97.05
2025-11-18 10:32:30,476 [sema.py] => Building rehearsal memory...
2025-11-18 10:32:30,479 [base.py] => Reducing exemplars...(100 per classes)
2025-11-18 10:32:35,343 [base.py] => Constructing exemplars...(100 per classes)
2025-11-18 10:32:58,167 [sema.py] => Memory size: 2000
2025-11-18 10:32:58,170 [trainer.py] => CNN: {'total': 97.05, '00-09': 97.0, '10-19': 97.1, 'old': 97.0, 'new': 97.1}
2025-11-18 10:32:58,170 [trainer.py] => NME: {'total': 49.15, '00-09': 98.3, '10-19': 0.0, 'old': 98.3, 'new': 0.0}
2025-11-18 10:32:58,171 [trainer.py] => CNN top1 curve: [98.8, 97.05]
2025-11-18 10:32:58,171 [trainer.py] => CNN top5 curve: [100.0, 99.85]
2025-11-18 10:32:58,171 [trainer.py] => NME top1 curve: [49.15]
2025-11-18 10:32:58,171 [trainer.py] => NME top5 curve: [49.95]

2025-11-18 10:32:58,172 [trainer.py] => Average Accuracy (CNN): 97.925
2025-11-18 10:32:58,172 [trainer.py] => Average Accuracy (NME): 49.15
2025-11-18 10:32:58,173 [trainer.py] => All params: 89357632
2025-11-18 10:32:58,174 [trainer.py] => Trainable params: 2439280
2025-11-18 10:32:58,174 [sema.py] => Learning on 20-30
2025-11-18 10:32:58,495 [sema.py] => [DEBUG] Task 2: New data size: 5000, Memory Size: 2000, Total Dataset Size: 7000
2025-11-18 10:32:58,632 [sema.py] => Unfreezing all the adapters and routers for rehearsal...
2025-11-18 10:33:02,682 [sema_block.py] => Adapter 11.2 added at block 11
2025-11-18 10:46:13,714 [sema.py] => func Task 2, Epoch 5/5 => Loss 0.302, Train_accy 91.03, Test_accy 96.03
2025-11-18 11:29:57,816 [sema.py] => rd Task 2, Epoch 20/20 => Loss 0.378, Train_accy 94.12, Test_accy 96.03
2025-11-18 11:30:07,513 [sema_block.py] => Adapter 9.1 added at block 9
2025-11-18 11:32:26,531 [sema.py] => func Task 2, Epoch 5/5 => Loss 0.297, Train_accy 91.23, Test_accy 95.70
2025-11-18 11:39:57,787 [sema.py] => rd Task 2, Epoch 20/20 => Loss 0.090, Train_accy 94.40, Test_accy 95.70
2025-11-18 11:40:06,514 [sema_block.py] => Adapter 10.1 added at block 10
2025-11-18 11:43:29,433 [sema.py] => func Task 2, Epoch 5/5 => Loss 0.268, Train_accy 92.19, Test_accy 95.80
2025-11-18 11:56:47,272 [sema.py] => rd Task 2, Epoch 20/20 => Loss 0.152, Train_accy 94.60, Test_accy 95.80
2025-11-18 12:04:11,688 [sema.py] => Building rehearsal memory...
2025-11-18 12:04:11,689 [base.py] => Reducing exemplars...(66 per classes)
2025-11-18 12:04:33,181 [base.py] => Constructing exemplars...(66 per classes)
2025-11-18 12:05:00,651 [sema.py] => Memory size: 1980
2025-11-18 12:05:00,652 [trainer.py] => CNN: {'total': 95.8, '00-09': 94.3, '10-19': 94.3, '20-29': 98.8, 'old': 94.3, 'new': 98.8}
2025-11-18 12:05:00,652 [trainer.py] => NME: {'total': 64.43, '00-09': 97.3, '10-19': 96.0, '20-29': 0.0, 'old': 96.65, 'new': 0.0}
2025-11-18 12:05:00,652 [trainer.py] => CNN top1 curve: [98.8, 97.05, 95.8]
2025-11-18 12:05:00,653 [trainer.py] => CNN top5 curve: [100.0, 99.85, 99.73]
2025-11-18 12:05:00,653 [trainer.py] => NME top1 curve: [49.15, 64.43]
2025-11-18 12:05:00,653 [trainer.py] => NME top5 curve: [49.95, 66.37]

2025-11-18 12:05:00,653 [trainer.py] => Average Accuracy (CNN): 97.21666666666665
2025-11-18 12:05:00,653 [trainer.py] => Average Accuracy (NME): 56.790000000000006
2025-11-18 12:05:00,654 [trainer.py] => All params: 90026224
2025-11-18 12:05:00,655 [trainer.py] => Trainable params: 2439280
2025-11-18 12:05:00,655 [sema.py] => Learning on 30-40
2025-11-18 12:05:00,964 [sema.py] => [DEBUG] Task 3: New data size: 5000, Memory Size: 1980, Total Dataset Size: 6980
2025-11-18 12:05:01,043 [sema.py] => Unfreezing all the adapters and routers for rehearsal...
2025-11-18 12:05:06,032 [sema_block.py] => Adapter 11.3 added at block 11
2025-11-18 12:13:20,986 [sema.py] => func Task 3, Epoch 5/5 => Loss 0.342, Train_accy 89.48, Test_accy 94.85
2025-11-18 12:34:03,744 [sema.py] => rd Task 3, Epoch 20/20 => Loss 0.409, Train_accy 92.42, Test_accy 94.85
2025-11-18 12:34:12,919 [sema_block.py] => Adapter 9.2 added at block 9
2025-11-18 12:36:59,609 [sema.py] => func Task 3, Epoch 5/5 => Loss 0.308, Train_accy 90.52, Test_accy 94.85
2025-11-18 12:46:09,477 [sema.py] => rd Task 3, Epoch 20/20 => Loss 0.094, Train_accy 92.10, Test_accy 94.85
2025-11-18 12:46:13,072 [sema_block.py] => Adapter 10.2 added at block 10
2025-11-18 12:48:59,414 [sema.py] => func Task 3, Epoch 5/5 => Loss 0.296, Train_accy 91.23, Test_accy 94.90
2025-11-18 12:58:43,925 [sema.py] => rd Task 3, Epoch 20/20 => Loss 0.158, Train_accy 92.72, Test_accy 94.90
2025-11-18 13:07:41,951 [sema.py] => Building rehearsal memory...
2025-11-18 13:07:41,952 [base.py] => Reducing exemplars...(50 per classes)
2025-11-18 13:07:51,126 [base.py] => Constructing exemplars...(50 per classes)
2025-11-18 13:08:13,852 [sema.py] => Memory size: 2000
2025-11-18 13:08:13,852 [trainer.py] => CNN: {'total': 94.9, '00-09': 92.9, '10-19': 93.1, '20-29': 95.6, '30-39': 98.0, 'old': 93.87, 'new': 98.0}
2025-11-18 13:08:13,853 [trainer.py] => NME: {'total': 71.85, '00-09': 95.2, '10-19': 95.6, '20-29': 96.6, '30-39': 0.0, 'old': 95.8, 'new': 0.0}
2025-11-18 13:08:13,853 [trainer.py] => CNN top1 curve: [98.8, 97.05, 95.8, 94.9]
2025-11-18 13:08:13,853 [trainer.py] => CNN top5 curve: [100.0, 99.85, 99.73, 99.6]
2025-11-18 13:08:13,853 [trainer.py] => NME top1 curve: [49.15, 64.43, 71.85]
2025-11-18 13:08:13,854 [trainer.py] => NME top5 curve: [49.95, 66.37, 74.72]

2025-11-18 13:08:13,854 [trainer.py] => Average Accuracy (CNN): 96.63749999999999
2025-11-18 13:08:13,854 [trainer.py] => Average Accuracy (NME): 61.81
2025-11-18 13:08:13,855 [trainer.py] => All params: 90694816
2025-11-18 13:08:13,856 [trainer.py] => Trainable params: 2439280
2025-11-18 13:08:13,856 [sema.py] => Learning on 40-50
2025-11-18 13:08:14,143 [sema.py] => [DEBUG] Task 4: New data size: 5000, Memory Size: 2000, Total Dataset Size: 7000
2025-11-18 13:08:14,204 [sema.py] => Unfreezing all the adapters and routers for rehearsal...
2025-11-18 13:08:17,422 [sema_block.py] => Adapter 9.3 added at block 9
2025-11-18 13:15:32,224 [sema.py] => func Task 4, Epoch 5/5 => Loss 0.313, Train_accy 90.84, Test_accy 93.32
2025-11-18 13:39:24,621 [sema.py] => rd Task 4, Epoch 20/20 => Loss 0.098, Train_accy 93.92, Test_accy 93.32
2025-11-18 13:39:25,026 [sema_block.py] => Adapter 10.3 added at block 10
2025-11-18 13:46:02,024 [sema.py] => func Task 4, Epoch 5/5 => Loss 0.303, Train_accy 91.39, Test_accy 93.38
2025-11-18 14:00:53,153 [sema.py] => rd Task 4, Epoch 20/20 => Loss 0.166, Train_accy 93.98, Test_accy 93.38
2025-11-18 14:00:58,589 [sema_block.py] => Adapter 11.4 added at block 11
2025-11-18 14:04:24,479 [sema.py] => func Task 4, Epoch 5/5 => Loss 0.296, Train_accy 91.71, Test_accy 93.40
2025-11-18 14:16:38,844 [sema.py] => rd Task 4, Epoch 20/20 => Loss 0.436, Train_accy 94.00, Test_accy 93.40
2025-11-18 14:23:03,333 [sema.py] => Building rehearsal memory...
2025-11-18 14:23:03,334 [base.py] => Reducing exemplars...(40 per classes)
2025-11-18 14:23:13,246 [base.py] => Constructing exemplars...(40 per classes)
2025-11-18 14:23:37,741 [sema.py] => Memory size: 2000
2025-11-18 14:23:37,742 [trainer.py] => CNN: {'total': 93.4, '00-09': 91.4, '10-19': 89.6, '20-29': 94.5, '30-39': 93.6, '40-49': 97.9, 'old': 92.28, 'new': 97.9}
2025-11-18 14:23:37,742 [trainer.py] => NME: {'total': 76.06, '00-09': 94.8, '10-19': 94.4, '20-29': 96.4, '30-39': 94.7, '40-49': 0.0, 'old': 95.08, 'new': 0.0}
2025-11-18 14:23:37,742 [trainer.py] => CNN top1 curve: [98.8, 97.05, 95.8, 94.9, 93.4]
2025-11-18 14:23:37,742 [trainer.py] => CNN top5 curve: [100.0, 99.85, 99.73, 99.6, 99.28]
2025-11-18 14:23:37,742 [trainer.py] => NME top1 curve: [49.15, 64.43, 71.85, 76.06]
2025-11-18 14:23:37,743 [trainer.py] => NME top5 curve: [49.95, 66.37, 74.72, 79.58]

2025-11-18 14:23:37,743 [trainer.py] => Average Accuracy (CNN): 95.98999999999998
2025-11-18 14:23:37,743 [trainer.py] => Average Accuracy (NME): 65.3725
2025-11-18 14:23:37,744 [trainer.py] => All params: 91363408
2025-11-18 14:23:37,745 [trainer.py] => Trainable params: 2439280
2025-11-18 14:23:37,745 [sema.py] => Learning on 50-60
2025-11-18 14:23:37,985 [sema.py] => [DEBUG] Task 5: New data size: 5000, Memory Size: 2000, Total Dataset Size: 7000
2025-11-18 14:23:38,055 [sema.py] => Unfreezing all the adapters and routers for rehearsal...
2025-11-18 14:23:41,291 [sema_block.py] => Adapter 9.4 added at block 9
2025-11-18 14:31:09,861 [sema.py] => func Task 5, Epoch 5/5 => Loss 0.343, Train_accy 90.50, Test_accy 91.68
2025-11-18 14:50:05,025 [sema.py] => rd Task 5, Epoch 20/20 => Loss 0.097, Train_accy 92.54, Test_accy 91.68
2025-11-18 14:50:05,545 [sema_block.py] => Adapter 10.4 added at block 10
2025-11-18 14:53:53,717 [sema.py] => func Task 5, Epoch 5/5 => Loss 0.328, Train_accy 90.70, Test_accy 91.73
2025-11-18 15:07:14,780 [sema.py] => rd Task 5, Epoch 20/20 => Loss 0.165, Train_accy 93.20, Test_accy 91.73
2025-11-18 15:07:19,312 [sema_block.py] => Adapter 11.5 added at block 11
2025-11-18 15:13:06,435 [sema.py] => func Task 5, Epoch 5/5 => Loss 0.308, Train_accy 90.79, Test_accy 91.77
2025-11-18 15:38:20,520 [sema.py] => rd Task 5, Epoch 20/20 => Loss 0.435, Train_accy 93.36, Test_accy 91.77
2025-11-18 15:42:43,947 [sema.py] => Building rehearsal memory...
2025-11-18 15:42:43,947 [base.py] => Reducing exemplars...(33 per classes)
2025-11-18 15:42:55,825 [base.py] => Constructing exemplars...(33 per classes)
2025-11-18 15:43:18,796 [sema.py] => Memory size: 1980
2025-11-18 15:43:18,796 [trainer.py] => CNN: {'total': 91.77, '00-09': 89.3, '10-19': 91.1, '20-29': 92.1, '30-39': 92.1, '40-49': 88.8, '50-59': 97.2, 'old': 90.68, 'new': 97.2}
2025-11-18 15:43:18,797 [trainer.py] => NME: {'total': 78.13, '00-09': 94.5, '10-19': 93.2, '20-29': 94.8, '30-39': 93.4, '40-49': 92.9, '50-59': 0.0, 'old': 93.76, 'new': 0.0}
2025-11-18 15:43:18,797 [trainer.py] => CNN top1 curve: [98.8, 97.05, 95.8, 94.9, 93.4, 91.77]
2025-11-18 15:43:18,797 [trainer.py] => CNN top5 curve: [100.0, 99.85, 99.73, 99.6, 99.28, 99.13]
2025-11-18 15:43:18,797 [trainer.py] => NME top1 curve: [49.15, 64.43, 71.85, 76.06, 78.13]
2025-11-18 15:43:18,797 [trainer.py] => NME top5 curve: [49.95, 66.37, 74.72, 79.58, 82.83]

2025-11-18 15:43:18,797 [trainer.py] => Average Accuracy (CNN): 95.28666666666665
2025-11-18 15:43:18,798 [trainer.py] => Average Accuracy (NME): 67.924
2025-11-18 15:43:18,798 [trainer.py] => All params: 92032000
2025-11-18 15:43:18,799 [trainer.py] => Trainable params: 2439280
2025-11-18 15:43:18,800 [sema.py] => Learning on 60-70
2025-11-18 15:43:18,904 [sema.py] => [DEBUG] Task 6: New data size: 5000, Memory Size: 1980, Total Dataset Size: 6980
2025-11-18 15:43:18,950 [sema.py] => Unfreezing all the adapters and routers for rehearsal...
2025-11-18 15:43:22,511 [sema_block.py] => Adapter 9.5 added at block 9
2025-11-18 15:51:15,754 [sema.py] => func Task 6, Epoch 5/5 => Loss 0.311, Train_accy 91.30, Test_accy 91.49
2025-11-18 16:12:34,707 [sema.py] => rd Task 6, Epoch 20/20 => Loss 0.099, Train_accy 94.54, Test_accy 91.49
2025-11-18 16:12:35,315 [sema_block.py] => Adapter 10.5 added at block 10
2025-11-18 16:16:56,127 [sema.py] => func Task 6, Epoch 5/5 => Loss 0.294, Train_accy 91.73, Test_accy 91.43
2025-11-18 16:33:00,056 [sema.py] => rd Task 6, Epoch 20/20 => Loss 0.166, Train_accy 94.98, Test_accy 91.43
2025-11-18 16:33:04,424 [sema_block.py] => Adapter 11.6 added at block 11
2025-11-18 16:54:59,024 [sema.py] => func Task 6, Epoch 5/5 => Loss 0.294, Train_accy 91.39, Test_accy 91.47
2025-11-18 18:19:48,933 [sema.py] => rd Task 6, Epoch 20/20 => Loss 0.406, Train_accy 94.88, Test_accy 91.47
2025-11-18 18:26:53,347 [sema.py] => Building rehearsal memory...
2025-11-18 18:26:53,347 [base.py] => Reducing exemplars...(28 per classes)
2025-11-18 18:27:06,641 [base.py] => Constructing exemplars...(28 per classes)
2025-11-18 18:27:38,934 [sema.py] => Memory size: 1960
2025-11-18 18:27:38,934 [trainer.py] => CNN: {'total': 91.47, '00-09': 88.3, '10-19': 90.3, '20-29': 94.4, '30-39': 92.1, '40-49': 91.4, '50-59': 85.2, '60-69': 98.6, 'old': 90.28, 'new': 98.6}
2025-11-18 18:27:38,935 [trainer.py] => NME: {'total': 79.26, '00-09': 92.3, '10-19': 92.6, '20-29': 93.7, '30-39': 93.5, '40-49': 93.2, '50-59': 89.5, '60-69': 0.0, 'old': 92.47, 'new': 0.0}
2025-11-18 18:27:38,935 [trainer.py] => CNN top1 curve: [98.8, 97.05, 95.8, 94.9, 93.4, 91.77, 91.47]
2025-11-18 18:27:38,935 [trainer.py] => CNN top5 curve: [100.0, 99.85, 99.73, 99.6, 99.28, 99.13, 99.09]
2025-11-18 18:27:38,935 [trainer.py] => NME top1 curve: [49.15, 64.43, 71.85, 76.06, 78.13, 79.26]
2025-11-18 18:27:38,935 [trainer.py] => NME top5 curve: [49.95, 66.37, 74.72, 79.58, 82.83, 85.06]

2025-11-18 18:27:38,936 [trainer.py] => Average Accuracy (CNN): 94.74142857142856
2025-11-18 18:27:38,936 [trainer.py] => Average Accuracy (NME): 69.81333333333333
2025-11-18 18:27:38,937 [trainer.py] => All params: 92700592
2025-11-18 18:27:38,937 [trainer.py] => Trainable params: 2439280
2025-11-18 18:27:38,938 [sema.py] => Learning on 70-80
2025-11-18 18:27:39,082 [sema.py] => [DEBUG] Task 7: New data size: 5000, Memory Size: 1960, Total Dataset Size: 6960
2025-11-18 18:27:39,111 [sema.py] => Unfreezing all the adapters and routers for rehearsal...
2025-11-18 18:27:42,520 [sema_block.py] => Adapter 9.6 added at block 9
2025-11-18 18:36:11,710 [sema.py] => func Task 7, Epoch 5/5 => Loss 0.366, Train_accy 89.73, Test_accy 88.34
2025-11-18 18:59:40,871 [sema.py] => rd Task 7, Epoch 20/20 => Loss 0.103, Train_accy 92.56, Test_accy 88.34
2025-11-18 18:59:41,504 [sema_block.py] => Adapter 10.6 added at block 10
2025-11-18 19:04:36,220 [sema.py] => func Task 7, Epoch 5/5 => Loss 0.335, Train_accy 90.68, Test_accy 88.26
2025-11-18 19:22:51,214 [sema.py] => rd Task 7, Epoch 20/20 => Loss 0.177, Train_accy 93.48, Test_accy 88.26
2025-11-18 19:22:55,824 [sema_block.py] => Adapter 11.7 added at block 11
2025-11-18 19:48:51,248 [sema.py] => func Task 7, Epoch 5/5 => Loss 0.323, Train_accy 90.52, Test_accy 88.10
2025-11-18 21:25:48,765 [sema.py] => rd Task 7, Epoch 20/20 => Loss 0.446, Train_accy 93.62, Test_accy 88.10
2025-11-18 21:33:02,488 [sema.py] => Building rehearsal memory...
2025-11-18 21:33:02,488 [base.py] => Reducing exemplars...(25 per classes)
2025-11-18 21:33:18,384 [base.py] => Constructing exemplars...(25 per classes)
2025-11-18 21:34:02,617 [sema.py] => Memory size: 2000
2025-11-18 21:34:02,617 [trainer.py] => CNN: {'total': 88.1, '00-09': 86.4, '10-19': 90.1, '20-29': 92.9, '30-39': 90.1, '40-49': 83.4, '50-59': 76.2, '60-69': 90.1, '70-79': 95.6, 'old': 87.03, 'new': 95.6}
2025-11-18 21:34:02,617 [trainer.py] => NME: {'total': 80.78, '00-09': 92.1, '10-19': 91.3, '20-29': 94.3, '30-39': 92.8, '40-49': 92.3, '50-59': 90.9, '60-69': 92.5, '70-79': 0.0, 'old': 92.31, 'new': 0.0}
2025-11-18 21:34:02,618 [trainer.py] => CNN top1 curve: [98.8, 97.05, 95.8, 94.9, 93.4, 91.77, 91.47, 88.1]
2025-11-18 21:34:02,618 [trainer.py] => CNN top5 curve: [100.0, 99.85, 99.73, 99.6, 99.28, 99.13, 99.09, 98.84]
2025-11-18 21:34:02,618 [trainer.py] => NME top1 curve: [49.15, 64.43, 71.85, 76.06, 78.13, 79.26, 80.78]
2025-11-18 21:34:02,618 [trainer.py] => NME top5 curve: [49.95, 66.37, 74.72, 79.58, 82.83, 85.06, 86.82]

2025-11-18 21:34:02,619 [trainer.py] => Average Accuracy (CNN): 93.91125
2025-11-18 21:34:02,619 [trainer.py] => Average Accuracy (NME): 71.38
2025-11-18 21:34:02,620 [trainer.py] => All params: 93369184
2025-11-18 21:34:02,621 [trainer.py] => Trainable params: 2439280
2025-11-18 21:34:02,621 [sema.py] => Learning on 80-90
2025-11-18 21:34:02,712 [sema.py] => [DEBUG] Task 8: New data size: 5000, Memory Size: 2000, Total Dataset Size: 7000
2025-11-18 21:34:02,743 [sema.py] => Unfreezing all the adapters and routers for rehearsal...
2025-11-18 21:34:06,393 [sema_block.py] => Adapter 9.7 added at block 9
2025-11-18 21:43:35,537 [sema.py] => func Task 8, Epoch 5/5 => Loss 0.331, Train_accy 90.79, Test_accy 87.60
2025-11-19 06:15:53,054 [trainer.py] => config: exps/sema_cifar.json
2025-11-19 06:15:53,054 [trainer.py] => eval: False
2025-11-19 06:15:53,054 [trainer.py] => prefix: reproduce
2025-11-19 06:15:53,054 [trainer.py] => dataset: cifar224
2025-11-19 06:15:53,055 [trainer.py] => memory_size: 2000
2025-11-19 06:15:53,055 [trainer.py] => memory_per_class: None
2025-11-19 06:15:53,055 [trainer.py] => fixed_memory: False
2025-11-19 06:15:53,055 [trainer.py] => shuffle: True
2025-11-19 06:15:53,055 [trainer.py] => init_cls: 10
2025-11-19 06:15:53,055 [trainer.py] => increment: 10
2025-11-19 06:15:53,055 [trainer.py] => model_name: sema
2025-11-19 06:15:53,056 [trainer.py] => backbone_type: pretrained_vit_b16_224_adapter
2025-11-19 06:15:53,056 [trainer.py] => device: [device(type='cuda', index=0)]
2025-11-19 06:15:53,056 [trainer.py] => seed: 1993
2025-11-19 06:15:53,056 [trainer.py] => batch_size: 32
2025-11-19 06:15:53,056 [trainer.py] => weight_decay: 0.0005
2025-11-19 06:15:53,056 [trainer.py] => min_lr: 0
2025-11-19 06:15:53,056 [trainer.py] => ffn_adapter_type: adaptmlp
2025-11-19 06:15:53,057 [trainer.py] => ffn_num: 16
2025-11-19 06:15:53,057 [trainer.py] => optimizer: sgd
2025-11-19 06:15:53,057 [trainer.py] => vpt_type: shallow
2025-11-19 06:15:53,057 [trainer.py] => prompt_token_num: 5
2025-11-19 06:15:53,057 [trainer.py] => func_epoch: 5
2025-11-19 06:15:53,057 [trainer.py] => rd_epoch: 20
2025-11-19 06:15:53,057 [trainer.py] => init_lr: 0.005
2025-11-19 06:15:53,058 [trainer.py] => rd_lr: 0.01
2025-11-19 06:15:53,058 [trainer.py] => rd_dim: 128
2025-11-19 06:15:53,058 [trainer.py] => buffer_size: 500
2025-11-19 06:15:53,058 [trainer.py] => detect_batch_size: 128
2025-11-19 06:15:53,058 [trainer.py] => exp_threshold: 2
2025-11-19 06:15:53,058 [trainer.py] => exp_k_std: 3.0
2025-11-19 06:15:53,058 [trainer.py] => router_attn_dim: 128
2025-11-19 06:15:53,059 [trainer.py] => adapt_start_layer: 9
2025-11-19 06:15:53,059 [trainer.py] => adapt_end_layer: 11
2025-11-19 06:15:56,389 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-11-19 06:15:58,058 [sema_block.py] => Adapter 0.0 added at block 0
2025-11-19 06:15:58,085 [sema_block.py] => Adapter 1.0 added at block 1
2025-11-19 06:15:58,111 [sema_block.py] => Adapter 2.0 added at block 2
2025-11-19 06:15:58,137 [sema_block.py] => Adapter 3.0 added at block 3
2025-11-19 06:15:58,163 [sema_block.py] => Adapter 4.0 added at block 4
2025-11-19 06:15:58,189 [sema_block.py] => Adapter 5.0 added at block 5
2025-11-19 06:15:58,220 [sema_block.py] => Adapter 6.0 added at block 6
2025-11-19 06:15:58,253 [sema_block.py] => Adapter 7.0 added at block 7
2025-11-19 06:15:58,284 [sema_block.py] => Adapter 8.0 added at block 8
2025-11-19 06:15:58,315 [sema_block.py] => Adapter 9.0 added at block 9
2025-11-19 06:15:58,351 [sema_block.py] => Adapter 10.0 added at block 10
2025-11-19 06:15:58,378 [sema_block.py] => Adapter 11.0 added at block 11
2025-11-19 06:15:58,946 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-11-19 06:15:59,194 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-11-19 06:15:59,389 [trainer.py] => All params: 89057868
2025-11-19 06:15:59,390 [trainer.py] => Trainable params: 3259212
2025-11-19 06:15:59,391 [sema.py] => Learning on 0-10
2025-11-19 06:17:43,416 [sema.py] => func Task 0, Epoch 5/5 => Loss 0.224, Train_accy 92.70, Test_accy 98.80
2025-11-19 06:24:45,960 [sema.py] => rd Task 0, Epoch 20/20 => Loss 0.692, Train_accy 94.18, Test_accy 98.80
2025-11-19 06:24:48,994 [sema.py] => Building rehearsal memory...
2025-11-19 06:24:48,994 [base.py] => Constructing exemplars...(200 per classes)
2025-11-19 06:25:14,475 [sema.py] => Memory size: 2000
2025-11-19 06:25:14,476 [trainer.py] => No NME accuracy.
2025-11-19 06:25:14,476 [trainer.py] => CNN: {'total': 98.8, '00-09': 98.8, 'old': 0, 'new': 98.8}
2025-11-19 06:25:14,476 [trainer.py] => CNN top1 curve: [98.8]
2025-11-19 06:25:14,476 [trainer.py] => CNN top5 curve: [100.0]

2025-11-19 06:25:14,477 [trainer.py] => Average Accuracy (CNN): 98.8 

2025-11-19 06:25:14,477 [trainer.py] => All params: 89134768
2025-11-19 06:25:14,478 [trainer.py] => Trainable params: 2439280
2025-11-19 06:25:14,478 [sema.py] => Learning on 10-20
2025-11-19 06:25:14,502 [sema.py] => [DEBUG] Task 1: New data size: 5000, Memory Size: 2000, Total Dataset Size: 7000
2025-11-19 06:25:14,514 [sema.py] => Unfreezing all the adapters and routers for rehearsal...
2025-11-19 06:25:17,643 [sema_block.py] => Adapter 11.1 added at block 11
2025-11-19 06:34:05,701 [sema.py] => func Task 1, Epoch 5/5 => Loss 0.329, Train_accy 89.91, Test_accy 97.05
2025-11-19 07:06:06,402 [sema.py] => rd Task 1, Epoch 20/20 => Loss 0.385, Train_accy 90.22, Test_accy 97.05
2025-11-19 07:13:54,826 [sema.py] => Building rehearsal memory...
2025-11-19 07:13:54,827 [base.py] => Reducing exemplars...(100 per classes)
2025-11-19 07:13:59,180 [base.py] => Constructing exemplars...(100 per classes)
2025-11-19 07:14:22,793 [sema.py] => Memory size: 2000
2025-11-19 07:14:22,793 [trainer.py] => CNN: {'total': 97.05, '00-09': 97.0, '10-19': 97.1, 'old': 97.0, 'new': 97.1}
2025-11-19 07:14:22,793 [trainer.py] => NME: {'total': 49.15, '00-09': 98.3, '10-19': 0.0, 'old': 98.3, 'new': 0.0}
2025-11-19 07:14:22,794 [trainer.py] => CNN top1 curve: [98.8, 97.05]
2025-11-19 07:14:22,794 [trainer.py] => CNN top5 curve: [100.0, 99.85]
2025-11-19 07:14:22,794 [trainer.py] => NME top1 curve: [49.15]
2025-11-19 07:14:22,794 [trainer.py] => NME top5 curve: [49.95]

2025-11-19 07:14:22,795 [trainer.py] => Average Accuracy (CNN): 97.925
2025-11-19 07:14:22,795 [trainer.py] => Average Accuracy (NME): 49.15
2025-11-19 07:14:22,795 [trainer.py] => All params: 89357632
2025-11-19 07:14:22,796 [trainer.py] => Trainable params: 2439280
2025-11-19 07:14:22,796 [sema.py] => Learning on 20-30
2025-11-19 07:14:22,830 [sema.py] => [DEBUG] Task 2: New data size: 5000, Memory Size: 2000, Total Dataset Size: 7000
2025-11-19 07:14:22,852 [sema.py] => Unfreezing all the adapters and routers for rehearsal...
2025-11-19 07:14:26,061 [sema_block.py] => Adapter 11.2 added at block 11
2025-11-19 07:27:38,239 [sema.py] => func Task 2, Epoch 5/5 => Loss 0.302, Train_accy 91.03, Test_accy 96.03
2025-11-19 08:25:27,605 [sema.py] => rd Task 2, Epoch 20/20 => Loss 0.378, Train_accy 94.12, Test_accy 96.03
2025-11-19 08:25:35,050 [sema_block.py] => Adapter 9.1 added at block 9
2025-11-19 08:27:56,271 [sema.py] => func Task 2, Epoch 5/5 => Loss 0.297, Train_accy 91.23, Test_accy 95.70
2025-11-19 08:35:22,875 [sema.py] => rd Task 2, Epoch 20/20 => Loss 0.090, Train_accy 94.40, Test_accy 95.70
2025-11-19 08:35:26,888 [sema_block.py] => Adapter 10.1 added at block 10
2025-11-19 08:37:56,339 [sema.py] => func Task 2, Epoch 5/5 => Loss 0.268, Train_accy 92.19, Test_accy 95.80
2025-11-19 08:46:18,947 [sema.py] => rd Task 2, Epoch 20/20 => Loss 0.152, Train_accy 94.60, Test_accy 95.80
2025-11-19 08:51:30,395 [sema.py] => Building rehearsal memory...
2025-11-19 08:51:30,396 [base.py] => Reducing exemplars...(66 per classes)
2025-11-19 08:51:37,685 [base.py] => Constructing exemplars...(66 per classes)
2025-11-19 08:51:59,886 [sema.py] => Memory size: 1980
2025-11-19 08:51:59,886 [trainer.py] => CNN: {'total': 95.8, '00-09': 94.3, '10-19': 94.3, '20-29': 98.8, 'old': 94.3, 'new': 98.8}
2025-11-19 08:51:59,887 [trainer.py] => NME: {'total': 64.43, '00-09': 97.3, '10-19': 96.0, '20-29': 0.0, 'old': 96.65, 'new': 0.0}
2025-11-19 08:51:59,887 [trainer.py] => CNN top1 curve: [98.8, 97.05, 95.8]
2025-11-19 08:51:59,887 [trainer.py] => CNN top5 curve: [100.0, 99.85, 99.73]
2025-11-19 08:51:59,887 [trainer.py] => NME top1 curve: [49.15, 64.43]
2025-11-19 08:51:59,888 [trainer.py] => NME top5 curve: [49.95, 66.37]

2025-11-19 08:51:59,888 [trainer.py] => Average Accuracy (CNN): 97.21666666666665
2025-11-19 08:51:59,888 [trainer.py] => Average Accuracy (NME): 56.790000000000006
2025-11-19 08:51:59,889 [trainer.py] => All params: 90026224
2025-11-19 08:51:59,889 [trainer.py] => Trainable params: 2439280
2025-11-19 08:51:59,890 [sema.py] => Learning on 30-40
2025-11-19 08:52:00,199 [sema.py] => [DEBUG] Task 3: New data size: 5000, Memory Size: 1980, Total Dataset Size: 6980
2025-11-19 08:52:00,285 [sema.py] => Unfreezing all the adapters and routers for rehearsal...
2025-11-19 08:52:07,342 [sema_block.py] => Adapter 11.3 added at block 11
2025-11-19 08:58:02,430 [sema.py] => func Task 3, Epoch 5/5 => Loss 0.342, Train_accy 89.48, Test_accy 94.85
2025-11-19 09:13:46,211 [sema.py] => rd Task 3, Epoch 20/20 => Loss 0.409, Train_accy 92.42, Test_accy 94.85
2025-11-19 09:13:55,600 [sema_block.py] => Adapter 9.2 added at block 9
2025-11-19 09:16:44,963 [sema.py] => func Task 3, Epoch 5/5 => Loss 0.308, Train_accy 90.52, Test_accy 94.85
2025-11-19 09:25:37,573 [sema.py] => rd Task 3, Epoch 20/20 => Loss 0.094, Train_accy 92.10, Test_accy 94.85
2025-11-19 09:25:41,011 [sema_block.py] => Adapter 10.2 added at block 10
2025-11-19 09:28:32,280 [sema.py] => func Task 3, Epoch 5/5 => Loss 0.296, Train_accy 91.23, Test_accy 94.90
2025-11-19 09:38:46,152 [sema.py] => rd Task 3, Epoch 20/20 => Loss 0.158, Train_accy 92.72, Test_accy 94.90
2025-11-19 09:51:59,400 [sema.py] => Building rehearsal memory...
2025-11-19 09:51:59,401 [base.py] => Reducing exemplars...(50 per classes)
2025-11-19 09:52:26,483 [base.py] => Constructing exemplars...(50 per classes)
2025-11-19 09:53:29,588 [sema.py] => Memory size: 2000
2025-11-19 09:53:29,589 [trainer.py] => CNN: {'total': 94.9, '00-09': 92.9, '10-19': 93.1, '20-29': 95.6, '30-39': 98.0, 'old': 93.87, 'new': 98.0}
2025-11-19 09:53:29,589 [trainer.py] => NME: {'total': 71.85, '00-09': 95.2, '10-19': 95.6, '20-29': 96.6, '30-39': 0.0, 'old': 95.8, 'new': 0.0}
2025-11-19 09:53:29,589 [trainer.py] => CNN top1 curve: [98.8, 97.05, 95.8, 94.9]
2025-11-19 09:53:29,589 [trainer.py] => CNN top5 curve: [100.0, 99.85, 99.73, 99.6]
2025-11-19 09:53:29,589 [trainer.py] => NME top1 curve: [49.15, 64.43, 71.85]
2025-11-19 09:53:29,590 [trainer.py] => NME top5 curve: [49.95, 66.37, 74.72]

2025-11-19 09:53:29,590 [trainer.py] => Average Accuracy (CNN): 96.63749999999999
2025-11-19 09:53:29,590 [trainer.py] => Average Accuracy (NME): 61.81
2025-11-19 09:53:29,591 [trainer.py] => All params: 90694816
2025-11-19 09:53:29,591 [trainer.py] => Trainable params: 2439280
2025-11-19 09:53:29,592 [sema.py] => Learning on 40-50
2025-11-19 09:53:30,319 [sema.py] => [DEBUG] Task 4: New data size: 5000, Memory Size: 2000, Total Dataset Size: 7000
2025-11-19 09:53:30,452 [sema.py] => Unfreezing all the adapters and routers for rehearsal...
2025-11-19 09:53:41,144 [sema_block.py] => Adapter 9.3 added at block 9
2025-11-19 10:13:21,367 [sema.py] => func Task 4, Epoch 5/5 => Loss 0.313, Train_accy 90.84, Test_accy 93.32
2025-11-19 11:03:30,499 [sema.py] => rd Task 4, Epoch 20/20 => Loss 0.098, Train_accy 93.92, Test_accy 93.32
2025-11-19 11:03:30,862 [sema_block.py] => Adapter 10.3 added at block 10
2025-11-19 11:12:35,500 [sema.py] => func Task 4, Epoch 5/5 => Loss 0.303, Train_accy 91.39, Test_accy 93.38
2025-11-19 12:18:54,238 [sema.py] => rd Task 4, Epoch 20/20 => Loss 0.166, Train_accy 93.98, Test_accy 93.38
2025-11-19 12:19:43,765 [sema_block.py] => Adapter 11.4 added at block 11
2025-11-19 12:53:54,056 [sema.py] => func Task 4, Epoch 5/5 => Loss 0.296, Train_accy 91.71, Test_accy 93.40
2025-11-19 13:21:33,342 [sema.py] => rd Task 4, Epoch 20/20 => Loss 0.436, Train_accy 94.00, Test_accy 93.40
2025-11-19 13:27:45,733 [sema.py] => Building rehearsal memory...
2025-11-19 13:27:45,734 [base.py] => Reducing exemplars...(40 per classes)
2025-11-19 13:27:57,066 [base.py] => Constructing exemplars...(40 per classes)
2025-11-19 13:28:19,988 [sema.py] => Memory size: 2000
2025-11-19 13:28:19,988 [trainer.py] => CNN: {'total': 93.4, '00-09': 91.4, '10-19': 89.6, '20-29': 94.5, '30-39': 93.6, '40-49': 97.9, 'old': 92.28, 'new': 97.9}
2025-11-19 13:28:19,988 [trainer.py] => NME: {'total': 76.06, '00-09': 94.8, '10-19': 94.4, '20-29': 96.4, '30-39': 94.7, '40-49': 0.0, 'old': 95.08, 'new': 0.0}
2025-11-19 13:28:19,988 [trainer.py] => CNN top1 curve: [98.8, 97.05, 95.8, 94.9, 93.4]
2025-11-19 13:28:19,989 [trainer.py] => CNN top5 curve: [100.0, 99.85, 99.73, 99.6, 99.28]
2025-11-19 13:28:19,989 [trainer.py] => NME top1 curve: [49.15, 64.43, 71.85, 76.06]
2025-11-19 13:28:19,989 [trainer.py] => NME top5 curve: [49.95, 66.37, 74.72, 79.58]

2025-11-19 13:28:19,989 [trainer.py] => Average Accuracy (CNN): 95.98999999999998
2025-11-19 13:28:19,989 [trainer.py] => Average Accuracy (NME): 65.3725
2025-11-19 13:28:19,990 [trainer.py] => All params: 91363408
2025-11-19 13:28:19,991 [trainer.py] => Trainable params: 2439280
2025-11-19 13:28:19,992 [sema.py] => Learning on 50-60
2025-11-19 13:28:20,267 [sema.py] => [DEBUG] Task 5: New data size: 5000, Memory Size: 2000, Total Dataset Size: 7000
2025-11-19 13:28:20,421 [sema.py] => Unfreezing all the adapters and routers for rehearsal...
2025-11-19 13:28:23,892 [sema_block.py] => Adapter 9.4 added at block 9
2025-11-19 13:35:27,012 [sema.py] => func Task 5, Epoch 5/5 => Loss 0.343, Train_accy 90.50, Test_accy 91.68
2025-11-19 13:54:20,482 [sema.py] => rd Task 5, Epoch 20/20 => Loss 0.097, Train_accy 92.54, Test_accy 91.68
2025-11-19 13:54:20,991 [sema_block.py] => Adapter 10.4 added at block 10
2025-11-19 13:58:11,552 [sema.py] => func Task 5, Epoch 5/5 => Loss 0.328, Train_accy 90.70, Test_accy 91.73
2025-11-19 14:26:46,991 [sema.py] => rd Task 5, Epoch 20/20 => Loss 0.165, Train_accy 93.20, Test_accy 91.73
2025-11-19 14:26:51,588 [sema_block.py] => Adapter 11.5 added at block 11
2025-11-19 14:32:15,767 [sema.py] => func Task 5, Epoch 5/5 => Loss 0.308, Train_accy 90.79, Test_accy 91.77
2025-11-19 14:56:20,152 [sema.py] => rd Task 5, Epoch 20/20 => Loss 0.435, Train_accy 93.36, Test_accy 91.77
2025-11-19 15:00:47,517 [sema.py] => Building rehearsal memory...
2025-11-19 15:00:47,518 [base.py] => Reducing exemplars...(33 per classes)
2025-11-19 15:01:00,870 [base.py] => Constructing exemplars...(33 per classes)
2025-11-19 15:01:24,132 [sema.py] => Memory size: 1980
2025-11-19 15:01:24,133 [trainer.py] => CNN: {'total': 91.77, '00-09': 89.3, '10-19': 91.1, '20-29': 92.1, '30-39': 92.1, '40-49': 88.8, '50-59': 97.2, 'old': 90.68, 'new': 97.2}
2025-11-19 15:01:24,133 [trainer.py] => NME: {'total': 78.13, '00-09': 94.5, '10-19': 93.2, '20-29': 94.8, '30-39': 93.4, '40-49': 92.9, '50-59': 0.0, 'old': 93.76, 'new': 0.0}
2025-11-19 15:01:24,133 [trainer.py] => CNN top1 curve: [98.8, 97.05, 95.8, 94.9, 93.4, 91.77]
2025-11-19 15:01:24,133 [trainer.py] => CNN top5 curve: [100.0, 99.85, 99.73, 99.6, 99.28, 99.13]
2025-11-19 15:01:24,134 [trainer.py] => NME top1 curve: [49.15, 64.43, 71.85, 76.06, 78.13]
2025-11-19 15:01:24,134 [trainer.py] => NME top5 curve: [49.95, 66.37, 74.72, 79.58, 82.83]

2025-11-19 15:01:24,134 [trainer.py] => Average Accuracy (CNN): 95.28666666666665
2025-11-19 15:01:24,134 [trainer.py] => Average Accuracy (NME): 67.924
2025-11-19 15:01:24,135 [trainer.py] => All params: 92032000
2025-11-19 15:01:24,136 [trainer.py] => Trainable params: 2439280
2025-11-19 15:01:24,137 [sema.py] => Learning on 60-70
2025-11-19 15:01:24,350 [sema.py] => [DEBUG] Task 6: New data size: 5000, Memory Size: 1980, Total Dataset Size: 6980
2025-11-19 15:01:24,423 [sema.py] => Unfreezing all the adapters and routers for rehearsal...
2025-11-19 15:01:28,000 [sema_block.py] => Adapter 9.5 added at block 9
2025-11-19 15:09:10,411 [sema.py] => func Task 6, Epoch 5/5 => Loss 0.311, Train_accy 91.30, Test_accy 91.49
2025-11-19 15:30:38,582 [sema.py] => rd Task 6, Epoch 20/20 => Loss 0.099, Train_accy 94.54, Test_accy 91.49
2025-11-19 15:30:39,198 [sema_block.py] => Adapter 10.5 added at block 10
2025-11-19 15:35:10,684 [sema.py] => func Task 6, Epoch 5/5 => Loss 0.294, Train_accy 91.73, Test_accy 91.43
2025-11-19 15:51:39,389 [sema.py] => rd Task 6, Epoch 20/20 => Loss 0.166, Train_accy 94.98, Test_accy 91.43
2025-11-19 15:51:44,035 [sema_block.py] => Adapter 11.6 added at block 11
2025-11-19 16:14:47,641 [sema.py] => func Task 6, Epoch 5/5 => Loss 0.294, Train_accy 91.39, Test_accy 91.47
2025-11-19 17:43:37,058 [sema.py] => rd Task 6, Epoch 20/20 => Loss 0.406, Train_accy 94.88, Test_accy 91.47
2025-11-19 17:50:39,953 [sema.py] => Building rehearsal memory...
2025-11-19 17:50:39,954 [base.py] => Reducing exemplars...(28 per classes)
2025-11-19 17:50:54,747 [base.py] => Constructing exemplars...(28 per classes)
2025-11-19 17:51:26,656 [sema.py] => Memory size: 1960
2025-11-19 17:51:26,656 [trainer.py] => CNN: {'total': 91.47, '00-09': 88.3, '10-19': 90.3, '20-29': 94.4, '30-39': 92.1, '40-49': 91.4, '50-59': 85.2, '60-69': 98.6, 'old': 90.28, 'new': 98.6}
2025-11-19 17:51:26,656 [trainer.py] => NME: {'total': 79.26, '00-09': 92.3, '10-19': 92.6, '20-29': 93.7, '30-39': 93.5, '40-49': 93.2, '50-59': 89.5, '60-69': 0.0, 'old': 92.47, 'new': 0.0}
2025-11-19 17:51:26,657 [trainer.py] => CNN top1 curve: [98.8, 97.05, 95.8, 94.9, 93.4, 91.77, 91.47]
2025-11-19 17:51:26,657 [trainer.py] => CNN top5 curve: [100.0, 99.85, 99.73, 99.6, 99.28, 99.13, 99.09]
2025-11-19 17:51:26,657 [trainer.py] => NME top1 curve: [49.15, 64.43, 71.85, 76.06, 78.13, 79.26]
2025-11-19 17:51:26,657 [trainer.py] => NME top5 curve: [49.95, 66.37, 74.72, 79.58, 82.83, 85.06]

2025-11-19 17:51:26,657 [trainer.py] => Average Accuracy (CNN): 94.74142857142856
2025-11-19 17:51:26,657 [trainer.py] => Average Accuracy (NME): 69.81333333333333
2025-11-19 17:51:26,658 [trainer.py] => All params: 92700592
2025-11-19 17:51:26,659 [trainer.py] => Trainable params: 2439280
2025-11-19 17:51:26,660 [sema.py] => Learning on 70-80
2025-11-19 17:51:26,791 [sema.py] => [DEBUG] Task 7: New data size: 5000, Memory Size: 1960, Total Dataset Size: 6960
2025-11-19 17:51:26,821 [sema.py] => Unfreezing all the adapters and routers for rehearsal...
2025-11-19 17:51:30,347 [sema_block.py] => Adapter 9.6 added at block 9
2025-11-19 17:59:49,531 [sema.py] => func Task 7, Epoch 5/5 => Loss 0.366, Train_accy 89.73, Test_accy 88.34
2025-11-19 18:23:31,280 [sema.py] => rd Task 7, Epoch 20/20 => Loss 0.103, Train_accy 92.56, Test_accy 88.34
2025-11-19 18:23:31,932 [sema_block.py] => Adapter 10.6 added at block 10
2025-11-19 18:28:40,524 [sema.py] => func Task 7, Epoch 5/5 => Loss 0.335, Train_accy 90.68, Test_accy 88.26
2025-11-19 18:47:27,372 [sema.py] => rd Task 7, Epoch 20/20 => Loss 0.177, Train_accy 93.48, Test_accy 88.26
2025-11-19 18:47:32,133 [sema_block.py] => Adapter 11.7 added at block 11
2025-11-19 19:14:49,011 [sema.py] => func Task 7, Epoch 5/5 => Loss 0.323, Train_accy 90.52, Test_accy 88.10
2025-11-19 20:56:26,170 [sema.py] => rd Task 7, Epoch 20/20 => Loss 0.446, Train_accy 93.62, Test_accy 88.10
2025-11-19 21:03:48,196 [sema.py] => Building rehearsal memory...
2025-11-19 21:03:48,196 [base.py] => Reducing exemplars...(25 per classes)
2025-11-19 21:04:05,084 [base.py] => Constructing exemplars...(25 per classes)
2025-11-19 21:04:49,952 [sema.py] => Memory size: 2000
2025-11-19 21:04:49,952 [trainer.py] => CNN: {'total': 88.1, '00-09': 86.4, '10-19': 90.1, '20-29': 92.9, '30-39': 90.1, '40-49': 83.4, '50-59': 76.2, '60-69': 90.1, '70-79': 95.6, 'old': 87.03, 'new': 95.6}
2025-11-19 21:04:49,952 [trainer.py] => NME: {'total': 80.78, '00-09': 92.1, '10-19': 91.3, '20-29': 94.3, '30-39': 92.8, '40-49': 92.3, '50-59': 90.9, '60-69': 92.5, '70-79': 0.0, 'old': 92.31, 'new': 0.0}
2025-11-19 21:04:49,952 [trainer.py] => CNN top1 curve: [98.8, 97.05, 95.8, 94.9, 93.4, 91.77, 91.47, 88.1]
2025-11-19 21:04:49,953 [trainer.py] => CNN top5 curve: [100.0, 99.85, 99.73, 99.6, 99.28, 99.13, 99.09, 98.84]
2025-11-19 21:04:49,953 [trainer.py] => NME top1 curve: [49.15, 64.43, 71.85, 76.06, 78.13, 79.26, 80.78]
2025-11-19 21:04:49,953 [trainer.py] => NME top5 curve: [49.95, 66.37, 74.72, 79.58, 82.83, 85.06, 86.82]

2025-11-19 21:04:49,953 [trainer.py] => Average Accuracy (CNN): 93.91125
2025-11-19 21:04:49,954 [trainer.py] => Average Accuracy (NME): 71.38
2025-11-19 21:04:49,954 [trainer.py] => All params: 93369184
2025-11-19 21:04:49,955 [trainer.py] => Trainable params: 2439280
2025-11-19 21:04:49,956 [sema.py] => Learning on 80-90
2025-11-19 21:04:50,038 [sema.py] => [DEBUG] Task 8: New data size: 5000, Memory Size: 2000, Total Dataset Size: 7000
2025-11-19 21:04:50,060 [sema.py] => Unfreezing all the adapters and routers for rehearsal...
2025-11-19 21:04:53,562 [sema_block.py] => Adapter 9.7 added at block 9
2025-11-19 21:14:09,553 [sema.py] => func Task 8, Epoch 5/5 => Loss 0.331, Train_accy 90.79, Test_accy 87.60
2025-11-19 21:40:21,053 [sema.py] => rd Task 8, Epoch 20/20 => Loss 0.100, Train_accy 93.26, Test_accy 87.60
2025-11-19 21:40:21,754 [sema_block.py] => Adapter 10.7 added at block 10
2025-11-19 21:46:03,551 [sema.py] => func Task 8, Epoch 5/5 => Loss 0.308, Train_accy 91.36, Test_accy 87.83
2025-11-19 22:07:17,983 [sema.py] => rd Task 8, Epoch 20/20 => Loss 0.168, Train_accy 94.04, Test_accy 87.83
2025-11-19 22:07:23,346 [sema_block.py] => Adapter 11.8 added at block 11
2025-11-19 22:13:12,309 [sema.py] => func Task 8, Epoch 5/5 => Loss 0.309, Train_accy 91.26, Test_accy 87.46
2025-11-19 22:35:22,656 [sema.py] => rd Task 8, Epoch 20/20 => Loss 0.405, Train_accy 94.24, Test_accy 87.46
2025-11-19 23:02:41,225 [sema.py] => Building rehearsal memory...
2025-11-19 23:02:41,228 [base.py] => Reducing exemplars...(22 per classes)
2025-11-19 23:06:30,345 [base.py] => Constructing exemplars...(22 per classes)
2025-11-19 23:11:37,797 [sema.py] => Memory size: 1980
2025-11-19 23:11:37,798 [trainer.py] => CNN: {'total': 87.46, '00-09': 84.4, '10-19': 84.3, '20-29': 90.0, '30-39': 90.7, '40-49': 83.7, '50-59': 83.2, '60-69': 89.1, '70-79': 84.4, '80-89': 97.3, 'old': 86.22, 'new': 97.3}
2025-11-19 23:11:37,798 [trainer.py] => NME: {'total': 80.0, '00-09': 91.4, '10-19': 91.0, '20-29': 93.4, '30-39': 92.0, '40-49': 90.8, '50-59': 85.4, '60-69': 91.2, '70-79': 84.8, '80-89': 0.0, 'old': 90.0, 'new': 0.0}
2025-11-19 23:11:37,798 [trainer.py] => CNN top1 curve: [98.8, 97.05, 95.8, 94.9, 93.4, 91.77, 91.47, 88.1, 87.46]
2025-11-19 23:11:37,799 [trainer.py] => CNN top5 curve: [100.0, 99.85, 99.73, 99.6, 99.28, 99.13, 99.09, 98.84, 98.44]
2025-11-19 23:11:37,799 [trainer.py] => NME top1 curve: [49.15, 64.43, 71.85, 76.06, 78.13, 79.26, 80.78, 80.0]
2025-11-19 23:11:37,799 [trainer.py] => NME top5 curve: [49.95, 66.37, 74.72, 79.58, 82.83, 85.06, 86.82, 88.11]

2025-11-19 23:11:37,799 [trainer.py] => Average Accuracy (CNN): 93.19444444444444
2025-11-19 23:11:37,800 [trainer.py] => Average Accuracy (NME): 72.4575
2025-11-19 23:11:37,801 [trainer.py] => All params: 94037776
2025-11-19 23:11:37,802 [trainer.py] => Trainable params: 2439280
2025-11-19 23:11:37,802 [sema.py] => Learning on 90-100
2025-11-19 23:11:37,895 [sema.py] => [DEBUG] Task 9: New data size: 5000, Memory Size: 1980, Total Dataset Size: 6980
2025-11-19 23:11:37,920 [sema.py] => Unfreezing all the adapters and routers for rehearsal...
2025-11-19 23:11:39,539 [sema_block.py] => Adapter 9.8 added at block 9
2025-11-20 00:00:13,495 [sema.py] => func Task 9, Epoch 5/5 => Loss 0.350, Train_accy 90.10, Test_accy 87.14
2025-11-20 02:23:57,109 [sema.py] => rd Task 9, Epoch 20/20 => Loss 0.106, Train_accy 92.00, Test_accy 87.14
2025-11-20 02:23:59,295 [sema_block.py] => Adapter 10.8 added at block 10
2025-11-20 03:24:11,982 [sema.py] => func Task 9, Epoch 5/5 => Loss 0.336, Train_accy 90.62, Test_accy 86.93
2025-11-21 10:29:08,276 [trainer.py] => config: exps/sema_cifar.json
2025-11-21 10:29:08,276 [trainer.py] => eval: False
2025-11-21 10:29:08,276 [trainer.py] => prefix: reproduce
2025-11-21 10:29:08,276 [trainer.py] => dataset: cifar224
2025-11-21 10:29:08,276 [trainer.py] => memory_size: 0
2025-11-21 10:29:08,276 [trainer.py] => memory_per_class: 0
2025-11-21 10:29:08,277 [trainer.py] => fixed_memory: False
2025-11-21 10:29:08,277 [trainer.py] => shuffle: True
2025-11-21 10:29:08,278 [trainer.py] => init_cls: 10
2025-11-21 10:29:08,278 [trainer.py] => increment: 10
2025-11-21 10:29:08,279 [trainer.py] => model_name: sema
2025-11-21 10:29:08,279 [trainer.py] => backbone_type: pretrained_vit_b16_224_adapter
2025-11-21 10:29:08,279 [trainer.py] => device: [device(type='cuda', index=0)]
2025-11-21 10:29:08,280 [trainer.py] => seed: 1993
2025-11-21 10:29:08,280 [trainer.py] => batch_size: 32
2025-11-21 10:29:08,280 [trainer.py] => weight_decay: 0.0005
2025-11-21 10:29:08,303 [trainer.py] => min_lr: 0
2025-11-21 10:29:08,304 [trainer.py] => ffn_adapter_type: adaptmlp
2025-11-21 10:29:08,304 [trainer.py] => ffn_num: 16
2025-11-21 10:29:08,304 [trainer.py] => optimizer: sgd
2025-11-21 10:29:08,305 [trainer.py] => vpt_type: shallow
2025-11-21 10:29:08,305 [trainer.py] => prompt_token_num: 5
2025-11-21 10:29:08,305 [trainer.py] => func_epoch: 5
2025-11-21 10:29:08,305 [trainer.py] => rd_epoch: 20
2025-11-21 10:29:08,305 [trainer.py] => init_lr: 0.005
2025-11-21 10:29:08,306 [trainer.py] => rd_lr: 0.01
2025-11-21 10:29:08,306 [trainer.py] => rd_dim: 128
2025-11-21 10:29:08,306 [trainer.py] => buffer_size: 500
2025-11-21 10:29:08,306 [trainer.py] => detect_batch_size: 128
2025-11-21 10:29:08,306 [trainer.py] => exp_threshold: 2
2025-11-21 10:29:08,306 [trainer.py] => exp_k_std: 3.0
2025-11-21 10:29:08,306 [trainer.py] => router_attn_dim: 128
2025-11-21 10:29:08,307 [trainer.py] => adapt_start_layer: 9
2025-11-21 10:29:08,307 [trainer.py] => adapt_end_layer: 11
2025-11-21 10:29:09,240 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-11-21 10:29:11,046 [sema_block.py] => Adapter 0.0 added at block 0
2025-11-21 10:29:11,072 [sema_block.py] => Adapter 1.0 added at block 1
2025-11-21 10:29:11,098 [sema_block.py] => Adapter 2.0 added at block 2
2025-11-21 10:29:11,124 [sema_block.py] => Adapter 3.0 added at block 3
2025-11-21 10:29:11,150 [sema_block.py] => Adapter 4.0 added at block 4
2025-11-21 10:29:11,176 [sema_block.py] => Adapter 5.0 added at block 5
2025-11-21 10:29:11,201 [sema_block.py] => Adapter 6.0 added at block 6
2025-11-21 10:29:11,228 [sema_block.py] => Adapter 7.0 added at block 7
2025-11-21 10:29:11,256 [sema_block.py] => Adapter 8.0 added at block 8
2025-11-21 10:29:11,285 [sema_block.py] => Adapter 9.0 added at block 9
2025-11-21 10:29:11,315 [sema_block.py] => Adapter 10.0 added at block 10
2025-11-21 10:29:11,345 [sema_block.py] => Adapter 11.0 added at block 11
2025-11-21 10:29:11,925 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-11-21 10:29:12,514 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-11-21 10:29:12,696 [trainer.py] => All params: 89067096
2025-11-21 10:29:12,697 [trainer.py] => Trainable params: 3268440
2025-11-21 10:29:12,699 [sema.py] => Learning on 0-10
2025-11-21 10:30:52,068 [sema.py] => func Task 0, Epoch 5/5 => Loss 0.207, Train_accy 93.04, Test_accy 99.20
2025-11-21 10:37:33,530 [sema.py] => rd Task 0, Epoch 20/20 => Loss 0.662, Train_accy 0.00, Test_accy 99.20
2025-11-21 10:37:33,531 [sema.py] => Computing class prototypes (NCM) for new classes...
2025-11-21 10:37:49,067 [sema.py] => Prototype replacement complete.
2025-11-21 10:37:52,351 [sema.py] => Caching old model for distillation...
2025-11-21 10:37:52,373 [trainer.py] => No NME accuracy.
2025-11-21 10:37:52,374 [trainer.py] => CNN: {'total': 97.8, '00-09': 97.8, 'old': 0, 'new': 97.8}
2025-11-21 10:37:52,374 [trainer.py] => CNN top1 curve: [97.8]
2025-11-21 10:37:52,374 [trainer.py] => CNN top5 curve: [100.0]

2025-11-21 10:37:52,374 [trainer.py] => Average Accuracy (CNN): 97.8 

2025-11-21 10:37:52,375 [trainer.py] => All params: 89143996
2025-11-21 10:37:52,376 [trainer.py] => Trainable params: 2448508
2025-11-21 10:37:52,376 [sema.py] => Learning on 10-20
2025-11-21 10:37:53,083 [sema_block.py] => Adapter 11.1 added at block 11
2025-11-21 10:45:20,656 [trainer.py] => config: exps/sema_cifar.json
2025-11-21 10:45:20,656 [trainer.py] => eval: False
2025-11-21 10:45:20,656 [trainer.py] => prefix: reproduce
2025-11-21 10:45:20,657 [trainer.py] => dataset: cifar224
2025-11-21 10:45:20,657 [trainer.py] => memory_size: 0
2025-11-21 10:45:20,658 [trainer.py] => memory_per_class: 0
2025-11-21 10:45:20,658 [trainer.py] => fixed_memory: False
2025-11-21 10:45:20,658 [trainer.py] => shuffle: True
2025-11-21 10:45:20,658 [trainer.py] => init_cls: 10
2025-11-21 10:45:20,659 [trainer.py] => increment: 10
2025-11-21 10:45:20,659 [trainer.py] => model_name: sema
2025-11-21 10:45:20,659 [trainer.py] => backbone_type: pretrained_vit_b16_224_adapter
2025-11-21 10:45:20,659 [trainer.py] => device: [device(type='cuda', index=0)]
2025-11-21 10:45:20,659 [trainer.py] => seed: 1993
2025-11-21 10:45:20,660 [trainer.py] => batch_size: 32
2025-11-21 10:45:20,660 [trainer.py] => weight_decay: 0.0005
2025-11-21 10:45:20,660 [trainer.py] => min_lr: 0
2025-11-21 10:45:20,660 [trainer.py] => ffn_adapter_type: adaptmlp
2025-11-21 10:45:20,660 [trainer.py] => ffn_num: 16
2025-11-21 10:45:20,661 [trainer.py] => optimizer: sgd
2025-11-21 10:45:20,661 [trainer.py] => vpt_type: shallow
2025-11-21 10:45:20,661 [trainer.py] => prompt_token_num: 5
2025-11-21 10:45:20,661 [trainer.py] => func_epoch: 5
2025-11-21 10:45:20,661 [trainer.py] => rd_epoch: 20
2025-11-21 10:45:20,662 [trainer.py] => init_lr: 0.005
2025-11-21 10:45:20,662 [trainer.py] => rd_lr: 0.01
2025-11-21 10:45:20,662 [trainer.py] => rd_dim: 128
2025-11-21 10:45:20,662 [trainer.py] => buffer_size: 500
2025-11-21 10:45:20,662 [trainer.py] => detect_batch_size: 128
2025-11-21 10:45:20,662 [trainer.py] => exp_threshold: 2
2025-11-21 10:45:20,662 [trainer.py] => exp_k_std: 3.0
2025-11-21 10:45:20,663 [trainer.py] => router_attn_dim: 128
2025-11-21 10:45:20,663 [trainer.py] => adapt_start_layer: 9
2025-11-21 10:45:20,663 [trainer.py] => adapt_end_layer: 11
2025-11-21 10:45:21,597 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-11-21 10:45:22,507 [sema_block.py] => Adapter 0.0 added at block 0
2025-11-21 10:45:22,535 [sema_block.py] => Adapter 1.0 added at block 1
2025-11-21 10:45:22,562 [sema_block.py] => Adapter 2.0 added at block 2
2025-11-21 10:45:22,590 [sema_block.py] => Adapter 3.0 added at block 3
2025-11-21 10:45:22,617 [sema_block.py] => Adapter 4.0 added at block 4
2025-11-21 10:45:22,644 [sema_block.py] => Adapter 5.0 added at block 5
2025-11-21 10:45:22,672 [sema_block.py] => Adapter 6.0 added at block 6
2025-11-21 10:45:22,700 [sema_block.py] => Adapter 7.0 added at block 7
2025-11-21 10:45:22,729 [sema_block.py] => Adapter 8.0 added at block 8
2025-11-21 10:45:22,758 [sema_block.py] => Adapter 9.0 added at block 9
2025-11-21 10:45:22,787 [sema_block.py] => Adapter 10.0 added at block 10
2025-11-21 10:45:22,816 [sema_block.py] => Adapter 11.0 added at block 11
2025-11-21 10:45:23,367 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-11-21 10:45:23,923 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-11-21 10:45:24,010 [trainer.py] => All params: 89067096
2025-11-21 10:45:24,011 [trainer.py] => Trainable params: 3268440
2025-11-21 10:45:24,013 [sema.py] => Learning on 0-10
2025-11-21 10:47:02,871 [sema.py] => func Task 0, Epoch 5/5 => Loss 0.207, Train_accy 93.04, Test_accy 99.20
2025-11-21 10:53:45,627 [sema.py] => rd Task 0, Epoch 20/20 => Loss 0.662, Train_accy 0.00, Test_accy 99.20
2025-11-21 10:53:45,628 [sema.py] => Computing class prototypes (NCM) for new classes...
2025-11-21 10:54:00,538 [sema.py] => Prototype replacement complete.
2025-11-21 10:54:03,637 [sema.py] => Caching old model for distillation...
2025-11-21 10:54:03,660 [trainer.py] => No NME accuracy.
2025-11-21 10:54:03,660 [trainer.py] => CNN: {'total': 97.8, '00-09': 97.8, 'old': 0, 'new': 97.8}
2025-11-21 10:54:03,660 [trainer.py] => CNN top1 curve: [97.8]
2025-11-21 10:54:03,660 [trainer.py] => CNN top5 curve: [100.0]

2025-11-21 10:54:03,661 [trainer.py] => Average Accuracy (CNN): 97.8 

2025-11-21 10:54:03,661 [trainer.py] => All params: 89143996
2025-11-21 10:54:03,662 [trainer.py] => Trainable params: 2448508
2025-11-21 10:54:03,662 [sema.py] => Learning on 10-20
2025-11-21 10:54:04,300 [sema_block.py] => Adapter 11.1 added at block 11
2025-11-21 10:56:03,752 [sema.py] => func Task 1, Epoch 5/5 => Loss nan, Train_accy 91.14, Test_accy 48.85
2025-11-21 11:01:44,816 [sema.py] => rd Task 1, Epoch 20/20 => Loss 0.450, Train_accy 0.00, Test_accy 48.85
2025-11-21 11:02:10,929 [sema_block.py] => Adapter 9.1 added at block 9
2025-11-21 11:04:15,629 [sema.py] => func Task 1, Epoch 5/5 => Loss nan, Train_accy 90.84, Test_accy 48.90
2025-11-21 11:09:29,392 [sema.py] => rd Task 1, Epoch 20/20 => Loss 0.091, Train_accy 0.00, Test_accy 48.90
2025-11-21 11:09:31,224 [sema_block.py] => Adapter 10.1 added at block 10
2025-11-21 11:11:34,539 [sema.py] => func Task 1, Epoch 5/5 => Loss nan, Train_accy 91.52, Test_accy 48.90
2025-11-21 11:17:19,897 [sema.py] => rd Task 1, Epoch 20/20 => Loss 0.164, Train_accy 0.00, Test_accy 48.90
2025-11-21 11:17:37,229 [sema.py] => Computing class prototypes (NCM) for new classes...
2025-11-21 11:17:53,372 [sema.py] => Prototype replacement complete.
2025-11-21 11:18:00,120 [sema.py] => Caching old model for distillation...
2025-11-21 11:18:00,146 [trainer.py] => No NME accuracy.
2025-11-21 11:18:00,147 [trainer.py] => CNN: {'total': 94.4, '00-09': 96.8, '10-19': 92.0, 'old': 96.8, 'new': 92.0}
2025-11-21 11:18:00,147 [trainer.py] => CNN top1 curve: [97.8, 94.4]
2025-11-21 11:18:00,147 [trainer.py] => CNN top5 curve: [100.0, 99.7]

2025-11-21 11:18:00,147 [trainer.py] => Average Accuracy (CNN): 96.1 

2025-11-21 11:18:00,148 [trainer.py] => All params: 89812588
2025-11-21 11:18:00,148 [trainer.py] => Trainable params: 2448508
2025-11-21 11:18:00,148 [sema.py] => Learning on 20-30
2025-11-21 11:18:00,842 [sema_block.py] => Adapter 11.2 added at block 11
2025-11-21 11:18:50,846 [trainer.py] => config: exps/sema_cifar.json
2025-11-21 11:18:50,847 [trainer.py] => eval: False
2025-11-21 11:18:50,847 [trainer.py] => prefix: reproduce
2025-11-21 11:18:50,847 [trainer.py] => dataset: cifar224
2025-11-21 11:18:50,847 [trainer.py] => memory_size: 0
2025-11-21 11:18:50,847 [trainer.py] => memory_per_class: 0
2025-11-21 11:18:50,847 [trainer.py] => fixed_memory: False
2025-11-21 11:18:50,848 [trainer.py] => shuffle: True
2025-11-21 11:18:50,848 [trainer.py] => init_cls: 10
2025-11-21 11:18:50,848 [trainer.py] => increment: 10
2025-11-21 11:18:50,848 [trainer.py] => model_name: sema
2025-11-21 11:18:50,849 [trainer.py] => backbone_type: pretrained_vit_b16_224_adapter
2025-11-21 11:18:50,849 [trainer.py] => device: [device(type='cuda', index=0)]
2025-11-21 11:18:50,849 [trainer.py] => seed: 1993
2025-11-21 11:18:50,849 [trainer.py] => batch_size: 32
2025-11-21 11:18:50,849 [trainer.py] => weight_decay: 0.0005
2025-11-21 11:18:50,849 [trainer.py] => min_lr: 0
2025-11-21 11:18:50,850 [trainer.py] => ffn_adapter_type: adaptmlp
2025-11-21 11:18:50,850 [trainer.py] => ffn_num: 16
2025-11-21 11:18:50,850 [trainer.py] => optimizer: sgd
2025-11-21 11:18:50,850 [trainer.py] => vpt_type: shallow
2025-11-21 11:18:50,850 [trainer.py] => prompt_token_num: 5
2025-11-21 11:18:50,850 [trainer.py] => func_epoch: 5
2025-11-21 11:18:50,851 [trainer.py] => rd_epoch: 20
2025-11-21 11:18:50,851 [trainer.py] => init_lr: 0.005
2025-11-21 11:18:50,851 [trainer.py] => rd_lr: 0.01
2025-11-21 11:18:50,851 [trainer.py] => rd_dim: 128
2025-11-21 11:18:50,851 [trainer.py] => buffer_size: 500
2025-11-21 11:18:50,851 [trainer.py] => detect_batch_size: 128
2025-11-21 11:18:50,851 [trainer.py] => exp_threshold: 2
2025-11-21 11:18:50,852 [trainer.py] => exp_k_std: 3.0
2025-11-21 11:18:50,852 [trainer.py] => router_attn_dim: 128
2025-11-21 11:18:50,852 [trainer.py] => adapt_start_layer: 9
2025-11-21 11:18:50,852 [trainer.py] => adapt_end_layer: 11
2025-11-21 11:18:51,768 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-11-21 11:18:52,679 [sema_block.py] => Adapter 0.0 added at block 0
2025-11-21 11:18:52,703 [sema_block.py] => Adapter 1.0 added at block 1
2025-11-21 11:18:52,726 [sema_block.py] => Adapter 2.0 added at block 2
2025-11-21 11:18:52,751 [sema_block.py] => Adapter 3.0 added at block 3
2025-11-21 11:18:52,773 [sema_block.py] => Adapter 4.0 added at block 4
2025-11-21 11:18:52,796 [sema_block.py] => Adapter 5.0 added at block 5
2025-11-21 11:18:52,819 [sema_block.py] => Adapter 6.0 added at block 6
2025-11-21 11:18:52,842 [sema_block.py] => Adapter 7.0 added at block 7
2025-11-21 11:18:52,867 [sema_block.py] => Adapter 8.0 added at block 8
2025-11-21 11:18:52,898 [sema_block.py] => Adapter 9.0 added at block 9
2025-11-21 11:18:52,928 [sema_block.py] => Adapter 10.0 added at block 10
2025-11-21 11:18:52,958 [sema_block.py] => Adapter 11.0 added at block 11
2025-11-21 11:18:53,511 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-11-21 11:18:54,122 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-11-21 11:18:54,224 [trainer.py] => All params: 89067096
2025-11-21 11:18:54,225 [trainer.py] => Trainable params: 3268440
2025-11-21 11:18:54,227 [sema.py] => Learning on 0-10
2025-11-21 11:20:33,601 [sema.py] => func Task 0, Epoch 5/5 => Loss 0.207, Train_accy 93.04, Test_accy 99.20
2025-11-21 11:27:17,866 [sema.py] => rd Task 0, Epoch 20/20 => Loss 0.662, Train_accy 0.00, Test_accy 99.20
2025-11-21 11:27:17,868 [sema.py] => Computing class prototypes (NCM) for new classes...
2025-11-21 11:27:32,461 [sema.py] => Prototype replacement complete.
2025-11-21 11:27:35,524 [sema.py] => Caching old model for distillation...
2025-11-21 11:27:35,544 [trainer.py] => No NME accuracy.
2025-11-21 11:27:35,544 [trainer.py] => CNN: {'total': 97.8, '00-09': 97.8, 'old': 0, 'new': 97.8}
2025-11-21 11:27:35,544 [trainer.py] => CNN top1 curve: [97.8]
2025-11-21 11:27:35,544 [trainer.py] => CNN top5 curve: [100.0]

2025-11-21 11:27:35,545 [trainer.py] => Average Accuracy (CNN): 97.8 

2025-11-21 11:27:35,545 [trainer.py] => All params: 89143996
2025-11-21 11:27:35,546 [trainer.py] => Trainable params: 2448508
2025-11-21 11:27:35,546 [sema.py] => Learning on 10-20
2025-11-21 11:27:36,197 [sema_block.py] => Adapter 11.1 added at block 11
2025-11-21 11:29:36,146 [sema.py] => func Task 1, Epoch 5/5 => Loss 0.334, Train_accy 91.16, Test_accy 48.90
2025-11-21 11:35:17,259 [sema.py] => rd Task 1, Epoch 20/20 => Loss 0.450, Train_accy 0.00, Test_accy 48.90
2025-11-21 11:35:42,408 [sema_block.py] => Adapter 9.1 added at block 9
2025-11-21 11:48:13,441 [sema.py] => func Task 1, Epoch 5/5 => Loss 0.346, Train_accy 90.84, Test_accy 48.85
2025-11-24 09:49:38,622 [trainer.py] => config: exps/sema_cifar.json
2025-11-24 09:49:38,622 [trainer.py] => eval: False
2025-11-24 09:49:38,622 [trainer.py] => prefix: reproduce
2025-11-24 09:49:38,623 [trainer.py] => dataset: cifar224
2025-11-24 09:49:38,623 [trainer.py] => memory_size: 0
2025-11-24 09:49:38,623 [trainer.py] => memory_per_class: 0
2025-11-24 09:49:38,624 [trainer.py] => fixed_memory: False
2025-11-24 09:49:38,624 [trainer.py] => shuffle: True
2025-11-24 09:49:38,625 [trainer.py] => init_cls: 10
2025-11-24 09:49:38,625 [trainer.py] => increment: 10
2025-11-24 09:49:38,625 [trainer.py] => model_name: sema
2025-11-24 09:49:38,625 [trainer.py] => backbone_type: pretrained_vit_b16_224_adapter
2025-11-24 09:49:38,625 [trainer.py] => device: [device(type='cuda', index=0)]
2025-11-24 09:49:38,626 [trainer.py] => seed: 1993
2025-11-24 09:49:38,626 [trainer.py] => batch_size: 32
2025-11-24 09:49:38,626 [trainer.py] => weight_decay: 0.0005
2025-11-24 09:49:38,626 [trainer.py] => min_lr: 0
2025-11-24 09:49:38,626 [trainer.py] => ffn_adapter_type: adaptmlp
2025-11-24 09:49:38,627 [trainer.py] => ffn_num: 16
2025-11-24 09:49:38,627 [trainer.py] => optimizer: sgd
2025-11-24 09:49:38,627 [trainer.py] => vpt_type: shallow
2025-11-24 09:49:38,627 [trainer.py] => prompt_token_num: 5
2025-11-24 09:49:38,627 [trainer.py] => func_epoch: 5
2025-11-24 09:49:38,627 [trainer.py] => rd_epoch: 20
2025-11-24 09:49:38,628 [trainer.py] => init_lr: 0.005
2025-11-24 09:49:38,628 [trainer.py] => rd_lr: 0.01
2025-11-24 09:49:38,628 [trainer.py] => rd_dim: 128
2025-11-24 09:49:38,628 [trainer.py] => buffer_size: 500
2025-11-24 09:49:38,628 [trainer.py] => detect_batch_size: 128
2025-11-24 09:49:38,628 [trainer.py] => exp_threshold: 2
2025-11-24 09:49:38,628 [trainer.py] => exp_k_std: 3.0
2025-11-24 09:49:38,629 [trainer.py] => router_attn_dim: 128
2025-11-24 09:49:38,629 [trainer.py] => adapt_start_layer: 9
2025-11-24 09:49:38,629 [trainer.py] => adapt_end_layer: 11
2025-11-24 09:49:39,633 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-11-24 09:49:41,330 [sema_block.py] => Adapter 0.0 added at block 0
2025-11-24 09:49:41,357 [sema_block.py] => Adapter 1.0 added at block 1
2025-11-24 09:49:41,385 [sema_block.py] => Adapter 2.0 added at block 2
2025-11-24 09:49:41,413 [sema_block.py] => Adapter 3.0 added at block 3
2025-11-24 09:49:41,440 [sema_block.py] => Adapter 4.0 added at block 4
2025-11-24 09:49:41,468 [sema_block.py] => Adapter 5.0 added at block 5
2025-11-24 09:49:41,496 [sema_block.py] => Adapter 6.0 added at block 6
2025-11-24 09:49:41,524 [sema_block.py] => Adapter 7.0 added at block 7
2025-11-24 09:49:41,551 [sema_block.py] => Adapter 8.0 added at block 8
2025-11-24 09:49:41,580 [sema_block.py] => Adapter 9.0 added at block 9
2025-11-24 09:49:41,610 [sema_block.py] => Adapter 10.0 added at block 10
2025-11-24 09:49:41,638 [sema_block.py] => Adapter 11.0 added at block 11
2025-11-24 09:49:42,230 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-11-24 09:49:42,861 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-11-24 09:49:43,051 [trainer.py] => All params: 89067096
2025-11-24 09:49:43,053 [trainer.py] => Trainable params: 3268440
2025-11-24 09:49:43,055 [sema.py] => Learning on 0-10
2025-11-24 09:51:24,180 [sema.py] => func Task 0, Epoch 5/5 => Loss 0.207, Train_accy 93.04, Test_accy 99.20
2025-11-24 09:58:05,286 [sema.py] => rd Task 0, Epoch 20/20 => Loss 0.662, Train_accy 0.00, Test_accy 99.20
2025-11-24 09:58:05,287 [sema.py] => Computing class prototypes (NCM) for new classes...
2025-11-24 09:58:20,896 [sema.py] => Prototype replacement complete.
2025-11-24 09:58:24,164 [sema.py] => Caching old model for distillation...
2025-11-24 09:58:24,185 [trainer.py] => No NME accuracy.
2025-11-24 09:58:24,185 [trainer.py] => CNN: {'total': 97.8, '00-09': 97.8, 'old': 0, 'new': 97.8}
2025-11-24 09:58:24,186 [trainer.py] => CNN top1 curve: [97.8]
2025-11-24 09:58:24,186 [trainer.py] => CNN top5 curve: [100.0]

2025-11-24 09:58:24,186 [trainer.py] => Average Accuracy (CNN): 97.8 

2025-11-24 09:58:24,187 [trainer.py] => All params: 89143996
2025-11-24 09:58:24,187 [trainer.py] => Trainable params: 2448508
2025-11-24 09:58:24,188 [sema.py] => Learning on 10-20
2025-11-24 09:58:24,917 [sema_block.py] => Adapter 11.1 added at block 11
2025-11-24 10:00:22,824 [sema.py] => func Task 1, Epoch 5/5 => Loss 0.334, Train_accy 91.16, Test_accy 48.90
2025-11-24 10:06:02,815 [sema.py] => rd Task 1, Epoch 20/20 => Loss 0.450, Train_accy 0.00, Test_accy 48.90
2025-11-24 10:06:28,555 [sema_block.py] => Adapter 9.1 added at block 9
2025-11-24 10:08:30,468 [sema.py] => func Task 1, Epoch 5/5 => Loss 0.346, Train_accy 90.84, Test_accy 48.85
2025-11-24 10:14:26,430 [sema.py] => rd Task 1, Epoch 20/20 => Loss 0.091, Train_accy 0.00, Test_accy 48.85
2025-11-24 10:14:29,875 [sema_block.py] => Adapter 10.1 added at block 10
2025-11-24 10:18:24,580 [sema.py] => func Task 1, Epoch 5/5 => Loss 0.314, Train_accy 91.50, Test_accy 48.85
